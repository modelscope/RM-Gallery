{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RM-Gallery Quick Start\n",
    "\n",
    "This notebook demonstrates the basic usage of RM-Gallery for reward model evaluation and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure you have RM-Gallery installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RM-Gallery\n",
    "# !pip install rm-gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import the necessary modules from RM-Gallery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm import RewardModel\n",
    "from rm_gallery.core.data import DataProcessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Pre-built Reward Model\n",
    "\n",
    "RM-Gallery supports 35+ pre-built reward models. Here's how to load one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a reward model\n",
    "rm = RewardModel(\n",
    "    model_name=\"Skywork/Skywork-Reward-Llama-3.1-8B\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(\"Reward model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Text Quality\n",
    "\n",
    "Use the reward model to score text quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample data\n",
    "prompt = \"What is the capital of France?\"\n",
    "response_a = \"The capital of France is Paris.\"\n",
    "response_b = \"I don't know.\"\n",
    "\n",
    "# Score the responses\n",
    "score_a = rm.score(prompt, response_a)\n",
    "score_b = rm.score(prompt, response_b)\n",
    "\n",
    "print(f\"Score for Response A: {score_a:.4f}\")\n",
    "print(f\"Score for Response B: {score_b:.4f}\")\n",
    "print(f\"\\nResponse A is better!\" if score_a > score_b else \"\\nResponse B is better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "Evaluate multiple response pairs efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch data\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing\",\n",
    "        \"response_a\": \"Quantum computing uses quantum bits...\",\n",
    "        \"response_b\": \"It's about computers.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"response_a\": \"Machine learning is a subset of AI...\",\n",
    "        \"response_b\": \"ML is when computers learn.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Batch scoring\n",
    "results = rm.batch_score(data)\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best-of-N Selection\n",
    "\n",
    "Select the best response from multiple candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple candidates\n",
    "prompt = \"Write a poem about AI\"\n",
    "candidates = [\n",
    "    \"AI is smart, AI is bright...\",\n",
    "    \"In circuits deep and neural nets...\",\n",
    "    \"Artificial minds that think and learn...\",\n",
    "    \"Code and data intertwine...\"\n",
    "]\n",
    "\n",
    "# Score all candidates\n",
    "scores = [rm.score(prompt, candidate) for candidate in candidates]\n",
    "\n",
    "# Select the best one\n",
    "best_idx = scores.index(max(scores))\n",
    "print(f\"Best candidate (score: {scores[best_idx]:.4f}):\\n{candidates[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- ðŸ“– Check out the [full documentation](https://modelscope.github.io/RM-Gallery/)\n",
    "- ðŸŽ¯ Explore custom reward models\n",
    "- ðŸ“Š Learn about evaluation pipelines\n",
    "- ðŸš€ Try RLHF training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
