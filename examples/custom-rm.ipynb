{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Reward Models\n",
    "\n",
    "This notebook demonstrates how to build custom reward models with RM-Gallery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.core.reward import BaseRewardModel\n",
    "from rm_gallery.gallery.rm import RubricRM, EnsembleRM\n",
    "from typing import List, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Rubric-Based Reward Model\n",
    "\n",
    "Create a reward model using custom evaluation rubrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom rubric\n",
    "custom_rubric = {\n",
    "    \"helpfulness\": {\n",
    "        \"description\": \"Does the response helpful answer the question?\",\n",
    "        \"weight\": 0.4\n",
    "    },\n",
    "    \"accuracy\": {\n",
    "        \"description\": \"Is the information factually correct?\",\n",
    "        \"weight\": 0.3\n",
    "    },\n",
    "    \"clarity\": {\n",
    "        \"description\": \"Is the response clear and well-structured?\",\n",
    "        \"weight\": 0.2\n",
    "    },\n",
    "    \"safety\": {\n",
    "        \"description\": \"Is the response safe and appropriate?\",\n",
    "        \"weight\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize rubric-based RM\n",
    "rubric_rm = RubricRM(\n",
    "    rubric=custom_rubric,\n",
    "    llm_model=\"gpt-4\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Rubric-based RM created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Custom Python Class\n",
    "\n",
    "Implement a completely custom reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthBasedRM(BaseRewardModel):\n",
    "    \"\"\"Reward model that scores based on response length.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimal_length: int = 200):\n",
    "        super().__init__()\n",
    "        self.optimal_length = optimal_length\n",
    "    \n",
    "    def score(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Score based on how close the response is to optimal length.\"\"\"\n",
    "        length = len(response.split())\n",
    "        deviation = abs(length - self.optimal_length)\n",
    "        score = max(0, 1 - (deviation / self.optimal_length))\n",
    "        return score\n",
    "    \n",
    "    def batch_score(self, prompts: List[str], responses: List[str]) -> List[float]:\n",
    "        \"\"\"Batch scoring.\"\"\"\n",
    "        return [self.score(p, r) for p, r in zip(prompts, responses)]\n",
    "\n",
    "# Test the custom RM\n",
    "length_rm = LengthBasedRM(optimal_length=50)\n",
    "score = length_rm.score(\n",
    "    prompt=\"Tell me about AI\",\n",
    "    response=\"Artificial Intelligence is a fascinating field.\" * 10\n",
    ")\n",
    "print(f\"Length-based score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Ensemble Reward Model\n",
    "\n",
    "Combine multiple reward models for robust scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm import RewardModel\n",
    "\n",
    "# Create ensemble of multiple RMs\n",
    "ensemble_rm = EnsembleRM(\n",
    "    models=[\n",
    "        RewardModel(\"Skywork/Skywork-Reward-Llama-3.1-8B\"),\n",
    "        RewardModel(\"Ray2333/GRM-Llama3-8B-rewardmodel-ft\"),\n",
    "        LengthBasedRM(optimal_length=100)\n",
    "    ],\n",
    "    weights=[0.4, 0.4, 0.2],  # Weight each model's contribution\n",
    "    aggregation=\"weighted_mean\"  # or \"max\", \"min\", \"median\"\n",
    ")\n",
    "\n",
    "# Test ensemble\n",
    "ensemble_score = ensemble_rm.score(\n",
    "    prompt=\"Explain machine learning\",\n",
    "    response=\"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    ")\n",
    "print(f\"Ensemble score: {ensemble_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Fine-tuned Reward Model\n",
    "\n",
    "Load and use a fine-tuned reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model from local path\n",
    "finetuned_rm = RewardModel(\n",
    "    model_name=\"./my_finetuned_rm\",  # Local path\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Or from Hugging Face Hub\n",
    "# finetuned_rm = RewardModel(\n",
    "#     model_name=\"your-username/your-rm-model\",\n",
    "#     device=\"cuda\"\n",
    "# )\n",
    "\n",
    "print(\"Fine-tuned RM loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Generate Rubrics\n",
    "\n",
    "Automatically generate rubrics from example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm import AutoRubric\n",
    "\n",
    "# Provide examples of good and bad responses\n",
    "examples = [\n",
    "    {\n",
    "        \"prompt\": \"Explain photosynthesis\",\n",
    "        \"good\": \"Photosynthesis is the process by which plants...\",\n",
    "        \"bad\": \"Plants make food.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is gravity?\",\n",
    "        \"good\": \"Gravity is a fundamental force...\",\n",
    "        \"bad\": \"Things fall down.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Auto-generate rubric\n",
    "auto_rubric = AutoRubric.from_examples(\n",
    "    examples=examples,\n",
    "    num_criteria=5,\n",
    "    llm_model=\"gpt-4\"\n",
    ")\n",
    "\n",
    "print(\"\\nAuto-generated Rubric:\")\n",
    "print(auto_rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Custom RMs\n",
    "\n",
    "Evaluate and compare different custom reward models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"Write a short story\",\n",
    "        \"response\": \"Once upon a time, in a land far away...\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum mechanics\",\n",
    "        \"response\": \"Quantum mechanics is a fundamental theory in physics...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    \"Rubric-based\": rubric_rm,\n",
    "    \"Length-based\": length_rm,\n",
    "    \"Ensemble\": ensemble_rm\n",
    "}\n",
    "\n",
    "for test in test_cases:\n",
    "    print(f\"\\nPrompt: {test['prompt']}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, model in models.items():\n",
    "        score = model.score(test[\"prompt\"], test[\"response\"])\n",
    "        print(f\"{name:20s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Custom Models\n",
    "\n",
    "Persist your custom reward models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "rubric_rm.save(\"my_custom_rubric_rm\")\n",
    "\n",
    "# Load model\n",
    "loaded_rm = RubricRM.load(\"my_custom_rubric_rm\")\n",
    "\n",
    "# Verify it works\n",
    "score = loaded_rm.score(\n",
    "    prompt=\"Test prompt\",\n",
    "    response=\"Test response\"\n",
    ")\n",
    "print(f\"Loaded model score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
