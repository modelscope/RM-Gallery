#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯• Conflict Detector çš„è„šæœ¬
æµ‹è¯•æ›´å¤šæ ·æœ¬ï¼ŒéªŒè¯å†²çªæ£€æµ‹å’ŒæŒ‡æ ‡è®¡ç®—
"""

import json
import os
import sys

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "***REMOVED***"
os.environ["BASE_URL"] = "https://dashscope.aliyuncs.com/compatible-mode/v1"

from rm_gallery.gallery.evaluation.conflict_detector import main


def print_result_summary(result_path: str, mode: str):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    with open(result_path, "r") as f:
        results = json.load(f)

    print(f"\n{'='*80}")
    print(f"{mode.upper()} æ¨¡å¼ç»“æœæ‘˜è¦")
    print(f"{'='*80}")
    print(f"æ¨¡å‹: {results['model']}")
    print(f"æ¯”è¾ƒæ¨¡å¼: {results['comparison_mode']}")

    # å‡†ç¡®ç‡æŒ‡æ ‡
    acc = results["accuracy_metrics"]
    print("\nâœ… å‡†ç¡®ç‡æŒ‡æ ‡:")
    print(f"  - å‡†ç¡®ç‡: {acc['accuracy']:.2%}")
    print(f"  - Chosenä¸»å¯¼ç‡: {acc['chosen_dominance_rate']:.2%}")
    print(
        f"  - Chosenè·èƒœ: {acc['total_chosen_wins']}/{acc['total_chosen_vs_rejected_comparisons']}"
    )
    print(f"  - Chosenå¤±è´¥: {acc['total_chosen_losses']}")
    print(f"  - å¹³å±€: {acc['total_chosen_ties']}")

    # å†²çªæŒ‡æ ‡
    conflict = results["conflict_metrics"]
    print("\nğŸ” å†²çªæŒ‡æ ‡:")
    print(f"  - æ€»ä½“å†²çªç‡: {conflict['overall_conflict_rate']:.2f}%")
    print(f"  - å¹³å‡æ¯æ ·æœ¬å†²çªèŠ‚ç‚¹æ•°: {conflict['conflict_density_rate']:.2f}")
    print(f"  - å¹³å‡æ¯æ¯”è¾ƒå†²çªèŠ‚ç‚¹æ•°: {conflict['conflicts_per_comparison']:.4f}")
    print(f"  - æ€»æ¯”è¾ƒæ¬¡æ•°: {conflict['total_comparisons']}")

    # è¯„ä¼°æ‘˜è¦
    summary = results["evaluation_summary"]
    print("\nğŸ“Š è¯„ä¼°æ‘˜è¦:")
    print(f"  - æˆåŠŸæ ·æœ¬: {summary['successful_samples']}/{results['total_count']}")
    print(f"  - æˆåŠŸç‡: {summary['success_rate']:.2%}")


def test_comprehensive():
    """å…¨é¢æµ‹è¯•ï¼šä½¿ç”¨æ›´å¤šæ ·æœ¬"""
    print("\n" + "=" * 80)
    print("å…¨é¢æµ‹è¯• - Pairwise æ¨¡å¼ (20ä¸ªæ ·æœ¬)")
    print("=" * 80 + "\n")

    main(
        data_path="data/benchmarks/reward-bench-2/data/test-00000-of-00001.parquet",
        result_path="data/results/conflict_pairwise_comprehensive.json",
        max_samples=20,
        model="qwen2.5-32b-instruct",
        max_workers=8,
        comparison_mode="pairwise",
        save_detailed_outputs=False,  # èŠ‚çœç©ºé—´
        random_seed=42,
    )

    print_result_summary(
        "data/results/conflict_pairwise_comprehensive.json", "pairwise"
    )

    print("\n\n" + "=" * 80)
    print("å…¨é¢æµ‹è¯• - Pointwise æ¨¡å¼ (20ä¸ªæ ·æœ¬)")
    print("=" * 80 + "\n")

    main(
        data_path="data/benchmarks/reward-bench-2/data/test-00000-of-00001.parquet",
        result_path="data/results/conflict_pointwise_comprehensive.json",
        max_samples=20,
        model="qwen2.5-32b-instruct",
        max_workers=8,
        comparison_mode="pointwise",
        save_detailed_outputs=False,
        random_seed=42,
    )

    print_result_summary(
        "data/results/conflict_pointwise_comprehensive.json", "pointwise"
    )


def compare_modes():
    """æ¯”è¾ƒä¸¤ç§æ¨¡å¼çš„æ€§èƒ½"""
    print("\n" + "=" * 80)
    print("æ¨¡å¼æ¯”è¾ƒåˆ†æ")
    print("=" * 80)

    with open("data/results/conflict_pairwise_comprehensive.json", "r") as f:
        pairwise = json.load(f)

    with open("data/results/conflict_pointwise_comprehensive.json", "r") as f:
        pointwise = json.load(f)

    print("\næ€§èƒ½å¯¹æ¯” (åŸºäº20ä¸ªæ ·æœ¬):")
    print(f"\n{'æŒ‡æ ‡':<30} {'Pairwise':<20} {'Pointwise':<20}")
    print("-" * 70)

    pw_acc = pairwise["accuracy_metrics"]["accuracy"]
    pt_acc = pointwise["accuracy_metrics"]["accuracy"]
    print(f"{'å‡†ç¡®ç‡':<30} {pw_acc:<20.2%} {pt_acc:<20.2%}")

    pw_dom = pairwise["accuracy_metrics"]["chosen_dominance_rate"]
    pt_dom = pointwise["accuracy_metrics"]["chosen_dominance_rate"]
    print(f"{'Chosenä¸»å¯¼ç‡':<30} {pw_dom:<20.2%} {pt_dom:<20.2%}")

    pw_conf = pairwise["conflict_metrics"]["overall_conflict_rate"]
    pt_conf = pointwise["conflict_metrics"]["overall_conflict_rate"]
    print(f"{'å†²çªç‡':<30} {pw_conf:<20.2f}% {pt_conf:<20.2f}%")

    pw_comp = pairwise["conflict_metrics"]["total_comparisons"]
    pt_comp = pointwise["conflict_metrics"]["total_comparisons"]
    print(f"{'æ€»æ¯”è¾ƒæ¬¡æ•°':<30} {pw_comp:<20} {pt_comp:<20}")

    print("\nç»“è®º:")
    if pw_acc > pt_acc:
        print(f"  âœ“ Pairwiseæ¨¡å¼å‡†ç¡®ç‡æ›´é«˜ (+{(pw_acc - pt_acc)*100:.1f}%)")
    else:
        print(f"  âœ“ Pointwiseæ¨¡å¼å‡†ç¡®ç‡æ›´é«˜ (+{(pt_acc - pw_acc)*100:.1f}%)")

    if pw_conf < pt_conf:
        print("  âœ“ Pairwiseæ¨¡å¼å†²çªç‡æ›´ä½")
    elif pt_conf < pw_conf:
        print("  âœ“ Pointwiseæ¨¡å¼å†²çªç‡æ›´ä½")
    else:
        print("  âœ“ ä¸¤ç§æ¨¡å¼å†²çªç‡ç›¸åŒ")


if __name__ == "__main__":
    try:
        # è¿è¡Œå…¨é¢æµ‹è¯•
        test_comprehensive()

        # æ¯”è¾ƒä¸¤ç§æ¨¡å¼
        compare_modes()

        print("\n\n" + "=" * 80)
        print("âœ… å…¨é¢æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        print("\nç»“æœæ–‡ä»¶:")
        print("  - Pairwise å…¨é¢æµ‹è¯•: data/results/conflict_pairwise_comprehensive.json")
        print("  - Pointwise å…¨é¢æµ‹è¯•: data/results/conflict_pointwise_comprehensive.json")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
