{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPrinciple Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is AutoPrinciple?\n",
    "\n",
    "AutoPrinciple is an LLM-based automated principle generation system designed to dynamically create task-specific evaluation criteria for reward modeling. It leverages large language models (like Qwen3) to extract high-quality assessment rules (e.g., \"Is generated content faithful to the source?\" or \"Is the answer factually accurate?\") from minimal example data, replacing traditional manual rule engineering. The system supports multi-modal tasks (text summarization, mathematical reasoning, code generation, etc.) and generates scenario-aware rules adaptively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why AutoPrinciple?\n",
    "Traditional manual rule engineering faces three critical limitations:\n",
    "\n",
    "- Poor Scalability: Manually designing rules for every task-scenario combination (e.g., 10 tasks × 5 scenarios = 50 rule sets) requires excessive human effort。\n",
    "\n",
    "- Subjective Bias: Human-defined rules often reflect individual cognitive biases (e.g., cultural differences in defining \"safe content\")。\n",
    "\n",
    "- Limited Adaptability: Static rules struggle to adapt to evolving model capabilities (e.g., new error patterns in upgraded models)\n",
    "\n",
    "\n",
    "AutoPrinciple's advantages:\n",
    "\n",
    "- Efficient Generation: Generates candidate rules in bulk via LLM (e.g., 5 samples × 5 candidates = 25 rules)\n",
    "\n",
    "- Dynamic Optimization: Uses clustering to extract core representative rules (e.g., compress 25 to 3 rules)\n",
    "\n",
    "- Cross-Domain Transfer: Applies the same framework to multi-modal tasks (e.g., \"syntax correctness\" for code → \"semantic fidelity\" for translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How AutoPrinciple Works\n",
    "\n",
    "The system operates through a streamlined three-step workflow (with optional iteration):\n",
    "\n",
    "- Candidate Principle Extraction from In-Distribution Data: Generate diverse candidate principles using task-specific in-distribution (ID) data.\n",
    "\n",
    "- High-Quality Principle Compression: Distill candidate principles into a compact, representative set, by applying semantic clustering to group similar candidates.\n",
    "\n",
    "- Iterative Optimization (Optional): Refine principles through evaluation feedback loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to Use AutoPrinciple\n",
    "Here we demonstrates how to use Principle Generator to create **Helpfulness** evaluation principles.\n",
    "\n",
    "Includes full workflow: Data loading → Model configuration → Principle generation → Result analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "# Add project root directory to Python path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Add environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"BASE_URL\"] = \"\"\n",
    "\n",
    "# Import local modules\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "from rm_gallery.core.reward.principle.generator import PrincipleGenerator\n",
    "from rm_gallery.core.utils.file import read_jsonl\n",
    "\n",
    "# Initialize logger\n",
    "from loguru import logger\n",
    "logger.add(\"principle_generator.log\", rotation=\"1 day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Using data from the \"Precise IF\" task as input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Data path (modify according to your actual path)\n",
    "    train_path = \"/mnt3/huangsen.huang/codes/RM-Gallery/data/RMBbench_Train/pairwise/Helpfulness/Summarization.jsonl\"\n",
    "    test_path = \"/mnt3/huangsen.huang/codes/RM-Gallery/data/RMBbench_Test/pairwise/Helpfulness/Summarization.jsonl\"\n",
    "    \n",
    "    # Read JSONL format data and convert to DataSample objects\n",
    "    train_samples = [DataSample(**sample) for sample in read_jsonl(train_path)]\n",
    "    test_samples = [DataSample(**sample) for sample in read_jsonl(test_path)]\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(train_samples)} training samples and {len(test_samples)} test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data loading failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure Generator Parameters\n",
    "\n",
    "- Using Qwen3 as the language model\n",
    "\n",
    "- Setting generation and clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize language model\n",
    "    llm = OpenaiLLM(\n",
    "        model=\"qwen3-235b-a22b\",  # Model name\n",
    "        enable_thinking=True      # Enable reasoning mode\n",
    "    )\n",
    "    \n",
    "    SCENARIO = \"Summarization: The text is compressed into a short form, retaining the main information, which is divided into extraction (directly selected from the original text) and production (rewriting the information).\"\n",
    "\n",
    "    # Create principle generator\n",
    "    generator = PrincipleGenerator( # or IterPrincipleGenerator\n",
    "        llm=llm,\n",
    "        scenario=SCENARIO,  # Scenario description\n",
    "        generate_number=5,   # Generate 5 candidate principles per sample\n",
    "        cluster_number=3     # Cluster to 3 representative principles\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Successfully initialized PrincipleGenerator\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Generator configuration failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Execute batch generation\n",
    "    principles = generator.run_batch(\n",
    "        train_samples[:10],  # Process first 10 samples as example\n",
    "        thread_pool=ThreadPoolExecutor(max_workers=12)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Successfully generated {len(principles)} principles\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Principle generation failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evauluation with Generated Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm.alignment.base import BaseHelpfulnessListwiseReward\n",
    "\n",
    "try:\n",
    "    principles = [f\"{k}: {v}\" for k, v in principles.items()][:3]\n",
    "    reward = BaseHelpfulnessListwiseReward(\n",
    "        name=\"test_helpfulness_listwise_reward\",\n",
    "        llm=llm,\n",
    "        principles=principles,\n",
    "        scenario=SCENARIO\n",
    "    )\n",
    "    evaluation_samples = reward.evaluate_batch(samples=test_samples[:20])\n",
    "    logger.info(f\"Successfully evaluate test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Reward evaluation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results Analysis\n",
    "Analyze the accuracy rate of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def calc_acc(samples: List[DataSample]) -> float:\n",
    "    labels = []\n",
    "    for sample in samples:\n",
    "        labels.append(0)\n",
    "        for output in sample.output:\n",
    "            if output.answer.label[\"preference\"] == \"chosen\":\n",
    "                score = sum(r.score for r in output.answer.reward.details)\n",
    "                if score > 0:\n",
    "                    labels[-1] = 1\n",
    "    return sum(labels) / len(labels)\n",
    "\n",
    "logger.info(f\"Accuracy: {calc_acc(evaluation_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Scenario Results\n",
    "Introduce our experimental result on built-in scenarios with generated principles.\n",
    "\n",
    "\n",
    "### Setting\n",
    "\n",
    "The experimental setup compares two approaches across multiple scenarios:\n",
    "\n",
    "- Base Configuration\n",
    "Uses built-in reward templates from the system (e.g., BaseHelpfulnessListwiseReward) without incorporating any additional principles.\n",
    "Relies solely on predefined reward criteria inherent to the framework.\n",
    "\n",
    "- Generated Configuration\n",
    "Extends the base approach by integrating automatically generated principles via the AutoPrinciple.\n",
    "Principles are dynamically extracted from task-specific data samples using an LLM (e.g., Qwen3).\n",
    "Includes clustering to compress 25 candidate principles into 3 representative rules for efficiency.\n",
    "\n",
    "#### Evaluation Protocol\n",
    "\n",
    "Models: Both configurations use qwen3-8b for evaluation, while principles are generated using qwen3-235b-a22b.\n",
    "\n",
    "Data: 10% of training samples are used to generate principles, and the remaining samples are evaluated.\n",
    "\n",
    "Metric: Accuracy, defined as the proportion of correctly preferred outputs based on reward scores, with 5 independent run.\n",
    "\n",
    "\n",
    "\n",
    "### RewardBench2\n",
    "| Scenario   | Base       | Generated    |\n",
    "|------------|------------|-------------|\n",
    "| Precise IF | 0.5653     | **0.6097**  |\n",
    "| Factuality | 0.7030     | **0.7663**  |\n",
    "| Math       | 0.8866     | **0.8927**  |\n",
    "| Safety     | 0.7946     | **0.9467**  |\n",
    "| Focus      | 0.9022     | **0.9404**  |\n",
    "\n",
    "### RMBBench\n",
    "Note: we only evaluate on the best-of-n data\n",
    "\n",
    "| Scenario        | Base       | Generated   |\n",
    "|-----------------|------------|------------|\n",
    "| Chat            | 0.6810     | **0.7603** |\n",
    "| Brainstorming   | 0.8129     | **0.8187** |\n",
    "| Classification  | **0.7200** | 0.6697     |\n",
    "| Closed QA       | **0.7213** | 0.6915     |\n",
    "| Open QA         | 0.6828     | **0.6937** |\n",
    "| Generation      | **0.7289** | 0.7205     |\n",
    "| Summarization   | 0.6333     | **0.6921** |\n",
    "| Translation     | **0.7336** | 0.6930     |\n",
    "| Rewrite         | **0.6743** | 0.5371     |\n",
    "| Reasoning       | **0.7080** | 0.6986     |\n",
    "| Role Playing    | 0.6164     | **0.6169** |\n",
    "| Code            | **0.8348** | 0.8251     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
