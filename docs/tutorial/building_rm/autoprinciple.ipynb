{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPrinciple Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is AutoPrinciple?\n",
    "\n",
    "AutoPrinciple is an LLM-based automated principle generation system designed to dynamically create task-specific evaluation criteria for reward modeling. It leverages large language models (like Qwen3) to extract high-quality assessment rules (e.g., \"Is generated content faithful to the source?\" or \"Is the answer factually accurate?\") from minimal example data, replacing traditional manual rule engineering. The system supports multi-modal tasks (text summarization, mathematical reasoning, code generation, etc.) and generates scenario-aware rules adaptively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why AutoPrinciple?\n",
    "Traditional manual rule engineering faces three critical limitations:\n",
    "\n",
    "- Poor Scalability: Manually designing rules for every task-scenario combination (e.g., 10 tasks × 5 scenarios = 50 rule sets) requires excessive human effort。\n",
    "\n",
    "- Subjective Bias: Human-defined rules often reflect individual cognitive biases (e.g., cultural differences in defining \"safe content\")。\n",
    "\n",
    "- Limited Adaptability: Static rules struggle to adapt to evolving model capabilities (e.g., new error patterns in upgraded models)\n",
    "\n",
    "\n",
    "AutoPrinciple's advantages:\n",
    "\n",
    "- Efficient Generation: Generates candidate rules in bulk via LLM (e.g., 5 samples × 5 candidates = 25 rules)\n",
    "\n",
    "- Dynamic Optimization: Uses clustering to extract core representative rules (e.g., compress 25 to 3 rules)\n",
    "\n",
    "- Cross-Domain Transfer: Applies the same framework to multi-modal tasks (e.g., \"syntax correctness\" for code → \"semantic fidelity\" for translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How AutoPrinciple Works\n",
    "\n",
    "The system operates through a streamlined three-step workflow (with optional iteration):\n",
    "\n",
    "- Candidate Principle Extraction from In-Distribution Data: Generate diverse candidate principles using task-specific in-distribution (ID) data.\n",
    "\n",
    "- High-Quality Principle Compression: Distill candidate principles into a compact, representative set, by applying semantic clustering to group similar candidates.\n",
    "\n",
    "- Iterative Optimization (Optional): Refine principles through evaluation feedback loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to Use AutoPrinciple\n",
    "Here we demonstrates how to use Principle Generator to create **Helpfulness** evaluation principles.\n",
    "\n",
    "Includes full workflow: Data loading → Model configuration → Principle generation → Result analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "# Add project root directory to Python path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Add environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"BASE_URL\"] = \"\"\n",
    "\n",
    "# Import local modules\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "from rm_gallery.core.reward.principle.generator import PrincipleGenerator\n",
    "from rm_gallery.core.utils.file import read_jsonl\n",
    "\n",
    "# Initialize logger\n",
    "from loguru import logger\n",
    "logger.add(\"principle_generator.log\", rotation=\"1 day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Using data from the \"Precise IF\" task as input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Data path (modify according to your actual path)\n",
    "    train_path = \"./data/Summarization Train.jsonl\"\n",
    "    test_path = \"./data/Summarization Test.jsonl\"\n",
    "    \n",
    "    # Read JSONL format data and convert to DataSample objects\n",
    "    train_samples = [DataSample(**sample) for sample in read_jsonl(train_path)]\n",
    "    test_samples = [DataSample(**sample) for sample in read_jsonl(test_path)]\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(train_samples)} training samples and {len(test_samples)} test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data loading failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure Generator Parameters\n",
    "\n",
    "- Using Qwen3 as the language model\n",
    "\n",
    "- Setting generation and clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize language model\n",
    "    llm = OpenaiLLM(\n",
    "        model=\"qwen3-235b-a22b\",  # Model name\n",
    "        enable_thinking=True      # Enable reasoning mode\n",
    "    )\n",
    "    \n",
    "    SCENARIO = \"Summarization: The text is compressed into a short form, retaining the main information, which is divided into extraction (directly selected from the original text) and production (rewriting the information).\"\n",
    "\n",
    "    # Create principle generator\n",
    "    generator = PrincipleGenerator( # or IterPrincipleGenerator\n",
    "        llm=llm,\n",
    "        scenario=SCENARIO,  # Scenario description\n",
    "        generate_number=5,   # Generate 5 candidate principles per sample\n",
    "        cluster_number=3     # Cluster to 3 representative principles\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Successfully initialized PrincipleGenerator\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Generator configuration failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Execute batch generation\n",
    "    principles = generator.run_batch(\n",
    "        train_samples[:10],  # Process first 10 samples as example\n",
    "        thread_pool=ThreadPoolExecutor(max_workers=12)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Successfully generated {len(principles)} principles\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Principle generation failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Generated Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm.alignment.base import BaseHelpfulnessListwiseReward\n",
    "\n",
    "try:\n",
    "    principles = [f\"{k}: {v}\" for k, v in principles.items()][:3]\n",
    "    reward = BaseHelpfulnessListwiseReward(\n",
    "        name=\"test_helpfulness_listwise_reward\",\n",
    "        llm=llm,\n",
    "        principles=principles,\n",
    "        scenario=SCENARIO\n",
    "    )\n",
    "    evaluation_samples = reward.evaluate_batch(samples=test_samples[:20])\n",
    "    logger.info(f\"Successfully evaluate test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Reward evaluation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results Analysis\n",
    "Analyze the accuracy rate of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def calc_acc(samples: List[DataSample]) -> float:\n",
    "    labels = []\n",
    "    for sample in samples:\n",
    "        labels.append(0)\n",
    "        for output in sample.output:\n",
    "            if output.answer.label[\"preference\"] == \"chosen\":\n",
    "                score = sum(r.score for r in output.answer.reward.details)\n",
    "                if score > 0:\n",
    "                    labels[-1] = 1\n",
    "    return sum(labels) / len(labels)\n",
    "\n",
    "logger.info(f\"Accuracy: {calc_acc(evaluation_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Reward Models Results\n",
    "Introduce our experimental result on built-in reward models with generated principles.\n",
    "\n",
    "\n",
    "### Setting\n",
    "\n",
    "The experimental setup compares two approaches across multiple scenarios:\n",
    "\n",
    "\n",
    "#### Experimental Configuration:\n",
    "\n",
    "Directly uses built-in reward models, which extend the base approach by integrating automatically generated principles via the AutoPrinciple. The generated principles may also be manually reviewed and lightly refined.\n",
    "\n",
    "##### Detailed Settings\n",
    "\n",
    "Models: Both configurations use qwen3-32b for evaluation, while principles are generated using qwen3-235b-a22b.\n",
    "\n",
    "Data: 10% of training samples are used to generate principles, and the remaining samples are evaluated.\n",
    "\n",
    "Metric: Accuracy, defined as the proportion of correctly preferred outputs based on reward scores, with 5-10 independent run.\n",
    "\n",
    "#### Baseline Configuration\n",
    "The baseline configuration uses only the built-in reward templates, removing all principles and related descriptions. This is designed to specifically evaluate the effectiveness of principles. Additionally, the evaluation model and metrics are consistent with the experimental group.\n",
    "The prompt is as follows:\n",
    "<details>\n",
    "<summary>Prompt</summary>\n",
    "# Task Description\\nYour role is that of a professional evaluation expert. I will provide you with a question and several candidate answers. Your task is to select the single best answer from the candidates.\\n\\n\\n\\n\\n\\n# Query\\n\\n\\n\\n# Answers\\n## Answer 1\\n## Answer 2\\n## Answer 3\\n## Answer 4\\n.\\n\\n\\n\\n# Output Requirement\\nNote: Ensure all outputs are placed within the tags like <tag> </tag> as required!!!\\n<best>\\nwhich answer is the best? just give the number here!!!\\n</best>\\n\\n\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation Results\n",
    "### RewardBench2\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"../../images/building_rm/rewardbench2_exp_result.png\" alt=\"RewardBench2\" width=\"75%\">\n",
    "</p>\n",
    "In the RewardBench2 dataset, principle-based reward models generally achieve higher accuracy across multiple subsets. However, the improvement is less pronounced in the Math scenario. Our preliminary hypothesis is that Math tasks rely more heavily on the base model's mathematical reasoning capabilities, which requires further investigation and validation.\n",
    "\n",
    "\n",
    "\n",
    "### RMBBench\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"../../images/building_rm/rmb_pairwise_exp_result.png\" alt=\"RMBBench\" width=\"75%\">\n",
    "</p>\n",
    "\n",
    "In the RMB Bench dataset, principle-based reward models consistently achieve higher accuracy across multiple subsets. We will continue to analyze these cases in depth. We will also further explore the effectiveness of principles in more scenarios in the future.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
