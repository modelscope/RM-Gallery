{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Generator Tutorial (Helpfulness)\n",
    "\n",
    "This notebook demonstrates how to use Principle Generator to create **Helpfulness** evaluation principles  \n",
    "Includes full workflow: Data loading → Model configuration → Principle generation → Result analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "# Add project root directory to Python path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Add environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"BASE_URL\"] = \"\"\n",
    "\n",
    "# Import local modules\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "from rm_gallery.core.reward.principle.base import PrincipleGenerator\n",
    "from rm_gallery.core.utils.file import read_jsonl\n",
    "\n",
    "# Initialize logger\n",
    "from loguru import logger\n",
    "logger.add(\"principle_generator.log\", rotation=\"1 day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load Data\n",
    "\n",
    "Using data from the \"Precise IF\" task as input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Data path (modify according to your actual path)\n",
    "    train_path = \"/mnt3/huangsen.huang/codes/RM-Gallery/data/RMBbench_Train/pairwise/Helpfulness/Summarization.jsonl\"\n",
    "    test_path = \"/mnt3/huangsen.huang/codes/RM-Gallery/data/RMBbench_Test/pairwise/Helpfulness/Summarization.jsonl\"\n",
    "    \n",
    "    # Read JSONL format data and convert to DataSample objects\n",
    "    train_samples = [DataSample(**sample) for sample in read_jsonl(train_path)]\n",
    "    test_samples = [DataSample(**sample) for sample in read_jsonl(test_path)]\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(train_samples)} training samples and {len(test_samples)} test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data loading failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configure Generator Parameters\n",
    "\n",
    "- Using Qwen3 as the language model\n",
    "- Setting generation and clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize language model\n",
    "    llm = OpenaiLLM(\n",
    "        model=\"qwen3-235b-a22b\",  # Model name\n",
    "        enable_thinking=True      # Enable reasoning mode\n",
    "    )\n",
    "    \n",
    "    SCENARIO = \"Summarization: The text is compressed into a short form, retaining the main information, which is divided into extraction (directly selected from the original text) and production (rewriting the information).\"\n",
    "\n",
    "    # Create principle generator\n",
    "    generator = PrincipleGenerator( # or IterPrincipleGenerator\n",
    "        llm=llm,\n",
    "        scenario=SCENARIO,  # Scenario description\n",
    "        generate_number=5,   # Generate 5 candidate principles per sample\n",
    "        cluster_number=3     # Cluster to 3 representative principles\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Successfully initialized PrincipleGenerator\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Generator configuration failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Execute batch generation\n",
    "    principles = generator.run_batch(\n",
    "        train_samples[:10],  # Process first 10 samples as example\n",
    "        thread_pool=ThreadPoolExecutor(max_workers=12)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Successfully generated {len(principles)} principles\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Principle generation failed: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evauluation with Generated Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm.alignment.base import BaseHelpfulnessListwiseReward\n",
    "\n",
    "try:\n",
    "    principles = [f\"{k}: {v}\" for k, v in principles.items()][:3]\n",
    "    reward = BaseHelpfulnessListwiseReward(\n",
    "        name=\"test_helpfulness_listwise_reward\",\n",
    "        llm=llm,\n",
    "        principles=principles,\n",
    "        scenario=SCENARIO\n",
    "    )\n",
    "    evaluation_samples = reward.evaluate_batch(samples=test_samples[:20])\n",
    "    logger.info(f\"Successfully evaluate test samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Reward evaluation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results Analysis\n",
    "Analyze the accuracy rate of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def calc_acc(samples: List[DataSample]) -> float:\n",
    "    labels = []\n",
    "    for sample in samples:\n",
    "        labels.append(0)\n",
    "        for output in sample.output:\n",
    "            if output.answer.label[\"preference\"] == \"chosen\":\n",
    "                score = sum(r.score for r in output.answer.reward.details)\n",
    "                if score > 0:\n",
    "                    labels[-1] = 1\n",
    "    return sum(labels) / len(labels)\n",
    "\n",
    "logger.info(f\"Accuracy: {calc_acc(evaluation_samples)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
