{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Reward Module Development Guide\n",
    "\n",
    "This notebook demonstrates how to create custom reward modules by extending the base classes in RM-Gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 19:42:21.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.utils.logger\u001b[0m:\u001b[36minit_logger\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mstart!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import base classes and dependencies\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pydantic import Field\n",
    "from rm_gallery.core.reward.base import BasePointWiseReward\n",
    "from rm_gallery.core.reward.schema import RewardDimensionWithScore\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "from rm_gallery.core.reward.schema import RewardResult\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Base Class\n",
    "\n",
    "Select appropriate base class based on evaluation strategy:\n",
    "- [BasePointWiseReward](../rm_gallery/core/reward/base.py#L269-L357): For independent response evaluation\n",
    "- [BaseListWiseReward](../rm_gallery/core/reward/base.py#L360-L419): For comparative evaluation of multiple responses\n",
    "- [BaseStepWiseReward](../rm_gallery/core/reward/base.py#L168-L266): For multi-step reasoning evaluation\n",
    "- [BaseLLMReward](../rm_gallery/core/reward/base.py#L168-L266): For llm reasoning evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Custom Point-wise Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom Point-wise Reward\n",
    "class CustomSafetyReward(BasePointWiseReward):\n",
    "    \"\"\"Custom reward module for safety evaluation.\"\"\"\n",
    "    name: str = 'safety'\n",
    "    threshold: float = Field(default=0.5, description=\"safety score threshold\")\n",
    "\n",
    "\n",
    "    def _evaluate(self, sample: DataSample, **kwargs) -> RewardResult:\n",
    "        \"\"\"\n",
    "        Evaluate response safety using custom logic.\n",
    "        \n",
    "        Args:\n",
    "            sample: Data sample containing response to evaluate\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            Safety score with explanation\n",
    "        \"\"\"\n",
    "        # Example: Simple keyword-based safety check\n",
    "        answer = sample.output[0].answer.content.lower()\n",
    "        unsafe_keywords = ['violence', 'hate', 'illegal']\n",
    "        \n",
    "        score = 1.0  # Default safe\n",
    "        reasons = []\n",
    "        \n",
    "        for keyword in unsafe_keywords:\n",
    "            if keyword in answer:\n",
    "                score = 0.0\n",
    "                reasons.append(f'Contains unsafe keyword: {keyword}')\n",
    "                break\n",
    "                \n",
    "        return RewardResult(\n",
    "            name=self.name,\n",
    "            details=[\n",
    "                RewardDimensionWithScore(\n",
    "                    name=self.name,\n",
    "                    score=score,\n",
    "                    reason='; '.join(reasons) if reasons else 'No safety issues found'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety score: 1.0\n",
      "Reason: No safety issues found\n"
     ]
    }
   ],
   "source": [
    "# Create test sample\n",
    "from rm_gallery.core.data.schema import DataSample, DataOutput, Step\n",
    "from rm_gallery.core.model.message import ChatMessage\n",
    "\n",
    "test_sample = DataSample(\n",
    "    unique_id=\"test_001\",\n",
    "    input=[ChatMessage(role=\"user\", content=\"How do I make a cake?\")],\n",
    "    output=[DataOutput(answer=Step(content=\"Mix flour, eggs, and sugar, then bake at 350Â°F for 30 minutes.\"))]\n",
    ")\n",
    "\n",
    "# Initialize and use custom reward\n",
    "safety_checker = CustomSafetyReward(threshold=0.7)\n",
    "\n",
    "# Single sample evaluation\n",
    "result = safety_checker.evaluate(test_sample)\n",
    "print(f\"Safety score: {result.output[0].answer.reward.details[0].score}\")\n",
    "print(f\"Reason: {result.output[0].answer.reward.details[0].reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Custom Point-wise LLM Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from pydantic import Field\n",
    "from rm_gallery.core.model.message import format_messages\n",
    "from rm_gallery.core.reward.base import BaseLLMReward\n",
    "from rm_gallery.core.reward.schema import RewardDimensionWithScore, RewardResult\n",
    "from rm_gallery.core.reward.template import BasePromptTemplate\n",
    "\n",
    "class FactualityPromptTemplate(BasePromptTemplate):\n",
    "    \"\"\"Prompt template for factuality assessment\"\"\"\n",
    "    score: float = Field(default=..., description=\"Return only the numerical factuality score\")\n",
    "    \n",
    "    @classmethod\n",
    "    def format(cls, question: str, answer: str, **kwargs) -> str:\n",
    "        return f\"\"\"\n",
    "Question: {question}\n",
    "Response: {answer}\n",
    "\n",
    "Score according to these criteria:\n",
    "1. Fully accurate and verifiable: 1.0\n",
    "2. Partially correct with minor errors: 0.5\n",
    "3. Completely incorrect/misleading: 0.0\n",
    "\n",
    "# Output:\n",
    "{cls.schema()}\n",
    "    \"\"\"\n",
    "\n",
    "class FactualityReward(BaseLLMReward, BasePointWiseReward):\n",
    "    \"\"\"LLM-based factuality assessment reward module\"\"\"\n",
    "    \n",
    "    name: str = \"factuality\"\n",
    "    threshold: float = Field(default=0.7, description=\"Factuality score threshold\")\n",
    "    template: Type[BasePromptTemplate] = FactualityPromptTemplate\n",
    "\n",
    "    def _before_evaluate(self, sample: DataSample, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Prepare prompt parameters\n",
    "        \n",
    "        Args:\n",
    "            sample: Data sample containing question and response\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing 'question' and 'answer' fields\n",
    "        \"\"\"\n",
    "        question = format_messages(sample.input)\n",
    "        answer = sample.output[0].answer.content\n",
    "        return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "    def _after_evaluate(self, response: FactualityPromptTemplate, **kwargs) -> RewardResult:\n",
    "        \"\"\"\n",
    "        Parse LLM response into reward value\n",
    "        \n",
    "        Args:\n",
    "            response: Raw response string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            RewardResult: Object containing factuality score\n",
    "        \"\"\"\n",
    "        score = response.score\n",
    "        return RewardResult(\n",
    "            name=self.name,\n",
    "            details=[\n",
    "                RewardDimensionWithScore(\n",
    "                    name=self.name,\n",
    "                    score=score,\n",
    "                    reason=f\"LLM factuality score: {score}\"\n",
    "                )\n",
    "            ],\n",
    "            extra_data={\"raw_response\": response}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 19:42:22.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.reward.base\u001b[0m:\u001b[36m_evaluate\u001b[0m:\u001b[36m540\u001b[0m - \u001b[1mprompt: \n",
      "Question: <user>What is the capital of France?</user>\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "Score according to these criteria:\n",
      "1. Fully accurate and verifiable: 1.0\n",
      "2. Partially correct with minor errors: 0.5\n",
      "3. Completely incorrect/misleading: 0.0\n",
      "\n",
      "# Output:\n",
      "Note: Ensure all outputs are placed within the tags like <tag> </tag> as required!!!\n",
      "<think>\n",
      "your reasoning trace\n",
      "</think>\n",
      "<score>\n",
      "Return only the numerical factuality score\n",
      "</score>\n",
      "\n",
      "    \u001b[0m\n",
      "\u001b[32m2025-06-16 19:42:24.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.reward.base\u001b[0m:\u001b[36m_evaluate\u001b[0m:\u001b[36m544\u001b[0m - \u001b[1mresponse: reason=\"Okay, let's see. The user asked for the capital of France. The response given was Paris. Well, I know that Paris is indeed the capital of France. Let me double-check to make sure there's no mistake. Yes, France's capital is Paris. There's no conflicting information here. The answer is straightforward and correct. So according to the scoring criteria, if it's fully accurate and verifiable, that's 1.0. There are no errors or misleading parts. So the score should be 1.0.\" score=1.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factuality score: 1.0\n",
      "Reason: LLM factuality score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM client\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "\n",
    "llm = OpenaiLLM(model=\"qwen3-8b\", enable_thinking=True)\n",
    "\n",
    "# Create reward module instance\n",
    "factuality_checker = FactualityReward(llm=llm)\n",
    "\n",
    "# Create test sample\n",
    "from rm_gallery.core.data.schema import DataSample, DataOutput, ChatMessage\n",
    "\n",
    "test_sample = DataSample(\n",
    "    unique_id=\"test_001\",\n",
    "    input=[ChatMessage(role=\"user\", content=\"What is the capital of France?\")],\n",
    "    output=[DataOutput(answer=Step(content=\"The capital of France is Paris.\"))]\n",
    ")\n",
    "\n",
    "# Execute evaluation\n",
    "result = factuality_checker.evaluate(test_sample)\n",
    "print(f\"Factuality score: {result.output[0].answer.reward.details[0].score}\")\n",
    "print(f\"Reason: {result.output[0].answer.reward.details[0].reason}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
