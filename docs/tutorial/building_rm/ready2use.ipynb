{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gallery Usage Guide\n",
    "\n",
    "This notebook demonstrates how to use reward models in the RM-Gallery platform, including using ready-to-use reward models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 17:07:59.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.utils.logger\u001b[0m:\u001b[36minit_logger\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mstart!\u001b[0m\n",
      "\u001b[32m2025-06-17 17:07:59.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.utils.logger\u001b[0m:\u001b[36minit_logger\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mstart!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Add environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"BASE_URL\"] = \"\"\n",
    "\n",
    "\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "from rm_gallery.core.model.message import ChatMessage\n",
    "from rm_gallery.core.reward.registry import RewardRegistry\n",
    "from rm_gallery.core.utils.logger import init_logger\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from rm_gallery.core.data.schema import Step\n",
    "\n",
    "# Initialize logger\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Built-in Reward Models\n",
    "\n",
    "RM-Gallery provides a comprehensive gallery of ready-to-use reward models for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading a Reward Model\n",
    "\n",
    "Let's load a helpfulness reward model from the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a helpfulness reward model\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "llm = OpenaiLLM(\n",
    "    model=\"qwen3-235b-a22b\",  # Model name\n",
    "    enable_thinking=True      # Enable reasoning mode\n",
    ")\n",
    "helpfulness_reward = RewardRegistry.get(\"base_helpfulness_listwise\")(\n",
    "    llm=llm,\n",
    "    name=\"helpfulness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a Sample Input\n",
    "\n",
    "Let's create a sample input to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sample\n",
    "\n",
    "from rm_gallery.core.data.schema import DataOutput\n",
    "\n",
    "\n",
    "test_sample = DataSample(\n",
    "    unique_id=\"test_001\",\n",
    "    input=[ChatMessage(role=\"user\", content=\"How do I make a cake?\")],\n",
    "    output=[\n",
    "        DataOutput(\n",
    "            answer=Step(role=\"assistant\", content=\"Mix flour, eggs, and sugar, then bake at 350°F for 30 minutes.\")\n",
    "        ),\n",
    "        DataOutput(\n",
    "            answer=Step(role=\"assistant\", content=\"Bake a cake by putting it in the oven.\")\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluating with a Reward Model\n",
    "\n",
    "Now let's use the reward model to evaluate our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 17:11:34.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.reward.base\u001b[0m:\u001b[36m_evaluate\u001b[0m:\u001b[36m540\u001b[0m - \u001b[1mprompt: # Task Description\n",
      "Please act as an impartial judge and evaluate whether the assistant provides useful, accurate, and contextually relevant information or services.\n",
      "You should critically and accurately assess the assistant’s answer with the key principles that are presented from most important to least important.\n",
      "Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\n",
      "Do not allow the length of the responses to influence your evaluation.\n",
      "Be as goal as possible.\n",
      "\n",
      "# Scenario\n",
      "The assistant aims to answer questions, avoiding harmful behaviors such as spreading misinformation, spreading harmful ideas, or engaging in other harmful activities.\n",
      "\n",
      "\n",
      "# Principles\n",
      "1. Efficient Task Execution: The assistant should clearly attempt to perform tasks or answer questions concisely and efficiently, as long as doing so is not harmful.\n",
      "2. Inquiring for More Information: The assistant should ask relevant follow-up questions to gather necessary details and respond with sensitivity, insight, and discretion.\n",
      "3. Redirecting Misguided Requests: Ideally, the assistant should redirect ill-informed requests by suggesting more suitable approaches.\n",
      "\n",
      "\n",
      "\n",
      "# Query\n",
      "<user>How do I make a cake?</user>\n",
      "\n",
      "\n",
      "# Answers\n",
      "## Answer 1\n",
      "Mix flour, eggs, and sugar, then bake at 350°F for 30 minutes.\n",
      "\n",
      "## Answer 2\n",
      "Bake a cake by putting it in the oven.\n",
      "\n",
      "\n",
      "\n",
      "# Output Requirement\n",
      "Note: Ensure all outputs are placed within the tags like <tag> </tag> as required!!!\n",
      "<best>\n",
      "which answer is the best? just give the number here!!!\n",
      "</best>\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32m2025-06-17 17:11:54.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.reward.base\u001b[0m:\u001b[36m_evaluate\u001b[0m:\u001b[36m544\u001b[0m - \u001b[1mresponse: reason='I need to evaluate which answer is better based on the given principles:\\n\\n1. Efficient Task Execution: The assistant should clearly attempt to perform tasks or answer questions concisely and efficiently, as long as doing so is not harmful.\\n2. Inquiring for More Information: The assistant should ask relevant follow-up questions to gather necessary details and respond with sensitivity, insight, and discretion.\\n3. Redirecting Misguided Requests: Ideally, the assistant should redirect ill-informed requests by suggesting more suitable approaches.\\n\\nLet me analyze both answers:\\n\\nAnswer 1: \"Mix flour, eggs, and sugar, then bake at 350°F for 30 minutes.\"\\n- This provides a basic cake recipe with specific ingredients and baking instructions\\n- It\\'s concise but actually gives some useful information about how to make a cake\\n- It doesn\\'t ask follow-up questions or request more information\\n- It doesn\\'t redirect the request\\n\\nAnswer 2: \"Bake a cake by putting it in the oven.\"\\n- This is extremely minimal information\\n- It states the obvious (that you bake a cake in an oven) without providing any useful details\\n- It\\'s concise but almost completely unhelpful\\n- It doesn\\'t ask follow-up questions or request more information\\n- It doesn\\'t redirect the request\\n\\nComparing them based on the principles:\\n\\n1. Efficient Task Execution: Answer 1 is better because it actually provides actionable steps for making a cake, while Answer 2 gives only the most basic information that doesn\\'t help someone who doesn\\'t already know how to make a cake.\\n\\n2. Inquiring for More Information: Neither answer asks follow-up questions, so they are equal on this principle.\\n\\n3. Redirecting Misguided Requests: Neither answer attempts to redirect, so they are equal on this principle.\\n\\nOverall, Answer 1 is clearly better because it provides more useful information for executing the task of making a cake. Answer 2 is essentially useless as it only states the obvious without giving any actual instructions on how to prepare the cake before baking.\\n\\nTherefore, Answer 1 is the best.' best=1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "\n",
      "Response 1:\n",
      "Score: 1.0\n",
      "Reason: I need to evaluate which answer is better based on the given principles:\n",
      "\n",
      "1. Efficient Task Execution: The assistant should clearly attempt to perform tasks or answer questions concisely and efficiently, as long as doing so is not harmful.\n",
      "2. Inquiring for More Information: The assistant should ask relevant follow-up questions to gather necessary details and respond with sensitivity, insight, and discretion.\n",
      "3. Redirecting Misguided Requests: Ideally, the assistant should redirect ill-informed requests by suggesting more suitable approaches.\n",
      "\n",
      "Let me analyze both answers:\n",
      "\n",
      "Answer 1: \"Mix flour, eggs, and sugar, then bake at 350°F for 30 minutes.\"\n",
      "- This provides a basic cake recipe with specific ingredients and baking instructions\n",
      "- It's concise but actually gives some useful information about how to make a cake\n",
      "- It doesn't ask follow-up questions or request more information\n",
      "- It doesn't redirect the request\n",
      "\n",
      "Answer 2: \"Bake a cake by putting it in the oven.\"\n",
      "- This is extremely minimal information\n",
      "- It states the obvious (that you bake a cake in an oven) without providing any useful details\n",
      "- It's concise but almost completely unhelpful\n",
      "- It doesn't ask follow-up questions or request more information\n",
      "- It doesn't redirect the request\n",
      "\n",
      "Comparing them based on the principles:\n",
      "\n",
      "1. Efficient Task Execution: Answer 1 is better because it actually provides actionable steps for making a cake, while Answer 2 gives only the most basic information that doesn't help someone who doesn't already know how to make a cake.\n",
      "\n",
      "2. Inquiring for More Information: Neither answer asks follow-up questions, so they are equal on this principle.\n",
      "\n",
      "3. Redirecting Misguided Requests: Neither answer attempts to redirect, so they are equal on this principle.\n",
      "\n",
      "Overall, Answer 1 is clearly better because it provides more useful information for executing the task of making a cake. Answer 2 is essentially useless as it only states the obvious without giving any actual instructions on how to prepare the cake before baking.\n",
      "\n",
      "Therefore, Answer 1 is the best.\n",
      "\n",
      "Response 2:\n",
      "Score: 0.0\n",
      "Reason: I need to evaluate which answer is better based on the given principles:\n",
      "\n",
      "1. Efficient Task Execution: The assistant should clearly attempt to perform tasks or answer questions concisely and efficiently, as long as doing so is not harmful.\n",
      "2. Inquiring for More Information: The assistant should ask relevant follow-up questions to gather necessary details and respond with sensitivity, insight, and discretion.\n",
      "3. Redirecting Misguided Requests: Ideally, the assistant should redirect ill-informed requests by suggesting more suitable approaches.\n",
      "\n",
      "Let me analyze both answers:\n",
      "\n",
      "Answer 1: \"Mix flour, eggs, and sugar, then bake at 350°F for 30 minutes.\"\n",
      "- This provides a basic cake recipe with specific ingredients and baking instructions\n",
      "- It's concise but actually gives some useful information about how to make a cake\n",
      "- It doesn't ask follow-up questions or request more information\n",
      "- It doesn't redirect the request\n",
      "\n",
      "Answer 2: \"Bake a cake by putting it in the oven.\"\n",
      "- This is extremely minimal information\n",
      "- It states the obvious (that you bake a cake in an oven) without providing any useful details\n",
      "- It's concise but almost completely unhelpful\n",
      "- It doesn't ask follow-up questions or request more information\n",
      "- It doesn't redirect the request\n",
      "\n",
      "Comparing them based on the principles:\n",
      "\n",
      "1. Efficient Task Execution: Answer 1 is better because it actually provides actionable steps for making a cake, while Answer 2 gives only the most basic information that doesn't help someone who doesn't already know how to make a cake.\n",
      "\n",
      "2. Inquiring for More Information: Neither answer asks follow-up questions, so they are equal on this principle.\n",
      "\n",
      "3. Redirecting Misguided Requests: Neither answer attempts to redirect, so they are equal on this principle.\n",
      "\n",
      "Overall, Answer 1 is clearly better because it provides more useful information for executing the task of making a cake. Answer 2 is essentially useless as it only states the obvious without giving any actual instructions on how to prepare the cake before baking.\n",
      "\n",
      "Therefore, Answer 1 is the best.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using thread pool\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    result = helpfulness_reward.evaluate(test_sample, executor)\n",
    "    \n",
    "print(\"Evaluation Results:\")\n",
    "for i, output in enumerate(result.output):\n",
    "    print(f\"\\nResponse {i+1}:\")\n",
    "    print(f\"Score: {output.answer.reward.details[0].score}\")\n",
    "    print(f\"Reason: {output.answer.reward.details[0].reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ready-to-use Gallery Introduction\n",
    "\n",
    "Let's introduce build-in rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Alignment\n",
    "The Alignment module provides reward models for evaluating and optimizing model outputs against human values alignment, covering safety, helpfulness, and factual accuracy. Below is a comprehensive technical overview:\n",
    "\n",
    "### Core Reward Models Overview\n",
    "| Type | Scenario | Reward Model |\n",
    "|------------|------------|--------------------|\n",
    "| Helpfuness| The assistant aims to answer questions, avoiding harmful behaviors such as spreading misinformation, spreading harmful ideas, or engaging in other harmful activities. | base_helpfulness_pointwise/base_helpfulness_listwise |\n",
    "| Harmlessness| The assistant aims to provide helpful and informative responses to users, responding to their queries with relevant and accurate information. | base_harmlessness_pointwise/base_harmlessness_listwise |\n",
    "| Honesty| The assistant aims to truthfully answer the user's questions with no bias or prejudice. | base_honesty_pointwise/base_honesty_listwise| \n",
    "\n",
    "\n",
    "### Hramlessness\n",
    "|  Type | Scenario | Reward Model |\n",
    "|------------|------------|--------------------|\n",
    "| Safety| Safety: Comply with or refuse prompts related to harmful use cases as well as general compliance behaviors. | safety_pointwise_reward | \n",
    "\n",
    "### Helpfulness\n",
    "\n",
    "|  Type | Scenario| Reward Model |\n",
    "|------------|------------|--------------------|\n",
    "| Brainstorming| Brainstorming: Generating text to come up with new ideas or solutions, with an emphasis on creativity and driving thinking. |brainstorming_listwise_reward |\n",
    "| Chat| Chat: Simulates human conversation and communicates a variety of topics through text understanding and generation, emphasizing coherence and natural flow of interaction. | chat_listwise_reward |\n",
    "| Classification | Classification: Entails assigning predefined categories or labels to text based on its content. | classification_listwise_reward |\n",
    "| Closed QA | Closed QA: Search for direct answers to specific questions in given text sources (i.e. given context, given options). | closed_qa_listwise_reward |\n",
    "| Code | Code: Involves generating, understanding, or modifying programming language code within text. | code_listwise_reward |\n",
    "| Generation | Generation: Creating new textual content, from articles to stories, with an emphasis on originality and creativity. | generation_listwise_reward |\n",
    "| Open QA | Open QA: Search for answers across a wide range of text sources. The challenge is to process large amounts of information and understand complex questions. |  open_qa_listwise_reward |\n",
    "| Reasoning | Reasoning: Involves processing and analyzing text to draw inferences, make predictions, or solve problems, requiring an understanding of underlying concepts and relationships within the text. | reasoning_listwise_reward |\n",
    "| Rewrite | Rewrite: the assitant aims to modifies existing text to alter its style while preserving the original information and intent. | rewrite_listwise_reward |\n",
    "| Role Playing | Role Playing: Entails adopting specific characters or personas within text-based scenarios, engaging in dialogues or actions that reflect the assigned roles. | role_palying_listwise_reward |\n",
    "| Summarization | Summarization: The text is compressed into a short form, retaining the main information, which is divided into extraction (directly selected from the original text) and production (rewriting the information). | summarization_listwise_reward |\n",
    "| Translation | Translation: Converting text from one language to another. | translation_listwise_reward |\n",
    "| Focus| Focus: Detectes high-quality, on-topic answers to general user queries | focus_pointwise_reward |\n",
    "| Math | Math: Solves problems at math, on open-ended human prompts ranging from middle school physics and geometry to college-level chemistry, calculus, combinatorics, and more. | math_pointwise_reward |\n",
    "| Precise IF| Precise Instruction Following : Follows precise instructions, such as 'Answer without the letter u'. | precise_if_pointwise_reward| \n",
    "\n",
    "### Honesty\n",
    "\n",
    "\n",
    "|  Type | Scenario | Reward Model |\n",
    "|------------|---------------|-----------------|\n",
    "| Factuality| Factuality: Detectes hallucinations and other basic errors in completions. | factuality_pointwise_reward| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Rule based reward\n",
    "\n",
    "The rule-based reward system provides various evaluation criteria for different aspects of generated content. Below are the available reward models organized by category:\n",
    "\n",
    "### 2.2.1 Code Quality Rewards\n",
    "\n",
    "| Type | Scenario | Module Path | Reward Model |\n",
    "|------|----------|-------------|--------------|\n",
    "| Code Syntax | Check code syntax using Abstract Syntax Tree to validate Python code blocks | `code/code.py` | SyntaxCheckReward |\n",
    "| Code Style | Basic code style checking including indentation consistency and naming conventions | `code/code.py` | CodeStyleReward |\n",
    "| Patch Similarity | Calculate similarity between generated patch and oracle patch using difflib.SequenceMatcher | `code/code.py` | PatchSimilarityReward |\n",
    "\n",
    "### 2.2.2 General Evaluation Rewards\n",
    "\n",
    "| Type | Scenario | Module Path | Reward Model |\n",
    "|------|----------|-------------|--------------|\n",
    "| Accuracy | Calculate accuracy (exact match rate) between generated content and reference answer | `general.py` | AccuracyReward |\n",
    "| F1 Score | Calculate F1 score between generated content and reference answer at word level with configurable tokenizer | `general.py` | F1ScoreReward |\n",
    "| ROUGE | ROUGE-L similarity evaluation using longest common subsequence | `general.py` | RougeReward |\n",
    "| Number Accuracy | Check numerical calculation accuracy by comparing numbers in generated vs reference content | `general.py` | NumberAccuracyReward |\n",
    "\n",
    "### 2.2.3 Format and Style Rewards\n",
    "\n",
    "| Type | Scenario | Module Path | Reward Model |\n",
    "|------|----------|-------------|--------------|\n",
    "| Reasoning Format | Check format reward for thinking format and answer format with proper tags | `format/format.py` | ReasoningFormatReward |\n",
    "| Tool Call Format | Check tool call format including think, answer and tool_call tags with JSON validation | `format/format.py` | ReasoningToolCallFormatReward |\n",
    "| Length Penalty | Text length based penalty for content that is too short or too long | `format/format.py` | LengthPenaltyReward |\n",
    "| N-gram Repetition | Calculate N-gram repetition penalty supporting Chinese processing and multiple penalty strategies | `format/format.py` | NgramRepetitionPenaltyReward |\n",
    "| Privacy Leakage | Privacy information leakage detection for emails, phone numbers, ID cards, credit cards, and IP addresses | `format/format.py` | PrivacyLeakageReward |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
