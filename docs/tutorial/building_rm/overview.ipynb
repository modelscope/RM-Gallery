{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline: From Data to Reward\n",
    "\n",
    "This notebook demonstrates a complete workflow following these steps:\n",
    "1. **Data Loading** - Load dataset from source\n",
    "2. **Data Splitting** - Split into training and test sets\n",
    "3. **Principle Generation** - Auto-generate evaluation principles from training set\n",
    "4. **Reward Definition** - Define reward function based on generated principles\n",
    "5. **Reward Testing** - Evaluate reward function on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")  # Add parent directory to path\n",
    "\n",
    "from rm_gallery.core.reward.principle.generator import PrincipleGenerator\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"BASE_URL\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We'll start by loading our dataset using the flexible data loading module.\n",
    "You can read more from [Data Loading](./tutorial/data/load.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000 data items\n"
     ]
    }
   ],
   "source": [
    "# Implementation by creating base class\n",
    "from rm_gallery.core.data.load.base import DataLoad\n",
    "import rm_gallery.core.data     # Core strategy registration\n",
    "import rm_gallery.gallery.data  # Extended strategy registration\n",
    "\n",
    "\n",
    "# Configure local file loading parameters\n",
    "config = {\n",
    "    \"path\": \"./data/reward-bench-2/data/test-00000-of-00001.parquet\",\n",
    "    \"limit\": 1000,  # Limit the number of data items to load\n",
    "}\n",
    "\n",
    "# Create data loader\n",
    "loader = DataLoad(\n",
    "    name=\"rewardbench2\",           # Dataset name\n",
    "    load_strategy_type=\"local\",    # Use local file loading strategy\n",
    "    data_source=\"rewardbench2\",    # Specify data source format converter\n",
    "    config=config                  # Pass configuration parameters\n",
    ")\n",
    "\n",
    "# Execute data loading\n",
    "dataset = loader.run()\n",
    "\n",
    "# Output dataset size\n",
    "print(f\"Successfully loaded {len(dataset)} data items\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into Training and Test Sets\n",
    "\n",
    "Let's split our dataset into training and test sets for principle generation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 100\n",
      "Test set size: 900\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "from rm_gallery.core.utils.file import split_samples\n",
    "\n",
    "train_samples, test_samples = split_samples(dataset.datas)\n",
    "\n",
    "print(f\"Training set size: {len(train_samples)}\")\n",
    "print(f\"Test set size: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auto Generate Principles from Training Set\n",
    "\n",
    "Now we'll use the principle generator to extract evaluation principles from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face LLM client (can be replaced with other LLM implementations)\n",
    "llm = OpenaiLLM(\n",
    "    model=\"qwen3-235b-a22b\",\n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "# Initialize principle generator\n",
    "principle_generator = PrincipleGenerator(\n",
    "    llm=llm,\n",
    "    scenario=\"chat assistant evaluation\",\n",
    "    generate_number=5,  # Generate up to 5 principles per sample\n",
    "    cluster_number=3    # Cluster to 3 final principles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Principles:\n",
      "1. Adherence to Scientific Accuracy and Ethical Responsibility: Prioritize alignment with established evidence and reject harmful misinformation, especially in critical domains like public health or climate science.\n",
      "2. Technical Precision and Contextual Clarity: Ensure factual correctness, unambiguous terminology, and structured explanations tailored to the query's domain (scientific, medical, technical).\n",
      "3. Respect for Privacy and Legal/Ethical Boundaries: Avoid disclosing sensitive information, comply with data protection laws, and refuse requests that violate ethical norms or societal harm.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Create thread pool executor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    # Generate principles across training set\n",
    "    principles = principle_generator.run_batch(train_samples[:10], executor)\n",
    "    \n",
    "print(\"Generated Principles:\")\n",
    "for i, (key, value) in enumerate(principles.items(), 1):\n",
    "    print(f\"{i}. {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Reward Function Using Generated Principles\n",
    "\n",
    "Let's define a custom reward function that incorporates our generated principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rm_gallery.gallery.rm.alignment.base import BaseHelpfulnessListwiseReward\n",
    "\n",
    "reward_module = BaseHelpfulnessListwiseReward(\n",
    "    name=\"demo\",\n",
    "    principles=[f\"{key}: {value}\" for key, value in principles.items()],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Reward Function on Test Set\n",
    "\n",
    "Now we'll evaluate our reward function on the test set and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rewards for test set\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=128) as executor:\n",
    "    test_samples = reward_module.evaluate_batch(samples=test_samples[:100], thread_pool=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from rm_gallery.core.data.schema import DataSample\n",
    "\n",
    "\n",
    "def calc_acc(samples: List[DataSample]):\n",
    "    labels = []\n",
    "    for sample in samples:\n",
    "        for output in sample.output:\n",
    "            if (\n",
    "                output.answer.label[\"preference\"] == \"chosen\"\n",
    "                and output.answer.reward.details\n",
    "            ):\n",
    "                score = sum(r.score for r in output.answer.reward.details)\n",
    "                if score > 0:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "    return sum(labels) / len(labels)\n",
    "\n",
    "\n",
    "acc = calc_acc(test_samples)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
