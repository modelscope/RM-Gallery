{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERL-based Reasoning Reward Model Training Guide\n",
    "\n",
    "## üìñ Overview\n",
    "\n",
    "This document provides a comprehensive guide for training Reasoning Reward Models using the VERL framework. Through this tutorial, you will learn how to configure the environment, prepare data, design reward functions, and execute the training pipeline.\n",
    "\n",
    "## üèóÔ∏è System Architecture\n",
    "\n",
    "### Core Components\n",
    "\n",
    "The VERL reasoning reward model training system consists of three core components:\n",
    "\n",
    "#### 1. **Training Dataset** - Inherits from `BaseTrainDataset`\n",
    "   - Supports multiple data formats and evaluation criteria\n",
    "   - Provides flexible conversation template system\n",
    "   - Integrates custom reward functions\n",
    "\n",
    "#### 2. **Prompt Template** - Based on `BasePromptTemplate`\n",
    "   - Defines structured output format\n",
    "   - Supports extensible scoring criteria\n",
    "   - Adapts to various task types\n",
    "\n",
    "#### 3. **Reward Function** - Customizable reward computation module\n",
    "   - Supports multiple reward calculation methods (pointwise, pairwise, etc.)\n",
    "   - Provides flexible evaluation metric configuration\n",
    "   - Real-time accuracy and reward score statistics\n",
    "\n",
    "## üîß Environment Configuration\n",
    "\n",
    "### Runtime Requirements\n",
    "\n",
    "Create a `runtime_env.yaml` configuration file:\n",
    "\n",
    "```yaml\n",
    "# runtime_env.yaml\n",
    "excludes: [\\\"/.git/\\\"]\n",
    "env_vars:\n",
    "  TORCH_NCCL_AVOID_RECORD_STREAMS: \\\"1\\\"\n",
    "  PYTORCH_CUDA_ALLOC_CONF: \\\"expandable_segments: False\\\"\n",
    "  WANDB_API_KEY: \\\"your_wandb_api_key\\\"\n",
    "  WANDB_BASE_URL: \\\"your_wandb_base_url\\\"\n",
    "  HYDRA_FULL_ERROR: \\\"1\\\"\n",
    "```\n",
    "\n",
    "### Dependency Installation\n",
    "\n",
    "Ensure the following essential dependencies are installed:\n",
    "- `verl==0.4.0` (core framework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "### Step 1: Prepare Training Data\n",
    "\n",
    "Training data should conform to the `DataSample` format specification. For detailed data loading and preprocessing steps, please refer to the data loading section.\n",
    "\n",
    "### Step 2: Start Ray Distributed Cluster\n",
    "\n",
    "#### Master Node Startup:\n",
    "```bash\n",
    "ray start --head --node-ip-address $MASTER_ADDR --num-gpus 8\n",
    "```\n",
    "\n",
    "#### Worker Nodes Startup:\n",
    "```bash\n",
    "ray start --address=$MASTER_ADDR:6379 --num-gpus 8\n",
    "```\n",
    "\n",
    "### Step 3: Execute Training Pipeline\n",
    "\n",
    "```bash\n",
    "# Navigate to training directory\n",
    "cd rm_gallery/gallery/train/<your_method>\n",
    "\n",
    "# Start training script\n",
    "bash run_train.sh\n",
    "```\n",
    "\n",
    "### Data Format Description\n",
    "\n",
    "- **Input Data Format**: All input data must conform to the `DataSample` format\n",
    "\n",
    "## üß© Core Component Details\n",
    "\n",
    "### Custom Training Dataset\n",
    "\n",
    "Here's a complete implementation example of a custom training dataset:\n",
    "\n",
    "```python\n",
    "class CustomTrainDataset(BaseTrainDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize reward module\n",
    "        self.reward_module = YourRewardModule(\n",
    "            name=\"custom_reward\",\n",
    "            template=YourTemplate,\n",
    "            examples=self._get_examples(),\n",
    "            llm=None,\n",
    "        )\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _build_messages(self, example):\n",
    "        # Build formatted messages\n",
    "        result = self.reward_module.format(sample=example)\n",
    "        return [{\"role\": \"user\", \"content\": result}]\n",
    "```\n",
    "\n",
    "> **Important Note: Reasoning Model Configuration**\n",
    "> \n",
    "> When training reasoning reward models, pay attention to the following configuration:\n",
    "> - For reasoning models (e.g., Qwen3):\n",
    ">   - `apply_chat_template` with `enable_thinking=True`\n",
    ">   - `format` with `enable_thinking=False`\n",
    "> - For non-reasoning models:\n",
    ">   - `apply_chat_template` with `enable_thinking=False`\n",
    ">   - `format` with `enable_thinking=True`\n",
    ">\n",
    "> ```python\n",
    "> # Reasoning model configuration example\n",
    "> self.tokenizer.apply_chat_template(\n",
    ">     messages, add_generation_prompt=True, tokenize=False, enable_thinking=True\n",
    "> )\n",
    "> \n",
    "> result = self.helpfulness_reward.format(sample=example, enable_thinking=False)\n",
    "> ```\n",
    "\n",
    "### Reward Function Design\n",
    "\n",
    "The reward function is a key component for evaluating model performance:\n",
    "\n",
    "```python\n",
    "def calculate_reward(predicted_score, true_score):\n",
    "    \"\"\"\n",
    "    Custom reward function\n",
    "    \n",
    "    Args:\n",
    "        predicted_score (float): Model predicted score\n",
    "        true_score (float): Ground truth score\n",
    "        \n",
    "    Returns:\n",
    "        float: Calculated reward value\n",
    "    \"\"\"\n",
    "    if true_score is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Reward calculation based on absolute error\n",
    "    abs_error = abs(predicted_score - true_score)\n",
    "    max_error = 4  # Adjust based on scoring range (e.g., 0-4 scale)\n",
    "    \n",
    "    # Linear decay reward function\n",
    "    reward = 1.0 - (abs_error / max_error)\n",
    "    return max(0.0, reward)  # Ensure non-negative reward\n",
    "```\n",
    "\n",
    "### Prompt Template System\n",
    "\n",
    "The template system defines the structured format for model input and output:\n",
    "\n",
    "```python\n",
    "class YourTemplate(BasePromptTemplate):\n",
    "    score: int = Field(description=\"Scoring result (0-4 scale)\")\n",
    "    \n",
    "    @classmethod\n",
    "    def format(cls, desc, examples, query, answer, **kwargs):\n",
    "        \"\"\"\n",
    "        Format prompt template\n",
    "        \n",
    "        Args:\n",
    "            desc (str): Task description\n",
    "            examples (str): Example data\n",
    "            query (str): User query\n",
    "            answer (str): Model response\n",
    "        \n",
    "        Returns:\n",
    "            str: Formatted prompt text\n",
    "        \"\"\"\n",
    "        return f\"\"\"# Task Description\n",
    "{desc}\n",
    "\n",
    "# Reference Examples\n",
    "{examples}\n",
    "\n",
    "# User Query\n",
    "{query}\n",
    "\n",
    "# Model Response\n",
    "{answer}\n",
    "\n",
    "# Output Requirements\n",
    "{cls.schema(**kwargs)}\n",
    "        \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Implementation Example: Pointwise Training\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "```\n",
    "rm_gallery/gallery/train/pointwise/\n",
    "‚îú‚îÄ‚îÄ run_pointwise.sh    # Training launch script\n",
    "‚îú‚îÄ‚îÄ dataset.py          # Dataset implementation\n",
    "‚îú‚îÄ‚îÄ reward_fn.py        # Reward function implementation\n",
    "‚îú‚îÄ‚îÄ template.py         # Prompt template\n",
    "‚îî‚îÄ‚îÄ runtime_env.yaml    # Training configuration file\n",
    "```\n",
    "\n",
    "### Experiment Configuration\n",
    "\n",
    "#### Dataset Selection\n",
    "- **Data Source**: `nvidia/helpsteer2` dataset\n",
    "- **Evaluation Dimension**: `helpfulness` annotation information\n",
    "- **Data Scale**: ~8K samples for training set, ~8K samples for validation set\n",
    "- **Scoring Range**: 0-4 scale (0=worst, 4=best)\n",
    "\n",
    "#### Model Configuration\n",
    "- **Base Model**: `Qwen3-8B`\n",
    "\n",
    "#### Training Parameters\n",
    "For detailed training parameter configuration, please check: `./gallery/train/pointwise/run_pointwise.sh`\n",
    "\n",
    "### Pointwise Training Dataset\n",
    "\n",
    "```python\n",
    "class PointwiseTrainDataset(BaseTrainDataset):\n",
    "    \"\"\"\n",
    "    Pointwise training dataset implementation\n",
    "    \n",
    "    This dataset is specifically designed for pointwise scoring tasks,\n",
    "    evaluating the quality of each sample independently\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.reward_module = PointwiseReward(\n",
    "            name=\"pointwise_reward\",\n",
    "            template=PointwiseTemplate,\n",
    "            examples=self._get_examples(),\n",
    "            llm=None,\n",
    "        )\n",
    "        super().__init__(*args, **kwargs)\n",
    "```\n",
    "\n",
    "### Pointwise Reward Function\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def pointwise_reward(predicted_score, true_score):\n",
    "    \"\"\"\n",
    "    Pointwise scoring reward function\n",
    "    \n",
    "    Uses exponential decay mechanism to penalize prediction errors\n",
    "    \n",
    "    Args:\n",
    "        predicted_score (float): Model predicted score\n",
    "        true_score (float): Ground truth score\n",
    "        \n",
    "    Returns:\n",
    "        float: Calculated reward value [0, 1]\n",
    "    \"\"\"\n",
    "    if true_score is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate absolute error\n",
    "    abs_error = abs(predicted_score - true_score)\n",
    "    max_error = 4  # Scoring range 0-4\n",
    "    \n",
    "    # Exponential decay reward calculation\n",
    "    k = 2.0  # Decay coefficient, controls penalty strength\n",
    "    error_ratio = abs_error / max_error\n",
    "    reward = math.exp(-k * error_ratio)\n",
    "    \n",
    "    return float(reward)\n",
    "```\n",
    "\n",
    "### Training Results Analysis\n",
    "\n",
    "#### Training Curves\n",
    "\n",
    "We evaluate the model's learning effectiveness through training and validation curves:\n",
    "\n",
    "**Training Reward Curve**:\n",
    "![Training Reward Curve](../images/data/pointwise_train.jpg)\n",
    "\n",
    "**Validation Reward Curve**:\n",
    "![Validation Reward Curve](../images/data/pointwise_val.jpg)\n",
    "\n",
    "## üîó Related Resources\n",
    "\n",
    "### Framework Documentation\n",
    "- **VERL Framework**: https://github.com/volcengine/verl - Core training framework\n",
    "- **Ray Distributed**: https://docs.ray.io/ - Distributed computing platform\n",
    "- **VLLM Inference**: https://docs.vllm.ai/ - High-performance inference engine\n",
    "\n",
    "### Dataset Resources\n",
    "- **HelpSteer2**: https://huggingface.co/datasets/nvidia/helpsteer2 - Human preference dataset\n",
    "- **UltraFeedback**: https://huggingface.co/datasets/openbmb/UltraFeedback - Multi-dimensional feedback data\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
