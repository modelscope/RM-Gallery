{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 VERL 的Reasoning Reward Model训练指南\n",
    "\n",
    "## 📖 概述\n",
    "\n",
    "本文档提供了使用 VERL 框架训练Reasoning Reward Model的完整指南。通过本教程，您将学会如何配置环境、准备数据、设计奖励函数并执行训练流程。\n",
    "\n",
    "## 🏗️ 系统架构\n",
    "\n",
    "### 核心组件\n",
    "\n",
    "VERL 推理奖励模型训练系统由以下三个核心组件构成：\n",
    "\n",
    "#### 1. **训练数据集** - 继承自 `BaseTrainDataset`\n",
    "   - 支持多种数据格式和评估标准\n",
    "   - 提供灵活的对话模板系统\n",
    "   - 集成自定义奖励函数\n",
    "\n",
    "#### 2. **提示模板** - 基于 `BasePromptTemplate`\n",
    "   - 定义结构化输出格式\n",
    "   - 支持可扩展的评分标准\n",
    "   - 适配多种任务类型\n",
    "\n",
    "#### 3. **奖励函数** - 可自定义的奖励计算模块\n",
    "   - 支持 pointwise、pairwise 等多种奖励计算方式\n",
    "   - 提供灵活的评估指标配置\n",
    "   - 实时统计准确率和奖励分数\n",
    "\n",
    "## 🔧 环境配置\n",
    "\n",
    "### 运行环境要求\n",
    "\n",
    "创建 `runtime_env.yaml` 配置文件：\n",
    "\n",
    "```yaml\n",
    "# runtime_env.yaml\n",
    "excludes: [\"/.git/\"]\n",
    "env_vars:\n",
    "  TORCH_NCCL_AVOID_RECORD_STREAMS: \"1\"\n",
    "  PYTORCH_CUDA_ALLOC_CONF: \"expandable_segments: False\"\n",
    "  WANDB_API_KEY: \"your_wandb_api_key\"\n",
    "  WANDB_BASE_URL: \"your_wandb_base_url\"\n",
    "  HYDRA_FULL_ERROR: \"1\"\n",
    "```\n",
    "\n",
    "### 依赖安装\n",
    "\n",
    "确保安装以下必要依赖：\n",
    "- `verl==0.4.0` (核心框架)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 快速开始\n",
    "\n",
    "### 第一步：准备训练数据\n",
    "\n",
    "训练数据应符合 `DataSample` 格式规范。详细的数据加载和预处理步骤请参考数据加载章节。\n",
    "\n",
    "### 第二步：启动 Ray 分布式集群\n",
    "\n",
    "#### 主节点 (Master) 启动：\n",
    "```bash\n",
    "ray start --head --node-ip-address $MASTER_ADDR --num-gpus 8\n",
    "```\n",
    "\n",
    "#### 工作节点 (Workers) 启动：\n",
    "```bash\n",
    "ray start --address=$MASTER_ADDR:6379 --num-gpus 8\n",
    "```\n",
    "\n",
    "### 第三步：执行训练流程\n",
    "\n",
    "```bash\n",
    "# 进入训练目录\n",
    "cd rm_gallery/gallery/train/<your_method>\n",
    "\n",
    "# 启动训练脚本\n",
    "bash run_train.sh\n",
    "```\n",
    "\n",
    "### 数据格式说明\n",
    "\n",
    "- **输入数据格式**：所有输入数据必须符合 `DataSample` 格式\n",
    "\n",
    "## 🧩 核心组件详解\n",
    "\n",
    "### 自定义训练数据集\n",
    "\n",
    "以下是自定义训练数据集的完整实现示例：\n",
    "\n",
    "```python\n",
    "class CustomTrainDataset(BaseTrainDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # 初始化奖励模块\n",
    "        self.reward_module = YourRewardModule(\n",
    "            name=\"custom_reward\",\n",
    "            template=YourTemplate,\n",
    "            examples=self._get_examples(),\n",
    "            llm=None,\n",
    "        )\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _build_messages(self, example):\n",
    "        # 构建格式化消息\n",
    "        result = self.reward_module.format(sample=example)\n",
    "        return [{\"role\": \"user\", \"content\": result}]\n",
    "```\n",
    "\n",
    "> **重要提示：推理模型配置**\n",
    "> \n",
    "> 训练推理奖励模型时，需要注意以下配置：\n",
    "> - 当基础模型为推理模型（如 Qwen3）时：\n",
    ">   - `apply_chat_template` 的 `enable_thinking=True`\n",
    ">   - `format` 的 `enable_thinking=False`\n",
    "> - 对于非推理模型：\n",
    ">   - `apply_chat_template` 的 `enable_thinking=False`\n",
    ">   - `format` 的 `enable_thinking=True`\n",
    ">\n",
    "> ```python\n",
    "> # 推理模型配置示例\n",
    "> self.tokenizer.apply_chat_template(\n",
    ">     messages, add_generation_prompt=True, tokenize=False, enable_thinking=True\n",
    "> )\n",
    "> \n",
    "> result = self.helpfulness_reward.format(sample=example, enable_thinking=False)\n",
    "> ```\n",
    "\n",
    "### 奖励函数设计\n",
    "\n",
    "奖励函数是评估模型性能的关键组件：\n",
    "\n",
    "```python\n",
    "def calculate_reward(predicted_score, true_score):\n",
    "    \"\"\"\n",
    "    自定义奖励函数\n",
    "    \n",
    "    Args:\n",
    "        predicted_score (float): 模型预测分数\n",
    "        true_score (float): 真实标签分数\n",
    "        \n",
    "    Returns:\n",
    "        float: 计算得到的奖励值\n",
    "    \"\"\"\n",
    "    if true_score is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # 基于绝对误差的奖励计算\n",
    "    abs_error = abs(predicted_score - true_score)\n",
    "    max_error = 4  # 根据评分范围调整 (例如：0-4分制)\n",
    "    \n",
    "    # 线性衰减奖励函数\n",
    "    reward = 1.0 - (abs_error / max_error)\n",
    "    return max(0.0, reward)  # 确保奖励值非负\n",
    "```\n",
    "\n",
    "### 提示模板系统\n",
    "\n",
    "模板系统定义了模型输入输出的结构化格式：\n",
    "\n",
    "```python\n",
    "class YourTemplate(BasePromptTemplate):\n",
    "    score: int = Field(description=\"评分结果 (0-4分)\")\n",
    "    \n",
    "    @classmethod\n",
    "    def format(cls, desc, examples, query, answer, **kwargs):\n",
    "        \"\"\"\n",
    "        格式化提示模板\n",
    "        \n",
    "        Args:\n",
    "            desc (str): 任务描述\n",
    "            examples (str): 示例数据\n",
    "            query (str): 用户查询\n",
    "            answer (str): 模型回答\n",
    "        \n",
    "        Returns:\n",
    "            str: 格式化后的提示文本\n",
    "        \"\"\"\n",
    "        return f\"\"\"# 任务描述\n",
    "{desc}\n",
    "\n",
    "# 参考示例\n",
    "{examples}\n",
    "\n",
    "# 用户查询\n",
    "{query}\n",
    "\n",
    "# 模型回答\n",
    "{answer}\n",
    "\n",
    "# 输出要求\n",
    "{cls.schema(**kwargs)}\n",
    "        \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 实现示例：Pointwise 训练\n",
    "\n",
    "### 项目结构\n",
    "\n",
    "```\n",
    "rm_gallery/gallery/train/pointwise/\n",
    "├── run_pointwise.sh    # 训练启动脚本\n",
    "├── dataset.py          # 数据集实现\n",
    "├── reward_fn.py        # 奖励函数实现\n",
    "├── template.py         # 提示模板\n",
    "└── runtime_env.yaml    # 训练配置文件\n",
    "```\n",
    "\n",
    "### 实验配置\n",
    "\n",
    "#### 数据集选择\n",
    "- **数据源**：`nvidia/helpsteer2` 数据集\n",
    "- **评估维度**：`helpfulness` 标注信息\n",
    "- **数据规模**：训练集约 8K 样本，验证集约 8K 样本\n",
    "- **评分范围**：0-4 分制 (0=最差，4=最佳)\n",
    "\n",
    "#### 模型配置\n",
    "- **基础模型**：`Qwen3-8B`\n",
    "\n",
    "#### 训练参数\n",
    "详细训练参数配置请查看：`./gallery/train/pointwise/run_pointwise.sh`\n",
    "\n",
    "### Pointwise 训练数据集\n",
    "\n",
    "```python\n",
    "class PointwiseTrainDataset(BaseTrainDataset):\n",
    "    \"\"\"\n",
    "    Pointwise 训练数据集实现\n",
    "    \n",
    "    该数据集专门用于Pointwise评分任务，对每个样本独立进行质量评估\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.reward_module = PointwiseReward(\n",
    "            name=\"pointwise_reward\",\n",
    "            template=PointwiseTemplate,\n",
    "            examples=self._get_examples(),\n",
    "            llm=None,\n",
    "        )\n",
    "        super().__init__(*args, **kwargs)\n",
    "```\n",
    "\n",
    "### Pointwise 奖励函数\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def pointwise_reward(predicted_score, true_score):\n",
    "    \"\"\"\n",
    "    点式评分奖励函数\n",
    "    \n",
    "    采用指数衰减机制，对预测误差进行惩罚\n",
    "    \n",
    "    Args:\n",
    "        predicted_score (float): 模型预测的分数\n",
    "        true_score (float): 真实的标准分数\n",
    "        \n",
    "    Returns:\n",
    "        float: 计算得到的奖励值 [0, 1]\n",
    "    \"\"\"\n",
    "    if true_score is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # 计算绝对误差\n",
    "    abs_error = abs(predicted_score - true_score)\n",
    "    max_error = 4  # 评分范围 0-4\n",
    "    \n",
    "    # 指数衰减奖励计算\n",
    "    k = 2.0  # 衰减系数，控制惩罚强度\n",
    "    error_ratio = abs_error / max_error\n",
    "    reward = math.exp(-k * error_ratio)\n",
    "    \n",
    "    return float(reward)\n",
    "```\n",
    "\n",
    "### 训练结果分析\n",
    "\n",
    "#### 训练曲线\n",
    "\n",
    "我们通过训练和验证曲线来评估模型的学习效果：\n",
    "\n",
    "**训练奖励曲线**：\n",
    "![训练奖励曲线](../../images/data/pointwise_train.jpg)\n",
    "\n",
    "**验证奖励曲线**：\n",
    "![验证奖励曲线](../../images/data/pointwise_val.jpg)\n",
    "\n",
    "## 🔗 相关资源\n",
    "\n",
    "### 框架文档\n",
    "- **VERL 框架**: https://github.com/volcengine/verl - 核心训练框架\n",
    "- **Ray 分布式**: https://docs.ray.io/ - 分布式计算平台\n",
    "- **VLLM 推理**: https://docs.vllm.ai/ - 高性能推理引擎\n",
    "\n",
    "### 数据集资源\n",
    "- **HelpSteer2**: https://huggingface.co/datasets/nvidia/helpsteer2 - 人类偏好数据集\n",
    "- **UltraFeedback**: https://huggingface.co/datasets/openbmb/UltraFeedback - 多维度反馈数据\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
