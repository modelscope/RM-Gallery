{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using RM-Gallery Reward Models in Post Training\n",
    "\n",
    "This tutorial provides a detailed guide on how to use RM-Gallery reward models for post training within the VERL framework. We will focus on implementing custom reward managers, including asynchronous processing of prompt groups and support for pairwise rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "In Reinforcement Learning from Human Feedback (RLHF) and other post training methods, reward models play a crucial role. This tutorial will demonstrate how to:\n",
    "\n",
    "1. **Integrate RM-Gallery into VERL Framework**: Create custom reward managers to support complex reward computations\n",
    "2. **Asynchronous Prompt Group Processing**: Improve computational efficiency and support batch processing of multiple candidate responses for the same prompt\n",
    "3. **Support Pairwise Rewards**: Implement more precise preference learning in algorithms like GRPO\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Asynchronous Parallel Computing**: Support parallel processing of multiple prompt groups, significantly improving efficiency\n",
    "- **Flexible Reward Composition**: Support combination of multiple reward functions (principled rewards, format rewards, length rewards, etc.)\n",
    "- **Pairwise Comparison**: Support pairwise comparisons to provide more precise preference signals for algorithms like GRPO\n",
    "- **Statistical Information Tracking**: Automatically calculate and record reward distribution statistics for training monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "First, ensure that the necessary dependencies are installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "%pip install rm-gallery\n",
    "%pip install verl\n",
    "\n",
    "# Import necessary libraries\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from verl import DataProto\n",
    "\n",
    "# Import RM-Gallery components\n",
    "from rm_gallery.core.reward import RewardRegistry\n",
    "from rm_gallery.core.reward.composition import RewardComposition\n",
    "from rm_gallery.gallery.rm.general import GeneralReward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Implementation of Custom Reward Manager\n",
    "\n",
    "### 3.1 Asynchronous Single Group Reward Computation Function\n",
    "\n",
    "First, implement asynchronous processing for reward computation of a single prompt group:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def single_compute_score(compute_score, prompt, responses, extras, reward_kwargs, meta_info, executor, timeout=300.0):\n",
    "    \"\"\"\n",
    "    Asynchronous task for single group reward computation\n",
    "    \n",
    "    Args:\n",
    "        compute_score: Reward computation function\n",
    "        prompt: Input prompt\n",
    "        responses: List of candidate responses\n",
    "        extras: Additional information\n",
    "        reward_kwargs: Reward computation parameters\n",
    "        meta_info: Meta information\n",
    "        executor: Thread pool executor\n",
    "        timeout: Timeout duration\n",
    "    \n",
    "    Returns:\n",
    "        Computed reward scores and detailed information\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_running_loop()\n",
    "    task = asyncio.wait_for(\n",
    "        loop.run_in_executor(\n",
    "            executor,\n",
    "            partial(compute_score, prompt=prompt, responses=responses, extras=extras, **reward_kwargs, **meta_info),\n",
    "        ),\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    return await task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom Reward Manager Class\n",
    "\n",
    "This is the core Reward Manager implementation, including asynchronous parallel processing and pairwise comparison functionality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMGalleryRewardManager:\n",
    "    \"\"\"\n",
    "    Custom reward manager based on RM-Gallery\n",
    "    \n",
    "    Core Features:\n",
    "    1. Asynchronous parallel processing: Support parallel computation of multiple prompt groups\n",
    "    2. Pairwise comparison: Provide pairwise comparison reward signals for algorithms like GRPO\n",
    "    3. Flexible reward composition: Support combination of multiple reward functions\n",
    "    4. Statistical tracking: Automatically compute reward distribution statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, num_examine=3, is_val_mode=False, compute_score=None, \n",
    "                 reward_fn_key=\"data_source\", **reward_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize Reward Manager\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Tokenizer for decoding\n",
    "            num_examine: Number of samples to print during debugging\n",
    "            is_val_mode: Whether in validation mode (supports pairwise comparison)\n",
    "            compute_score: Reward computation function\n",
    "            reward_fn_key: Data source key name\n",
    "            **reward_kwargs: Additional parameters for reward computation\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_examine = num_examine\n",
    "        self.is_val_mode = is_val_mode\n",
    "        self.compute_score = compute_score\n",
    "        self.reward_fn_key = reward_fn_key\n",
    "        self.reward_kwargs = reward_kwargs\n",
    "        self.max_workers = reward_kwargs.get(\"max_workers\", 8)\n",
    "        self.timeout = reward_kwargs.get(\"timeout\", 300.0)\n",
    "        self.meta_info = {}\n",
    "        \n",
    "        # Initialize RM-Gallery reward components\n",
    "        if compute_score is None:\n",
    "            self._init_rm_gallery_components()\n",
    "    \n",
    "    def _init_rm_gallery_components(self):\n",
    "        \"\"\"Initialize RM-Gallery reward components\"\"\"\n",
    "        # Get reward functions from registry\n",
    "        registry = RewardRegistry()\n",
    "        \n",
    "        # Combine multiple reward functions\n",
    "        self.reward_composition = RewardComposition([\n",
    "            registry.get(\"general\"),  # General reward\n",
    "            registry.get(\"format\"),   # Format reward  \n",
    "            registry.get(\"length\"),   # Length reward\n",
    "        ])\n",
    "        \n",
    "        self.compute_score = self.reward_composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue RMGalleryRewardManager class with asynchronous parallel computation methods\n",
    "def extend_reward_manager():\n",
    "    \"\"\"Extend RMGalleryRewardManager class by adding parallel computation methods\"\"\"\n",
    "    \n",
    "    async def parallel_compute_scores(self, prompt_to_indices, responses_str, extras_info):\n",
    "        \"\"\"\n",
    "        Parallel computation of reward scores for multiple groups\n",
    "        \n",
    "        This is the core function for asynchronous processing, which groups candidate responses \n",
    "        with the same prompt and computes them in parallel across different groups, \n",
    "        significantly improving computational efficiency.\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Create asynchronous tasks for each group\n",
    "            tasks = []\n",
    "            for prompt, indices in prompt_to_indices.items():\n",
    "                group_responses = [responses_str[i] for i in indices]\n",
    "                group_extras = [extras_info[i] for i in indices]\n",
    "\n",
    "                # In validation mode, add reference answer for pairwise comparison\n",
    "                if self.is_val_mode:\n",
    "                    reference_response = group_extras[0][\"y\"][0].get(\"content\", \"\")\n",
    "                    group_responses.append(reference_response)\n",
    "                    group_extras.append(group_extras[0])\n",
    "                    \n",
    "                # Create asynchronous task\n",
    "                task = single_compute_score(\n",
    "                    self.compute_score, prompt, group_responses, group_extras,\n",
    "                    self.reward_kwargs, self.meta_info, executor, timeout=self.timeout\n",
    "                )\n",
    "                tasks.append((task, indices))\n",
    "            \n",
    "            # Execute all tasks in parallel\n",
    "            results = await asyncio.gather(*(task for task, _ in tasks))\n",
    "            \n",
    "            # Process pairwise comparison results\n",
    "            for (result, indices) in zip(results, [indices for _, indices in tasks]):\n",
    "                if self.is_val_mode:\n",
    "                    scores, reward_info = result[0], result[1]\n",
    "                    scores = scores[:-1]  # Remove reference answer score\n",
    "                    \n",
    "                    # Calculate win rate statistics (key metric for pairwise comparison)\n",
    "                    comparison_scores = reward_info[\"comparison_score\"]\n",
    "                    win_rate = [1.0 if comparison_scores[0] > comparison_scores[1] else 0.0]\n",
    "                    win_and_rate = [1.0 if comparison_scores[0] >= comparison_scores[1] else 0.0]\n",
    "                    \n",
    "                    # Update reward information\n",
    "                    for key, vals in reward_info.items():\n",
    "                        reward_info[key] = vals[:-1]\n",
    "                    reward_info.update({\"win\": win_rate, \"win_and\": win_and_rate})\n",
    "                    \n",
    "                    print(f\"Pairwise results: scores={scores}, win_rate={win_rate}\")\n",
    "                    result = (scores, reward_info)\n",
    "                    \n",
    "                all_results.append((result, indices))\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    # Add method to the class\n",
    "    RMGalleryRewardManager.parallel_compute_scores = parallel_compute_scores\n",
    "\n",
    "extend_reward_manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add main call method to RMGalleryRewardManager class\n",
    "def add_call_method():\n",
    "    \"\"\"Add main __call__ method\"\"\"\n",
    "    \n",
    "    def __call__(self, data: DataProto, return_dict=False):\n",
    "        \"\"\"\n",
    "        Calculate reward values for input data, supports batch processing and async parallel computation\n",
    "        \n",
    "        Args:\n",
    "            data: Data object containing model inputs and outputs\n",
    "            return_dict: Whether to return results as dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Reward tensor or dictionary containing reward information\n",
    "        \"\"\"\n",
    "        # If reward scores already exist, return directly\n",
    "        if \"rm_scores\" in data.batch.keys():\n",
    "            if return_dict:\n",
    "                return {\"reward_tensor\": data.batch[\"rm_scores\"]}\n",
    "            else:\n",
    "                return data.batch[\"rm_scores\"]\n",
    "\n",
    "        # Initialize reward tensor\n",
    "        reward_tensor = torch.zeros_like(data.batch[\"responses\"], dtype=torch.float32)\n",
    "        prompt_ids = data.batch[\"prompts\"]\n",
    "        prompt_len = prompt_ids.shape[-1]\n",
    "        attention_mask = data.batch[\"attention_mask\"]\n",
    "        valid_response_lengths = attention_mask[:, prompt_len:].sum(dim=-1)\n",
    "\n",
    "        # Update meta info (for statistical tracking)\n",
    "        if data.meta_info.get(\"last_reward_info\", None) is not None:\n",
    "            self.meta_info.update({\"last_mean_std\": np.mean(data.meta_info[\"last_reward_info\"][\"reward_std\"])})\n",
    "        \n",
    "        # Decode prompt and response\n",
    "        responses_str = []\n",
    "        prompts_str = []\n",
    "        extras_info = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            length = valid_response_lengths[i].item()\n",
    "            response_str = self.tokenizer.decode(data.batch[\"responses\"][i][:length], skip_special_tokens=True)\n",
    "            prompt_str = self.tokenizer.decode(data.batch[\"prompts\"][i], skip_special_tokens=True)\n",
    "            extra_info = data.non_tensor_batch['extra_info'][i]\n",
    "            \n",
    "            responses_str.append(response_str)\n",
    "            prompts_str.append(prompt_str)\n",
    "            extras_info.append(extra_info)\n",
    "\n",
    "        # Group by prompt (key for async parallel processing)\n",
    "        prompt_to_indices = defaultdict(list)\n",
    "        for i, prompt in enumerate(prompts_str):\n",
    "            prompt_to_indices[prompt].append(i)\n",
    "\n",
    "        # Validate consistent sample count per group\n",
    "        group_sizes = [len(indices) for indices in prompt_to_indices.values()]\n",
    "        if len(set(group_sizes)) > 1:\n",
    "            raise AssertionError(f\"Sample count must be same per group, current group_sizes: {group_sizes}\")\n",
    "        \n",
    "        print(f\"Total {len(prompt_to_indices)} groups, {group_sizes[0]} samples per group, starting async parallel computation...\")\n",
    "        \n",
    "        # Run async parallel computation\n",
    "        all_results = asyncio.run(\n",
    "            self.parallel_compute_scores(prompt_to_indices, responses_str, extras_info)\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        all_rewards = [0.0] * len(data)\n",
    "        all_reward_infos = defaultdict(list)\n",
    "        \n",
    "        for result, indices in all_results:\n",
    "            scores, reward_info = result[0], result[1]\n",
    "            \n",
    "            # Map scores back to original indices\n",
    "            for score, idx in zip(scores, indices):\n",
    "                all_rewards[idx] = score\n",
    "            \n",
    "            # Process reward info\n",
    "            if reward_info and isinstance(reward_info, dict):\n",
    "                for key, values in reward_info.items():\n",
    "                    if key not in all_reward_infos:\n",
    "                        all_reward_infos[key] = [0.0] * len(data)\n",
    "                    for value, idx in zip(values, indices):\n",
    "                        all_reward_infos[key][idx] = value\n",
    "        \n",
    "        # Populate reward tensor\n",
    "        for i in range(len(data)):\n",
    "            length = valid_response_lengths[i].item()\n",
    "            reward = all_rewards[i]\n",
    "            reward_tensor[i, length - 1] = reward\n",
    "            \n",
    "            # Debug output\n",
    "            if i < self.num_examine:\n",
    "                print(f\"[Sample {i}] Prompt: {prompts_str[i]}\")\n",
    "                print(f\"[Sample {i}] Response: {responses_str[i]}\")\n",
    "                print(f\"[Sample {i}] Score: {reward}\")\n",
    "        \n",
    "        # Add accuracy info\n",
    "        data.batch[\"acc\"] = torch.tensor(all_rewards, dtype=torch.float32, device=prompt_ids.device)\n",
    "        \n",
    "        if return_dict:\n",
    "            return {\"reward_tensor\": reward_tensor, \"reward_extra_info\": dict(all_reward_infos)}\n",
    "        else:\n",
    "            return reward_tensor\n",
    "    \n",
    "    # Add method to class\n",
    "    RMGalleryRewardManager.__call__ = __call__\n",
    "\n",
    "add_call_method()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RM-Gallery Reward Function Implementation\n",
    "\n",
    "Next, we implement the RM-Gallery-based reward computation function that supports combination of multiple reward types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rm_gallery_reward_function(use_group_reward=True, return_details=False, return_statistics=True):\n",
    "    \"\"\"\n",
    "    Create RM-Gallery-based reward computation function\n",
    "    \n",
    "    Args:\n",
    "        use_group_reward: Whether to use group reward (supports pairwise comparison)\n",
    "        return_details: Whether to return detailed information\n",
    "        return_statistics: Whether to return statistical information\n",
    "    \n",
    "    Returns:\n",
    "        Configured reward computation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def reward_func(prompt, responses, extras=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Comprehensive reward computation function that combines multiple reward types\n",
    "        \n",
    "        Reward combination includes:\n",
    "        1. Principled rewards (95% weight): Based on helpfulness, harmlessness, honesty principles\n",
    "        2. Format rewards (5% weight): Ensure output format correctness\n",
    "        3. Length rewards: Control appropriate response length\n",
    "        4. N-gram rewards: Reduce penalties for repetitive content\n",
    "        \"\"\"\n",
    "        details = []\n",
    "        \n",
    "        # Ensure responses is in list format\n",
    "        if not isinstance(responses, list):\n",
    "            responses = [responses]\n",
    "            if prompt and not isinstance(prompt, list):\n",
    "                prompt = [prompt]\n",
    "        \n",
    "        # 1. Principled reward computation (core reward)\n",
    "        if use_group_reward:\n",
    "            # Group reward supporting pairwise comparison\n",
    "            scores_principle, details = group_rm_gallery_grader(prompt, responses, extras, **kwargs)\n",
    "        else:\n",
    "            # Individual scoring reward\n",
    "            scores_principle, details = rm_gallery_grader(prompt, responses, extras, **kwargs)\n",
    "        \n",
    "        # 2. Format reward computation\n",
    "        scores_format = compute_format_reward(responses)\n",
    "        \n",
    "        # 3. N-gram repetition penalty\n",
    "        ngram_penalty_fn = create_ngram_penalty_reward(ngram_size=5, max_penalty=-1.0, min_scaling=0.1)\n",
    "        scores_ngram = ngram_penalty_fn(responses)\n",
    "        \n",
    "        # 4. Length reward computation\n",
    "        scores_thought_length, thought_lengths = compute_thought_length_reward(responses)\n",
    "        scores_total_length, total_lengths = compute_total_length_reward(responses)\n",
    "        \n",
    "        # Convert to tensor format\n",
    "        scores_principle = torch.tensor(scores_principle)\n",
    "        scores_format = torch.tensor(scores_format)\n",
    "        scores_ngram = torch.tensor(scores_ngram)\n",
    "        scores_thought_length = torch.tensor(scores_thought_length)\n",
    "        scores_total_length = torch.tensor(scores_total_length)\n",
    "        thought_lengths = torch.tensor(thought_lengths, dtype=torch.float32)\n",
    "        \n",
    "        # Weighted reward combination\n",
    "        scores = (0.95 * scores_principle + \n",
    "                 0.05 * scores_format + \n",
    "                 scores_total_length + \n",
    "                 scores_ngram)\n",
    "        \n",
    "        # Handle invalid rewards (e.g., HTTP errors)\n",
    "        INVALID_REWARD = -999.0\n",
    "        scores[scores_principle == INVALID_REWARD] = INVALID_REWARD\n",
    "        scores = scores.tolist()\n",
    "        \n",
    "        # Build reward information dictionary\n",
    "        reward_info = {\n",
    "            \"reward_principle\": scores_principle.tolist(),\n",
    "            \"reward_format\": scores_format.tolist(),\n",
    "            \"reward_ngram\": scores_ngram.tolist(),\n",
    "            \"thought_lengths\": thought_lengths.tolist(),\n",
    "            \"scores_thought_length\": scores_thought_length.tolist(),\n",
    "            \"scores_total_lengths\": scores_total_length.tolist(),\n",
    "        }\n",
    "        \n",
    "        if return_details:\n",
    "            return scores, reward_info, details\n",
    "        return scores, reward_info\n",
    "    \n",
    "    return reward_func\n",
    "\n",
    "# Create reward function instance\n",
    "rm_gallery_reward_function = create_rm_gallery_reward_function(\n",
    "    use_group_reward=True,  # Enable pairwise comparison\n",
    "    return_details=False,\n",
    "    return_statistics=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Registering Custom Reward Manager in VERL\n",
    "\n",
    "To use our custom Reward Manager in the VERL framework, we need to register it in VERL's module system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom manager in VERL's reward manager initialization file\n",
    "# File path: verl/workers/reward_manager/__init__.py\n",
    "\n",
    "registration_code = '''\n",
    "from .batch import BatchRewardManager\n",
    "from .dapo import DAPORewardManager  \n",
    "from .naive import NaiveRewardManager\n",
    "from .prime import PrimeRewardManager\n",
    "from .rm_gallery import RMGalleryRewardManager  # Add our reward manager\n",
    "\n",
    "__all__ = [\n",
    "    \"BatchRewardManager\", \n",
    "    \"DAPORewardManager\", \n",
    "    \"NaiveRewardManager\", \n",
    "    \"PrimeRewardManager\",\n",
    "    \"RMGalleryRewardManager\"  # Add to export list\n",
    "]\n",
    "'''\n",
    "\n",
    "print(\"Need to add the following registration code to the VERL project:\")\n",
    "print(registration_code)\n",
    "\n",
    "# Create reward manager configuration example\n",
    "reward_manager_config = {\n",
    "    \"reward_manager\": {\n",
    "        \"type\": \"RMGalleryRewardManager\",\n",
    "        \"args\": {\n",
    "            \"num_examine\": 3,\n",
    "            \"is_val_mode\": True,  # Enable pairwise validation mode\n",
    "            \"compute_score\": rm_gallery_reward_function,\n",
    "            \"max_workers\": 8,\n",
    "            \"timeout\": 300.0,\n",
    "            \"use_group_reward\": True,\n",
    "            \"return_details\": False,\n",
    "            \"return_statistics\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration example:\")\n",
    "import json\n",
    "print(json.dumps(reward_manager_config, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Core Feature Detailed Explanation\n",
    "\n",
    "### 6.1 Asynchronous Processing of Prompt Groups\n",
    "\n",
    "One core innovation of our Reward Manager is **asynchronous parallel processing by prompt grouping**:\n",
    "\n",
    "#### Why do we need prompt grouping?\n",
    "During post training, typically multiple candidate responses (e.g., 4-8) are generated for each prompt, and these candidate responses need to be compared with each other to provide preference signals. The traditional approach is to compute rewards for each response individually, but this approach has several problems:\n",
    "\n",
    "1. **Low efficiency**: Cannot leverage the advantages of batch processing\n",
    "2. **Lack of comparison**: Cannot perform pairwise comparisons\n",
    "3. **Resource waste**: Repeated computation of the same prompt's context\n",
    "\n",
    "#### Our solution:\n",
    "```python\n",
    "# Group by prompt\n",
    "prompt_to_indices = defaultdict(list)\n",
    "for i, prompt in enumerate(prompts_str):\n",
    "    prompt_to_indices[prompt].append(i)\n",
    "```\n",
    "\n",
    "**Advantages of asynchronous parallel processing:**\n",
    "- **Intra-group batch processing**: Multiple candidate responses for the same prompt are processed together, supporting pairwise comparison\n",
    "- **Inter-group parallelism**: Groups with different prompts can be computed in parallel, significantly improving efficiency\n",
    "- **Resource optimization**: Avoid repeated computation of prompt embeddings, etc.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
