{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Response Refinement Tutorial\n",
    "## 1. Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use the [LLMRefinement](../../../rm_gallery/core/reward/refinement.py#L12-L131) class for iterative improvement of LLM responses using reward model feedback.\n",
    "\n",
    "For more advanced usage, such as iterative refinement with comprehensive evaluation to correct datasamples, see [data_correction](../../../examples/rm_application/data_correction.py).\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- **Iterative Refinement**: Repeatedly improve responses through feedback loops\n",
    "\n",
    "- **Reward Model Feedback**: Use reward model assessments to guide improvements\n",
    "\n",
    "- **Response Evolution**: Maintain response history to enable refinement\n",
    "\n",
    "- **Dynamic Prompting**: Construct prompts based on feedback and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "First, let's import necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core modules\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from rm_gallery.core.data.schema import DataSample, DataOutput, Step, ChatMessage\n",
    "from rm_gallery.core.model.message import MessageRole\n",
    "from rm_gallery.core.model.openai_llm import OpenaiLLM\n",
    "from rm_gallery.core.reward.registry import RewardRegistry\n",
    "from rm_gallery.core.reward.base import BaseLLMReward\n",
    "from rm_gallery.core.reward.refinement import LLMRefinement\n",
    "from loguru import logger\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Input\n",
    "\n",
    "Let's start by creating a sample input to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input\n",
    "sample = DataSample(\n",
    "    unique_id=\"refinement_demo\",\n",
    "    input=[\n",
    "        ChatMessage(\n",
    "            role=MessageRole.USER,\n",
    "            content=\"Explain quantum computing in simple terms\"\n",
    "        )\n",
    "    ],\n",
    "    output=[]  # We'll generate responses later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Reward\n",
    "\n",
    "We'll initialize our reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for response generation\n",
    "llm = OpenaiLLM(model=\"qwen3-8b\", enable_thinking=True)\n",
    "\n",
    "# Initialize reward model\n",
    "reward: BaseLLMReward = RewardRegistry.get(\"base_helpfulness_listwise\")(\n",
    "    name=\"helpfulness\",\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Refinement Process\n",
    "\n",
    "We will give two examples of how to run the refinement process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Run in Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = refined_sample = reward.refine(sample, max_iterations=3)\n",
    "print(\"\\nüèÜ Final Refined Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Run in Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create refinement module\n",
    "refiner = LLMRefinement(\n",
    "    llm=llm,\n",
    "    reward=reward,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "result = refiner.run(sample)\n",
    "print(\"\\nüèÜ Final Refined Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis\n",
    "\n",
    "Let's look at what happens during each iteration of the refinement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_run(sample: DataSample, max_iterations: int = 3):\n",
    "    \"\"\"Run refinement process with detailed output for each iteration.\"\"\"\n",
    "\n",
    "    # Initial response generation\n",
    "    response = llm.chat(sample.input)\n",
    "    sample.output.append(DataOutput(answer=Step(\n",
    "        role=MessageRole.ASSISTANT, \n",
    "        content=response.content\n",
    "    )))\n",
    "    \n",
    "    print(\"Initial Response:\")\n",
    "    print(response.content)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    # Iterative refinement loop\n",
    "    for i in range(max_iterations):\n",
    "        \n",
    "        # Generate feedback\n",
    "        feedback = refiner._generate_feedback(sample)\n",
    "        \n",
    "        # Print iteration details\n",
    "        print(f\"Iteration {i+1}/{max_iterations}:\")\n",
    "        print(\"Feedback Received:\", feedback)\n",
    "        \n",
    "        # Generate refined response\n",
    "        sample = refiner._generate_response(sample, feedback)\n",
    "        \n",
    "        print(\"Refined Response:\")\n",
    "        print(sample.output[-1].answer.content)\n",
    "        print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    return sample.output[-1].answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with detailed analysis\n",
    "\n",
    "sample = DataSample(\n",
    "    unique_id=\"detailed_run_demo\",\n",
    "    input=[\n",
    "        ChatMessage(\n",
    "            role=MessageRole.USER,\n",
    "            content=\"What are the benefits of regular exercise?\"\n",
    "        )\n",
    "    ],\n",
    "    output=[]  # We'll generate responses later\n",
    ")\n",
    "detailed_run(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world Applications\n",
    "\n",
    "The refinement approach can be applied in various scenarios such as:\n",
    "\n",
    "- Academic writing assistance\n",
    "- Technical documentation improvement\n",
    "- Educational content creation\n",
    "- Code explanation refinement\n",
    "- Research summarization\n",
    "- Business communication optimization\n",
    "\n",
    "For production environments, you might want to:\n",
    "- Implement caching for intermediate responses\n",
    "- Add comprehensive error handling\n",
    "- Set up detailed logging\n",
    "- Implement batch processing capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
