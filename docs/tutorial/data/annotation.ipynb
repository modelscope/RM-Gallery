{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Annotation Module\n",
    "\n",
    "The Data Annotation Module is built on Label Studio, providing efficient and flexible data annotation solutions for machine learning projects. It supports multiple annotation scenarios and is particularly suitable for data preparation in reward model and dialogue system projects.\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "- **Deep Label Studio Integration**: Uses Label Studio as the annotation interface, providing an intuitive and user-friendly annotation experience\n",
    "- **Multi-scenario Data Support**: Comprehensive support for dialogue annotation, quality scoring, preference ranking, and other annotation tasks\n",
    "- **Quick Deployment**: Provides both Docker and pip deployment options with one-click service startup\n",
    "- **Templated Annotation Configuration**: Built-in annotation templates like RewardBench for out-of-the-box usage\n",
    "- **Seamless Ecosystem Integration**: Deep integration with RM Gallery data processing pipeline for smooth data flow\n",
    "- **Enterprise-level Batch Processing**: Supports large-scale batch annotation, export, and management\n",
    "\n",
    "## 2. Application Scenarios\n",
    "\n",
    "This module is primarily suitable for the following machine learning data preparation scenarios:\n",
    "\n",
    "1. **Reward Model Training Data Annotation** - Preparing high-quality preference data for reward model training\n",
    "2. **Dialogue System Quality Assessment** - Evaluating and improving the output quality of dialogue models\n",
    "3. **Preference Learning Data Preparation** - Building comparison datasets for preference learning\n",
    "4. **Text Classification and Sentiment Analysis** - Preparing annotated data for supervised learning tasks\n",
    "\n",
    "## 3. Quick Start\n",
    "\n",
    "### 3.1. Environment Setup\n",
    "\n",
    "Ensure the following dependencies are installed:\n",
    "```\n",
    "label_studio==1.17.0\n",
    "```\n",
    "\n",
    "### 3.2. Start Label Studio Annotation Service\n",
    "\n",
    "Use the following commands to start the annotation service:\n",
    "\n",
    "```bash\n",
    "# Start using Docker (recommended)\n",
    "python ./rm_gallery/core/data/annotation/server.py start\n",
    "\n",
    "# Start using pip\n",
    "python ./rm_gallery/core/data/annotation/server.py start --use-pip\n",
    "\n",
    "# Check service status\n",
    "python ./rm_gallery/core/data/annotation/server.py status\n",
    "\n",
    "# Stop annotation service\n",
    "python ./rm_gallery/core/data/annotation/server.py stop\n",
    "```\n",
    "\n",
    "After successful startup, the console will display:\n",
    "```\n",
    "============================================================\n",
    "ðŸš€ Label Studio Successfully Started!\n",
    "============================================================\n",
    "ðŸŒ Web Interface: http://localhost:8080\n",
    "ðŸ“§ Username: admin@rmgallery.com\n",
    "ðŸ” Password: RM-Gallery\n",
    "ðŸ“ Data Directory: ./log/label_studio_logs\n",
    "ðŸ³ Deployment: Pip\n",
    "============================================================\n",
    "```\n",
    "\n",
    "### 3.3. Verify Service Status\n",
    "\n",
    "Run the status check command to confirm the service is running normally:\n",
    "```\n",
    "==================================================\n",
    "ðŸ“Š Label Studio Status\n",
    "==================================================\n",
    "ðŸŒ Server URL: http://localhost:8080\n",
    "ðŸš€ Deployment: PIP\n",
    "ðŸ”Œ Port: 8080\n",
    "âœ… Running\n",
    "ðŸ“ Data Dir: ./log/label_studio_logs\n",
    "ðŸ‘¤ Username: admin@rmgallery.com\n",
    "ðŸ”„ Process PIDs: 65727\n",
    "ðŸ”Œ Port PIDs: 65727\n",
    "==================================================\n",
    "```\n",
    "\n",
    "### 3.4. Obtain API Token\n",
    "\n",
    "After completing service startup, follow these steps to obtain the API Token:\n",
    "\n",
    "1. Visit http://localhost:8080 in your browser\n",
    "2. Login using the following credentials:\n",
    "   - Username: `admin@rmgallery.com`\n",
    "   - Password: `RM-Gallery`\n",
    "3. Click \"Organization\" in the left navigation bar and go to API Tokens Settings\n",
    "4. Set both Personal Access Tokens and Legacy Tokens to True\n",
    "5. Click the user avatar in the top right corner and select \"Account & Settings\"\n",
    "6. Copy the token value from the Access Token section - this is the API Token you'll need later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Usage Example\n",
    "\n",
    "The following complete example demonstrates how to use the data annotation module, including the full workflow of data import, project creation, annotation execution, and result export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Import data and create annotation project\n",
    "\"\"\"\n",
    "\n",
    "from rm_gallery.core.data.annotation.annotation import create_annotator\n",
    "from rm_gallery.core.data.load.base import create_loader\n",
    "import rm_gallery.core.data     # Core strategy registration\n",
    "import rm_gallery.gallery.data  # Extension strategy registration\n",
    "\n",
    "# Replace with your actual API Token obtained from Label Studio\n",
    "API_TOKEN = \"\"\n",
    "\n",
    "# Step 1.1: Configure data loading parameters\n",
    "load_config = {\n",
    "    \"path\": \"../../../data/reward-bench-2/data/test-00000-of-00001.parquet\",  # Replace with actual data path\n",
    "    \"limit\": 1000,  # Limit the number of records loaded to avoid loading too much data for initial testing\n",
    "}\n",
    "\n",
    "# Step 1.2: Create data loader\n",
    "loader = create_loader(\n",
    "    name=\"rewardbench2\",           # Dataset identifier name\n",
    "    load_strategy_type=\"local\",    # Use local file loading strategy\n",
    "    data_source=\"rewardbench2\",    # Specify data source format converter\n",
    "    config=load_config             # Pass loading configuration parameters\n",
    ")\n",
    "\n",
    "# Step 1.3: Execute data loading\n",
    "print(\"Loading data...\")\n",
    "dataset = loader.run()\n",
    "print(f\"Data loading completed, {len(dataset.datasamples)} records total\")\n",
    "\n",
    "# Step 1.4: Configure annotation project parameters\n",
    "annotation_config = {\n",
    "    \"server_url\": \"http://localhost:8080\",           # Label Studio service address\n",
    "    \"api_token\": API_TOKEN,                          # API access token\n",
    "    \"project_title\": \"RM Gallery Quality Annotation\", # Project display name\n",
    "    \"template_name\": \"rewardbench2\",                 # Use built-in RewardBench2 template\n",
    "    \"project_description\": \"Data quality annotation project based on RewardBench2 template\"\n",
    "}\n",
    "\n",
    "# Step 1.5: Create annotation module instance\n",
    "annotation_module = create_annotator(\n",
    "    name=\"rm_gallery_annotation\",\n",
    "    **annotation_config\n",
    ")\n",
    "\n",
    "# Step 1.6: Create annotation project and import data\n",
    "print(\"Creating annotation project...\")\n",
    "result = annotation_module.run(dataset, create_new_project=True)\n",
    "\n",
    "if result:\n",
    "    project_url = f\"{result.metadata['annotation_server_url']}/projects/{result.metadata['annotation_project_id']}\"\n",
    "    print(f\"âœ… Annotation project created successfully!\")\n",
    "    print(f\"ðŸŒ Project URL: {project_url}\")\n",
    "    print(f\"ðŸ“Š Project ID: {result.metadata['annotation_project_id']}\")\n",
    "else:\n",
    "    print(\"âŒ Failed to create annotation project, please check configuration and network connection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Perform data annotation\n",
    "\n",
    "After the project is created successfully, you can:\n",
    "1. Visit the project URL output above\n",
    "2. Login using admin@rmgallery.com / RM-Gallery\n",
    "3. Annotate data in the annotation interface\n",
    "4. After annotation is complete, run the next step to export annotation results\n",
    "\n",
    "Note: In actual usage, you need to manually complete the annotation work, then run the export code below.\n",
    "\"\"\"\n",
    "\n",
    "# This is a placeholder for annotation operations\n",
    "# Actual annotation work needs to be completed in the Label Studio web interface\n",
    "print(\"ðŸ“ Please complete data annotation work in the Label Studio interface\")\n",
    "print(\"ðŸ’¡ After annotation is complete, run the code below to export results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Export annotation results\n",
    "\"\"\"\n",
    "\n",
    "from rm_gallery.core.data.annotation.annotation import create_annotator\n",
    "from rm_gallery.core.data.export import create_exporter\n",
    "import rm_gallery.core.data     # Core strategy registration\n",
    "import rm_gallery.gallery.data  # Extension strategy registration\n",
    "\n",
    "# Use the same API Token as when creating the project\n",
    "API_TOKEN = \"\"\n",
    "\n",
    "# Step 3.1: Recreate annotation module instance\n",
    "annotation_module = create_annotator(\n",
    "    name=\"rm_gallery_annotation\",\n",
    "    template_name=\"rewardbench2\",\n",
    "    api_token=API_TOKEN\n",
    ")\n",
    "\n",
    "# Step 3.2: Set project ID (obtained from Step 1 output)\n",
    "annotation_module.project_id = 3  # Replace with actual project_id\n",
    "\n",
    "# Step 3.3: Export annotation data from Label Studio\n",
    "print(\"Exporting annotation data...\")\n",
    "try:\n",
    "    annotated_dataset = annotation_module.export_annotations_to_dataset()\n",
    "    print(f\"âœ… Annotation data exported successfully, {len(annotated_dataset.datasamples)} annotated records total\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Export failed: {e}\")\n",
    "    annotated_dataset = None\n",
    "\n",
    "# Step 3.4: Configure file exporter\n",
    "if annotated_dataset:\n",
    "    exporter = create_exporter(\n",
    "        name=\"annotation_exporter\",\n",
    "        config={\n",
    "            \"output_dir\": \"./exports\",          # Export file storage directory\n",
    "            \"formats\": [\"jsonl\"]        # Support multiple export formats\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Step 3.5: Execute data export\n",
    "    print(\"Saving annotation results to file...\")\n",
    "    export_result = exporter.run(annotated_dataset)\n",
    "    \n",
    "    if export_result:\n",
    "        print(\"âœ… Annotation results saved to ./exports directory\")\n",
    "        print(f\"ðŸ“Š Dataset info: {annotated_dataset.name}\")\n",
    "        print(f\"ðŸ“ Contains {len(annotated_dataset.datasamples)} annotation data\")\n",
    "    else:\n",
    "        print(\"âŒ File export failed\")\n",
    "    \n",
    "    # Display partial data preview\n",
    "    if annotated_dataset.datasamples:\n",
    "        print(\"\\nðŸ“‹ Data preview:\")\n",
    "        sample = annotated_dataset.datasamples[0]\n",
    "        print(f\"  - Sample ID: {sample.unique_id}\")\n",
    "        print(f\"  - Annotation status: {sample.metadata.get('annotation_status', 'unknown')}\")\n",
    "        print(f\"  - Output count: {len(sample.output) if sample.output else 0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Built-in Annotation Templates\n",
    "\n",
    "The system provides the following pre-configured annotation templates for out-of-the-box usage, located in `rm_gallery/gallery/data/annotation/`:\n",
    "\n",
    "| Template Name | Template ID | Source | Description |\n",
    "|---------------|-------------|--------|-------------|\n",
    "| RewardBenchAnnotationTemplate | `rewardbench` | RewardBench | Supports 2-choice quality scoring and ranking |\n",
    "| RewardBench2AnnotationTemplate | `rewardbench2` | RewardBench2 | Supports 4-choice quality scoring and ranking |\n",
    "\n",
    "## 6. Custom Annotation Template Development\n",
    "\n",
    "If the built-in templates don't meet your annotation needs, you can develop custom templates following these steps:\n",
    "\n",
    "### Step 1: Create Template Class\n",
    "\n",
    "Create a new template file in the `rm_gallery/gallery/data/annotation/` directory, inheriting from the `BaseAnnotationTemplate` base class:\n",
    "\n",
    "```python\n",
    "from rm_gallery.core.data.annotation.template import BaseAnnotationTemplate, AnnotationTemplateRegistry\n",
    "\n",
    "@AnnotationTemplateRegistry.register(\"custom_template\")\n",
    "class CustomAnnotationTemplate(BaseAnnotationTemplate):\n",
    "    @property\n",
    "    def label_config(self) -> str:\n",
    "        \"\"\"\n",
    "        Define Label Studio annotation interface configuration\n",
    "        Using Label Studio's XML configuration syntax\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "        <View>\n",
    "            <Text name=\"question\" value=\"$question\"/>\n",
    "            <Choices name=\"quality\" toName=\"question\" choice=\"single-radio\">\n",
    "                <Choice value=\"excellent\" background=\"green\"/>\n",
    "                <Choice value=\"good\" background=\"blue\"/>\n",
    "                <Choice value=\"fair\" background=\"yellow\"/>\n",
    "                <Choice value=\"poor\" background=\"red\"/>\n",
    "            </Choices>\n",
    "            <Rating name=\"score\" toName=\"question\" maxRating=\"10\" />\n",
    "            <TextArea name=\"comments\" toName=\"question\" \n",
    "                     placeholder=\"Please enter evaluation reason...\" rows=\"3\"/>\n",
    "        </View>\n",
    "        \"\"\"\n",
    "    \n",
    "    def process_annotations(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Process annotation data obtained from Label Studio\n",
    "        Convert raw annotation data to structured format\n",
    "        \"\"\"\n",
    "        processed_data = {\n",
    "            \"quality_rating\": annotation_data.get(\"choices\", {}).get(\"quality\", {}).get(\"choices\", []),\n",
    "            \"numerical_score\": annotation_data.get(\"rating\", {}).get(\"score\", {}).get(\"rating\", 0),\n",
    "            \"textual_feedback\": annotation_data.get(\"textarea\", {}).get(\"comments\", {}).get(\"text\", [\"\"])[0]\n",
    "        }\n",
    "        return processed_data\n",
    "```\n",
    "\n",
    "### Step 2: Register Template\n",
    "\n",
    "Import your template class in the `rm_gallery/gallery/data/__init__.py` file to complete registration:\n",
    "\n",
    "```python\n",
    "# Import custom annotation template\n",
    "from rm_gallery.gallery.data.annotation.custom_template import CustomAnnotationTemplate\n",
    "```\n",
    "\n",
    "### Step 3: Use Custom Template\n",
    "\n",
    "When creating an annotation module, specify your custom template name:\n",
    "\n",
    "```python\n",
    "annotation_module = create_annotation_module(\n",
    "    name=\"custom_annotation_project\",\n",
    "    template_name=\"custom_template\",  # Use your registered template name\n",
    "    # ... other configuration parameters\n",
    ")\n",
    "```\n",
    "\n",
    "## 7. Data Format Specifications\n",
    "\n",
    "### Input Data Requirements\n",
    "\n",
    "- **DataSample Standard Format**  \n",
    "\n",
    "### Output Data Format\n",
    "\n",
    "- **Annotation Result Integration**: Annotation data is automatically added to the `label` field of DataSample\n",
    "- **Original Data Protection**: Maintains integrity and original structure of input data\n",
    "- **Rich Metadata**: Includes annotation project ID, annotation status, timestamps, and other tracking information\n",
    "- **Multi-format Export**: Supports JSON, JSONL, and other export formats\n",
    "\n",
    "### Data Structure Example\n",
    "\n",
    "```python\n",
    "# Annotated DataSample structure example\n",
    "data_sample = DataSample(\n",
    "    unique_id=\"sample_001\",\n",
    "    input=[...],  # Original input data\n",
    "    output=[...], # Original output data (if any)\n",
    "    metadata={\n",
    "        \"annotation_status\": \"completed\",\n",
    "        \"annotation_project_id\": 123,\n",
    "        \"annotator_id\": \"user@example.com\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# The label field in each output contains annotation results\n",
    "output.label = {\n",
    "    \"annotation_data\": {\n",
    "        \"ratings\": {...},      # Rating data\n",
    "        \"choices\": {...},      # Choice data\n",
    "        \"text_areas\": {...}    # Text input data\n",
    "    },\n",
    "    \"processed\": {...}         # Structured data processed by template\n",
    "}\n",
    "```\n",
    "\n",
    "## 8. Troubleshooting Guide\n",
    "\n",
    "### 8.1. Common Issues and Solutions\n",
    "\n",
    "#### 8.1.1. Label Studio Service Startup Failure\n",
    "\n",
    "**Problem**: Service cannot start or is inaccessible after startup\n",
    "\n",
    "**Solution Steps**:\n",
    "```bash\n",
    "# Check if port is occupied\n",
    "lsof -i :8080\n",
    "\n",
    "# If port is occupied, start with different port\n",
    "python ./rm_gallery/core/data/annotation/server.py start --port 8081\n",
    "\n",
    "# View detailed startup logs\n",
    "python ./rm_gallery/core/data/annotation/server.py start --data-dir ./custom_log --verbose\n",
    "\n",
    "# Clean previous data directory (use with caution)\n",
    "rm -rf ./log/label_studio_logs\n",
    "```\n",
    "\n",
    "#### 8.1.2. API Token Acquisition Failure\n",
    "\n",
    "**Problem**: Cannot find API Token in the interface\n",
    "\n",
    "**Solution Steps**:\n",
    "1. Ensure correct login to Label Studio interface\n",
    "2. Check user permission settings, ensure admin privileges\n",
    "3. Enable API Tokens feature in Organization settings\n",
    "4. If still unable to obtain, try recreating user account\n",
    "\n",
    "#### 8.1.3. Data Import Failure\n",
    "\n",
    "**Problem**: Project created successfully but data cannot be imported\n",
    "\n",
    "**Solution Steps**:\n",
    "- Check if data format meets requirements\n",
    "- Verify API Token correctness\n",
    "- Confirm normal network connection\n",
    "- Check if data size exceeds limits\n",
    "\n",
    "#### 8.1.4. Annotation Result Export Exception\n",
    "\n",
    "**Problem**: Exported data is incomplete or format is abnormal\n",
    "\n",
    "**Solution Steps**:\n",
    "- Confirm all data has been annotated\n",
    "- Check write permissions for export path\n",
    "- Verify correct implementation of template's process_annotations method\n",
    "\n",
    "### 8.2 Getting Technical Support\n",
    "\n",
    "If you encounter unresolvable issues, you can get help through the following methods:\n",
    "\n",
    "1. **View log files**: Log files in the `./log/label_studio_logs/` directory\n",
    "2. **Check system status**: Run `python ./rm_gallery/core/data/annotation/server.py status`\n",
    "3. **Restart service**: Completely stop the service and restart\n",
    "4. **Community support**: Submit an Issue in the project repository with detailed error information and reproduction steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm-gallery-VQCvXsd2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
