# RM-Gallery vs ä¸»æµ Eval æ¡†æ¶å¯¹æ¯”åˆ†ææŠ¥å‘Š

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-10-24
**åˆ†æå¯¹è±¡**: OpenAI Evals, LangChain OpenEvals, Google ADK, Azure AI Agent Evals
**åˆ†æç»´åº¦**: åŠŸèƒ½å®Œæ•´æ€§ã€æ¶æ„è®¾è®¡ã€å·¥ç¨‹åŒ–èƒ½åŠ›

---

## ğŸ“‘ ç›®å½•

1. [æ‰§è¡Œæ‘˜è¦](#æ‰§è¡Œæ‘˜è¦)
2. [åˆ†ææ–¹æ³•](#åˆ†ææ–¹æ³•)
3. [æ¡†æ¶æ¦‚è§ˆ](#æ¡†æ¶æ¦‚è§ˆ)
4. [å…³é”®èƒ½åŠ›å¯¹æ¯”](#å…³é”®èƒ½åŠ›å¯¹æ¯”)
5. [ç¼ºå¤±èƒ½åŠ›è¯¦ç»†åˆ†æ](#ç¼ºå¤±èƒ½åŠ›è¯¦ç»†åˆ†æ)
6. [ä¼˜å…ˆçº§å»ºè®®](#ä¼˜å…ˆçº§å»ºè®®)
7. [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)
8. [å·®å¼‚åŒ–å®šä½å»ºè®®](#å·®å¼‚åŒ–å®šä½å»ºè®®)

---

## æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒå‘ç°

é€šè¿‡æ·±åº¦åˆ†æå››ä¸ªä¸»æµ LLM è¯„ä¼°æ¡†æ¶çš„æºä»£ç å’Œæ¶æ„è®¾è®¡ï¼Œæˆ‘ä»¬å‘ç°äº†**10ä¸ªå…³é”®èƒ½åŠ›é¢†åŸŸ**ï¼Œå…¶ä¸­ RM-Gallery åœ¨ä»¥ä¸‹æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºå¤±ï¼š

1. **ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (User Simulator)** - æ‰€æœ‰å¯¹æ¯”æ¡†æ¶éƒ½å…·å¤‡ â­â­â­
2. **Eval Case/Set ç®¡ç†ç³»ç»Ÿ** - æ‰€æœ‰å¯¹æ¯”æ¡†æ¶éƒ½å…·å¤‡ â­â­â­
3. **ç»Ÿè®¡åˆ†æå’Œå¯¹æ¯”** - Azure AI Evals çš„æ ¸å¿ƒä¼˜åŠ¿ â­â­â­
4. **Trajectory/Tool è¯„ä¼°** - Google ADK çš„æ ¸å¿ƒèƒ½åŠ› â­â­â­
5. **ä»£ç è¯„ä¼°èƒ½åŠ›** - OpenEvals çš„ç‹¬ç‰¹ä¼˜åŠ¿ â­â­

### æˆ˜ç•¥å»ºè®®

RM-Gallery åº”è¯¥åœ¨ä¿æŒå…¶æ ¸å¿ƒä¼˜åŠ¿ï¼ˆReward Model ä¸“ä¸šæ€§ã€Rubric ç³»ç»Ÿã€Training é›†æˆï¼‰çš„åŸºç¡€ä¸Šï¼Œè¡¥é½é€šç”¨è¯„ä¼°æ¡†æ¶çš„åŸºç¡€èƒ½åŠ›ï¼Œå°†è‡ªèº«å®šä½ä¸ºï¼š

> **"ä¸“ä¸šçš„ Reward Model å¹³å° + å®Œæ•´çš„ LLM è¯„ä¼°æ¡†æ¶"**

### ä¼˜å…ˆçº§çŸ©é˜µ

| èƒ½åŠ› | ç´§æ€¥åº¦ | é‡è¦åº¦ | å»ºè®®ä¼˜å…ˆçº§ |
|------|--------|--------|------------|
| Eval Case/Set ç®¡ç†ç³»ç»Ÿ | é«˜ | é«˜ | P0 |
| ç”¨æˆ·æ¨¡æ‹Ÿå™¨ | é«˜ | é«˜ | P0 |
| ç»Ÿè®¡åˆ†æå’Œå¯¹æ¯” | é«˜ | é«˜ | P0 |
| Trajectory/Tool è¯„ä¼° | ä¸­ | é«˜ | P1 |
| RAG ä¸“ç”¨è¯„ä¼° | ä¸­ | ä¸­ | P1 |

---

## åˆ†ææ–¹æ³•

### ç ”ç©¶å¯¹è±¡

æˆ‘ä»¬é€‰æ‹©äº†å››ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è¯„ä¼°æ¡†æ¶è¿›è¡Œæ·±åº¦å¯¹æ¯”ï¼š

1. **OpenAI Evals** - OpenAI å®˜æ–¹è¯„ä¼°æ¡†æ¶ï¼Œå¹¿æ³›åº”ç”¨äº GPT ç³»åˆ—æ¨¡å‹è¯„ä¼°
2. **LangChain OpenEvals** - LangChain ç”Ÿæ€çš„è¯„ä¼°å·¥å…·ï¼Œä¸“æ³¨äºåº”ç”¨åœºæ™¯
3. **Google ADK (Agent Development Kit)** - Google çš„ Agent å¼€å‘å’Œè¯„ä¼°æ¡†æ¶
4. **Azure AI Agent Evals** - Microsoft Azure çš„ AI Agent è¯„ä¼°è§£å†³æ–¹æ¡ˆ

### åˆ†æç»´åº¦

- **æ¶æ„è®¾è®¡**: æ ¸å¿ƒæŠ½è±¡ã€æ‰©å±•æ€§ã€æ¨¡å—åŒ–ç¨‹åº¦
- **åŠŸèƒ½å®Œæ•´æ€§**: è¯„ä¼°ç±»å‹ã€ä½¿ç”¨åœºæ™¯è¦†ç›–åº¦
- **å·¥ç¨‹åŒ–èƒ½åŠ›**: CI/CD é›†æˆã€å¯ç»´æŠ¤æ€§ã€å›¢é˜Ÿåä½œ
- **ç”Ÿæ€ç³»ç»Ÿ**: ä¸å…¶ä»–å·¥å…·çš„é›†æˆèƒ½åŠ›

### æ•°æ®æ¥æº

- æ¡†æ¶æºä»£ç æ·±åº¦åˆ†æ (ä½äº `data/eval_examples/`)
- å®˜æ–¹æ–‡æ¡£å’Œ README
- æ¶æ„è®¾è®¡å’Œæ¥å£å®šä¹‰

---

## æ¡†æ¶æ¦‚è§ˆ

### OpenAI Evals

**å®šä½**: é€šç”¨ LLM è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒæ¨¡å‹èƒ½åŠ›åŸºå‡†æµ‹è¯•

**æ ¸å¿ƒç‰¹ç‚¹**:
- å¼ºå¤§çš„ Registry ç³»ç»Ÿ (evals, data, eval_sets, completion_fns)
- Solver æŠ½è±¡ï¼Œæ”¯æŒå¤æ‚æ¨ç†é“¾è¯„ä¼°
- YAML é©±åŠ¨çš„é…ç½®ç®¡ç†
- ä¸°å¯Œçš„å†…ç½®è¯„ä¼°å¥—ä»¶ (elsuite)

**æ¶æ„äº®ç‚¹**:
```python
class Eval(abc.ABC):
    def __init__(self, completion_fns: list[Union[CompletionFn, Solver]]):
        # æ”¯æŒå¤šç§ completion function
        self.completion_fns = [maybe_wrap_with_compl_fn(fn) for fn in completion_fns]

    @abc.abstractmethod
    def eval_sample(self, sample: Any, rng: random.Random):
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""

    @abc.abstractmethod
    def run(self, recorder: RecorderBase) -> Dict[str, float]:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
```

**å…¸å‹åº”ç”¨åœºæ™¯**:
- æ¨¡å‹åŸºå‡†æµ‹è¯• (MMLU, HumanEval, etc.)
- æ¨ç†èƒ½åŠ›è¯„ä¼° (CoT, ReAct)
- è‡ªå®šä¹‰è¯„ä¼°ä»»åŠ¡

---

### LangChain OpenEvals

**å®šä½**: åº”ç”¨å¯¼å‘çš„è¯„ä¼°å·¥å…·ï¼Œä¸“æ³¨äºç”Ÿäº§ç¯å¢ƒ LLM åº”ç”¨

**æ ¸å¿ƒç‰¹ç‚¹**:
- LLM-as-Judge æ ¸å¿ƒèŒƒå¼
- ä¸°å¯Œçš„é¢„æ„å»ºè¯„ä¼°å™¨ (Correctness, Conciseness, Hallucination, etc.)
- RAG ä¸“ç”¨è¯„ä¼°å™¨å¥—ä»¶
- ä»£ç è¯„ä¼°èƒ½åŠ› (Pyright, Mypy, Sandboxed Execution)
- å¤šè½®å¯¹è¯æ¨¡æ‹Ÿ (Multiturn Simulation)

**æ¶æ„äº®ç‚¹**:
```python
def create_llm_as_judge(
    *,
    prompt: Union[str, Runnable, Callable],
    judge: Optional[Union[ModelClient, BaseChatModel]] = None,
    continuous: bool = False,
    use_reasoning: bool = True,
    few_shot_examples: Optional[list[FewShotExample]] = None,
) -> SimpleEvaluator:
    """åˆ›å»º LLM-as-Judge è¯„ä¼°å™¨"""
```

**å…¸å‹åº”ç”¨åœºæ™¯**:
- RAG åº”ç”¨è´¨é‡è¯„ä¼°
- ä»£ç ç”Ÿæˆä»»åŠ¡è¯„ä¼°
- å¤šè½®å¯¹è¯ç³»ç»Ÿè¯„ä¼°
- ç»“æ„åŒ–è¾“å‡ºéªŒè¯

---

### Google ADK

**å®šä½**: Agent å¼€å‘å’Œè¯„ä¼°çš„å®Œæ•´å·¥å…·åŒ…

**æ ¸å¿ƒç‰¹ç‚¹**:
- å®Œæ•´çš„ Agent è¯„ä¼°ç”Ÿå‘½å‘¨æœŸç®¡ç†
- ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (User Simulator)
- Trajectory è¯„ä¼° (Agent æ‰§è¡Œè½¨è¿¹)
- Tool Use Quality è¯„ä¼°
- å¤šç§å­˜å‚¨åç«¯ (Local, GCS, Memory, Vertex AI)
- Rubric-based è¯„ä¼°

**æ¶æ„äº®ç‚¹**:
```python
# Evaluator æ¥å£
class Evaluator(ABC):
    def evaluate_invocations(
        self,
        actual_invocations: list[Invocation],
        expected_invocations: list[Invocation],
    ) -> EvaluationResult:
        """è¯„ä¼° Agent è°ƒç”¨åºåˆ—"""

# User Simulator æ¥å£
class UserSimulator(ABC):
    async def get_next_user_message(
        self, events: list[Event]
    ) -> NextUserMessage:
        """ç”Ÿæˆä¸‹ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯"""
```

**å…¸å‹åº”ç”¨åœºæ™¯**:
- Agent èƒ½åŠ›è¯„ä¼°
- å¤šè½®äº¤äº’è‡ªåŠ¨åŒ–æµ‹è¯•
- Tool calling è´¨é‡è¯„ä¼°
- Agent æ¨ç†è¿‡ç¨‹åˆ†æ

---

### Azure AI Agent Evals

**å®šä½**: ä¼ä¸šçº§ AI Agent è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼Œæ·±åº¦é›†æˆ Azure ç”Ÿæ€

**æ ¸å¿ƒç‰¹ç‚¹**:
- GitHub Action å¼€ç®±å³ç”¨
- ç»Ÿè®¡åˆ†æèƒ½åŠ› (ç½®ä¿¡åŒºé—´ã€æ˜¾è‘—æ€§æ£€éªŒ)
- A/B æµ‹è¯•æ”¯æŒ (Baseline vs Treatment)
- ç³»ç»ŸåŒ– Safety è¯„ä¼°
- Azure AI Studio é›†æˆ

**æ¶æ„äº®ç‚¹**:
```python
# ç»Ÿè®¡åˆ†æ
class EvaluationScoreCI:
    def _compute_ci(self, data: pd.Series, confidence_level: float = 0.95):
        if self.score.data_type == EvaluationScoreDataType.BOOLEAN:
            result = binomtest(data.sum(), data.count())
            ci = result.proportion_ci(confidence_level, method="wilsoncc")
        elif self.score.data_type == EvaluationScoreDataType.CONTINUOUS:
            # t-distribution for continuous data
            stderr = data.std() / (self.count**0.5)
            z_ao2 = t.ppf(1 - (1 - confidence_level) / 2, df=self.count - 1)

# ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
def mcnemar(contingency_table: np.ndarray) -> float:
    """McNemar's test for paired boolean data"""
```

**å…¸å‹åº”ç”¨åœºæ™¯**:
- CI/CD é›†æˆçš„è‡ªåŠ¨åŒ–è¯„ä¼°
- ç§‘å­¦ä¸¥è°¨çš„æ¨¡å‹å¯¹æ¯”å®éªŒ
- ä¼ä¸šçº§å®‰å…¨é£é™©è¯„ä¼°
- å›¢é˜Ÿåä½œå’Œç»“æœå…±äº«

---

## å…³é”®èƒ½åŠ›å¯¹æ¯”

### èƒ½åŠ›çŸ©é˜µ

| èƒ½åŠ›ç»´åº¦ | RM-Gallery | OpenAI Evals | OpenEvals | Google ADK | Azure AI Evals | æ‰€æœ‰æ¡†æ¶å…±æ€§ |
|---------|------------|--------------|-----------|------------|----------------|--------------|
| **åŸºç¡€è¯„ä¼°èƒ½åŠ›** |
| Pointwise è¯„ä¼° | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Pairwise è¯„ä¼° | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Listwise è¯„ä¼° | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| LLM-as-Judge | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| **æµ‹è¯•ç®¡ç†** |
| Eval Case ç®¡ç† | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… |
| Eval Set ç®¡ç† | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… |
| Registry ç³»ç»Ÿ | âš ï¸ (ä»… RM) | âœ… | âœ… | âœ… | âœ… | âœ… |
| ç»“æœç®¡ç† | âš ï¸ (ç®€å•) | âœ… | âœ… | âœ… | âœ… | âœ… |
| **é«˜çº§è¯„ä¼°** |
| User Simulator | âŒ | âŒ | âœ… | âœ… | âŒ | âš ï¸ |
| Trajectory è¯„ä¼° | âŒ | âš ï¸ | âŒ | âœ… | âŒ | âŒ |
| Tool Use è¯„ä¼° | âŒ | âš ï¸ | âŒ | âœ… | âœ… | âš ï¸ |
| RAG ä¸“ç”¨è¯„ä¼° | âŒ | âŒ | âœ… | âš ï¸ | âŒ | âŒ |
| ä»£ç è¯„ä¼° | âŒ | âš ï¸ | âœ… | âŒ | âŒ | âŒ |
| **ç»Ÿè®¡åˆ†æ** |
| ç½®ä¿¡åŒºé—´ | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ |
| æ˜¾è‘—æ€§æ£€éªŒ | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ |
| A/B æµ‹è¯• | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ |
| **å·¥ç¨‹åŒ–** |
| CI/CD é›†æˆ | âŒ | âš ï¸ | âš ï¸ | âš ï¸ | âœ… | âš ï¸ |
| å¤šå­˜å‚¨åç«¯ | âŒ | âš ï¸ | âŒ | âœ… | âœ… | âš ï¸ |
| å¯è§†åŒ–æŠ¥å‘Š | âš ï¸ (ç®€å•) | âœ… | âœ… | âœ… | âœ… | âœ… |
| **RM-Gallery ç‹¬æœ‰** |
| Rubric ç”Ÿæˆ | âœ… | âŒ | âŒ | âš ï¸ | âŒ | - |
| RM è®­ç»ƒé›†æˆ | âœ… | âŒ | âŒ | âŒ | âŒ | - |
| é«˜æ€§èƒ½ Serving | âœ… | âŒ | âŒ | âš ï¸ | âŒ | - |

**å›¾ä¾‹**: âœ… å®Œæ•´æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âŒ ä¸æ”¯æŒ

---

## ç¼ºå¤±èƒ½åŠ›è¯¦ç»†åˆ†æ

### 1. ç”¨æˆ·æ¨¡æ‹Ÿå™¨ (User Simulator) â­â­â­

#### é—®é¢˜æè¿°

**æ‰€æœ‰å¯¹æ¯”æ¡†æ¶éƒ½å…·å¤‡ï¼ŒRM-Gallery å®Œå…¨ç¼ºå¤±**

ç”¨æˆ·æ¨¡æ‹Ÿå™¨æ˜¯è‡ªåŠ¨åŒ–å¤šè½®å¯¹è¯è¯„ä¼°çš„å…³é”®ç»„ä»¶ã€‚å½“å‰ RM-Gallery åªèƒ½è¯„ä¼°å•æ¬¡äº¤äº’ï¼ˆä¸€é—®ä¸€ç­”ï¼‰ï¼Œæ— æ³•æµ‹è¯•ï¼š
- å¤šè½®å¯¹è¯çš„è¿è´¯æ€§
- ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
- é•¿å¯¹è¯ä¸­çš„è®°å¿†ä¿æŒ
- å¤æ‚ä»»åŠ¡çš„åˆ†è§£å’Œæ‰§è¡Œ

#### å¯¹æ¯”æ¡†æ¶å®ç°

**Google ADK çš„å®ç°**:

```python
class UserSimulator(ABC):
    """ç”¨æˆ·æ¨¡æ‹Ÿå™¨åŸºç±»"""

    async def get_next_user_message(
        self, events: list[Event]
    ) -> NextUserMessage:
        """æ ¹æ®å†å²äº¤äº’ç”Ÿæˆä¸‹ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯

        Args:
            events: Agent çš„å†å²æ‰§è¡Œäº‹ä»¶

        Returns:
            NextUserMessage: åŒ…å«çŠ¶æ€å’Œä¸‹ä¸€æ¡æ¶ˆæ¯
        """
        pass

class Status(enum.Enum):
    SUCCESS = "success"
    TURN_LIMIT_REACHED = "turn_limit_reached"
    STOP_SIGNAL_DETECTED = "stop_signal_detected"
    NO_MESSAGE_GENERATED = "no_message_generated"
```

**LangChain OpenEvals çš„å®ç°**:

```python
from openevals.simulators import simulate_user

# è‡ªåŠ¨åŒ–å¤šè½®å¯¹è¯
async for turn in simulate_user(
    user_simulator=my_simulator,
    agent=my_agent,
    max_turns=10
):
    print(f"Turn {turn.turn_number}: {turn.message}")
```

#### å½±å“åˆ†æ

**ä¸šåŠ¡å½±å“**:
- ğŸš« æ— æ³•è¯„ä¼°å¤šè½®å¯¹è¯èƒ½åŠ›
- ğŸš« æ— æ³•æµ‹è¯•é•¿å¯¹è¯åœºæ™¯
- ğŸš« æ— æ³•è‡ªåŠ¨åŒ–äº¤äº’å¼ä»»åŠ¡æµ‹è¯•
- ğŸš« æ— æ³•è¯„ä¼° Agent çš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›

**æŠ€æœ¯å€ºåŠ¡**:
- åªèƒ½ä¾èµ–äººå·¥æµ‹è¯•å¤šè½®å¯¹è¯
- æ— æ³•è¿›è¡Œå¤§è§„æ¨¡è‡ªåŠ¨åŒ–æµ‹è¯•
- è¯„ä¼°ç»“æœä¸å¤Ÿå…¨é¢

#### å®ç°å»ºè®®

**Phase 1: åŸºç¡€ç”¨æˆ·æ¨¡æ‹Ÿå™¨**

```python
from rm_gallery.core.evaluation import BaseUserSimulator, UserMessage

class StaticUserSimulator(BaseUserSimulator):
    """é¢„å®šä¹‰è„šæœ¬çš„ç”¨æˆ·æ¨¡æ‹Ÿå™¨"""

    def __init__(self, script: list[str]):
        self.script = script
        self.current_turn = 0

    async def get_next_message(
        self, history: list[Message]
    ) -> UserMessage | None:
        if self.current_turn >= len(self.script):
            return None
        message = self.script[self.current_turn]
        self.current_turn += 1
        return UserMessage(content=message)

class LLMUserSimulator(BaseUserSimulator):
    """LLM é©±åŠ¨çš„åŠ¨æ€ç”¨æˆ·æ¨¡æ‹Ÿå™¨"""

    def __init__(self, task_description: str, llm: BaseLLM):
        self.task_description = task_description
        self.llm = llm

    async def get_next_message(
        self, history: list[Message]
    ) -> UserMessage | None:
        # æ ¹æ®ä»»åŠ¡æè¿°å’Œå†å²å¯¹è¯ç”Ÿæˆä¸‹ä¸€æ¡æ¶ˆæ¯
        prompt = self._build_prompt(history)
        response = await self.llm.generate(prompt)
        return self._parse_response(response)
```

**Phase 2: å¤šè½®è¯„ä¼°æ¡†æ¶**

```python
class MultiturnEvaluator:
    """å¤šè½®å¯¹è¯è¯„ä¼°å™¨"""

    async def evaluate_multiturn(
        self,
        agent: BaseAgent,
        simulator: BaseUserSimulator,
        max_turns: int = 10,
        evaluators: list[BaseReward] = None
    ) -> MultiturnResult:
        """æ‰§è¡Œå¤šè½®å¯¹è¯è¯„ä¼°"""
        history = []

        for turn in range(max_turns):
            # ç”¨æˆ·æ¨¡æ‹Ÿå™¨ç”Ÿæˆæ¶ˆæ¯
            user_msg = await simulator.get_next_message(history)
            if user_msg is None:
                break

            # Agent å“åº”
            agent_response = await agent.respond(user_msg, history)

            # è®°å½•å†å²
            history.extend([user_msg, agent_response])

            # è¯„ä¼°æ¯ä¸ªå›åˆ
            turn_result = self._evaluate_turn(
                user_msg, agent_response, evaluators
            )

        return self._aggregate_results(history)
```

**é¢„è®¡å·¥ä½œé‡**: 2-3 å‘¨ (1 ä¸ªå·¥ç¨‹å¸ˆ)

---

### 2. Eval Case/Set ç®¡ç†ç³»ç»Ÿ â­â­â­

#### é—®é¢˜æè¿°

**æ‰€æœ‰å¯¹æ¯”æ¡†æ¶éƒ½å…·å¤‡å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹ç®¡ç†ç³»ç»Ÿ**

å½“å‰ RM-Gallery çš„è¯„ä¼°æ˜¯é’ˆå¯¹ç‰¹å®š benchmarkï¼ˆå¦‚ RewardBench2, JudgeBenchï¼‰ç¡¬ç¼–ç çš„ï¼Œç¼ºä¹ï¼š
- é€šç”¨çš„æµ‹è¯•ç”¨ä¾‹å®šä¹‰å’Œç®¡ç†
- æµ‹è¯•é›†åˆçš„ç»„ç»‡å’Œç‰ˆæœ¬æ§åˆ¶
- ç»“æœçš„æŒä¹…åŒ–å’Œæ£€ç´¢
- æµ‹è¯•ç”¨ä¾‹çš„å¤ç”¨å’Œå…±äº«

#### å¯¹æ¯”æ¡†æ¶å®ç°

**Google ADK çš„å®ç°**:

```python
@dataclass
class EvalCase:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
    id: str
    inputs: dict  # è¾“å…¥æ•°æ®
    expected_outputs: dict  # æœŸæœ›è¾“å‡º
    metadata: dict = None  # å…ƒæ•°æ®

class EvalSet:
    """æµ‹è¯•é›†åˆ"""
    id: str
    name: str
    description: str
    eval_cases: list[EvalCase]

class EvalSetsManager:
    """æµ‹è¯•é›†ç®¡ç†å™¨"""
    def save_eval_set(self, eval_set: EvalSet) -> None:
        """ä¿å­˜æµ‹è¯•é›†"""

    def load_eval_set(self, eval_set_id: str) -> EvalSet:
        """åŠ è½½æµ‹è¯•é›†"""

    def list_eval_sets(self) -> list[str]:
        """åˆ—å‡ºæ‰€æœ‰æµ‹è¯•é›†"""
```

**OpenAI Evals çš„ Registry ç³»ç»Ÿ**:

```yaml
# evals/registry/evals/my_eval.yaml
my_eval:
  id: my_eval.v1
  description: "æµ‹è¯•æ•°å­¦èƒ½åŠ›"
  metrics: [accuracy]

my_eval.train:
  class: evals.elsuite.basic.match:Match
  args:
    samples_jsonl: train_samples.jsonl

my_eval.test:
  class: evals.elsuite.basic.match:Match
  args:
    samples_jsonl: test_samples.jsonl
```

#### å½±å“åˆ†æ

**ä¸šåŠ¡å½±å“**:
- ğŸš« éš¾ä»¥ç³»ç»ŸåŒ–ç®¡ç†æµ‹è¯•ç”¨ä¾‹
- ğŸš« æ— æ³•æ„å»ºå¯å¤ç”¨çš„è¯„ä¼°å¥—ä»¶
- ğŸš« å›¢é˜Ÿåä½œå›°éš¾ï¼ˆæ— æ³•å…±äº«æµ‹è¯•ç”¨ä¾‹ï¼‰
- ğŸš« æ— æ³•è¿½è¸ªå†å²è¯„ä¼°ç»“æœ

**æŠ€æœ¯å€ºåŠ¡**:
- æ¯ä¸ª benchmark éƒ½éœ€è¦å•ç‹¬å®ç°åŠ è½½é€»è¾‘
- æµ‹è¯•ç”¨ä¾‹åˆ†æ•£ï¼Œéš¾ä»¥ç»´æŠ¤
- æ— æ³•è¿›è¡Œç‰ˆæœ¬æ§åˆ¶

#### å®ç°å»ºè®®

**Phase 1: æ ¸å¿ƒæ•°æ®æ¨¡å‹**

```python
from pydantic import BaseModel
from typing import Any, Optional
from datetime import datetime

class EvalCase(BaseModel):
    """æµ‹è¯•ç”¨ä¾‹"""
    id: str
    inputs: dict[str, Any]
    expected_outputs: Optional[dict[str, Any]] = None
    metadata: dict[str, Any] = {}
    tags: list[str] = []

    class Config:
        json_schema_extra = {
            "example": {
                "id": "math_001",
                "inputs": {
                    "query": "What is 2+2?",
                    "context": "Basic arithmetic"
                },
                "expected_outputs": {
                    "answer": "4"
                },
                "tags": ["math", "basic"]
            }
        }

class EvalSet(BaseModel):
    """æµ‹è¯•é›†åˆ"""
    id: str
    name: str
    description: str
    version: str = "1.0"
    eval_cases: list[EvalCase]
    created_at: datetime
    metadata: dict[str, Any] = {}

class EvalResult(BaseModel):
    """è¯„ä¼°ç»“æœ"""
    eval_set_id: str
    eval_case_id: str
    model_name: str
    reward_name: str
    score: float
    details: dict[str, Any]
    timestamp: datetime
```

**Phase 2: ç®¡ç†å™¨å®ç°**

```python
from abc import ABC, abstractmethod
import json
from pathlib import Path

class BaseEvalSetsManager(ABC):
    """æµ‹è¯•é›†ç®¡ç†å™¨åŸºç±»"""

    @abstractmethod
    def save_eval_set(self, eval_set: EvalSet) -> None:
        pass

    @abstractmethod
    def load_eval_set(self, eval_set_id: str) -> EvalSet:
        pass

    @abstractmethod
    def list_eval_sets(self) -> list[str]:
        pass

class LocalEvalSetsManager(BaseEvalSetsManager):
    """æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.base_dir.mkdir(parents=True, exist_ok=True)

    def save_eval_set(self, eval_set: EvalSet) -> None:
        path = self.base_dir / f"{eval_set.id}.json"
        with open(path, 'w') as f:
            json.dump(eval_set.model_dump(), f, indent=2, default=str)

    def load_eval_set(self, eval_set_id: str) -> EvalSet:
        path = self.base_dir / f"{eval_set_id}.json"
        with open(path, 'r') as f:
            data = json.load(f)
        return EvalSet(**data)

    def list_eval_sets(self) -> list[str]:
        return [p.stem for p in self.base_dir.glob("*.json")]

class EvalResultsManager:
    """è¯„ä¼°ç»“æœç®¡ç†å™¨"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.base_dir.mkdir(parents=True, exist_ok=True)

    def save_result(self, result: EvalResult) -> None:
        """ä¿å­˜å•ä¸ªè¯„ä¼°ç»“æœ"""
        path = self._get_result_path(result)
        with open(path, 'a') as f:
            f.write(result.model_dump_json() + '\n')

    def load_results(
        self,
        eval_set_id: str,
        model_name: Optional[str] = None
    ) -> list[EvalResult]:
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        results = []
        pattern = f"{eval_set_id}_{model_name or '*'}_*.jsonl"
        for path in self.base_dir.glob(pattern):
            with open(path, 'r') as f:
                for line in f:
                    results.append(EvalResult.model_validate_json(line))
        return results
```

**Phase 3: YAML é…ç½®æ”¯æŒ**

```python
class YAMLEvalSetLoader:
    """ä» YAML æ–‡ä»¶åŠ è½½æµ‹è¯•é›†"""

    @staticmethod
    def load_from_yaml(yaml_path: Path) -> EvalSet:
        """ä» YAML é…ç½®åŠ è½½æµ‹è¯•é›†

        Example YAML:
        ```yaml
        id: math_basic
        name: "Basic Math Evaluation"
        description: "Tests basic arithmetic"
        version: "1.0"

        cases:
          - id: case_001
            inputs:
              query: "What is 2+2?"
            expected_outputs:
              answer: "4"
            tags: [math, arithmetic]

          - id: case_002
            inputs:
              query: "Calculate 10 * 5"
            expected_outputs:
              answer: "50"
            tags: [math, multiplication]
        ```
        """
        import yaml
        with open(yaml_path) as f:
            data = yaml.safe_load(f)

        cases = [EvalCase(**case) for case in data.pop('cases', [])]
        return EvalSet(eval_cases=cases, **data)
```

**Phase 4: CLI å·¥å…·**

```bash
# åˆ—å‡ºæ‰€æœ‰æµ‹è¯•é›†
rm-gallery eval list

# åˆ›å»ºæ–°æµ‹è¯•é›†
rm-gallery eval create --name "my_test" --from-jsonl data.jsonl

# è¿è¡Œè¯„ä¼°
rm-gallery eval run --eval-set math_basic --reward math_rm

# æŸ¥çœ‹ç»“æœ
rm-gallery eval results --eval-set math_basic --model gpt-4
```

**é¢„è®¡å·¥ä½œé‡**: 3-4 å‘¨ (1 ä¸ªå·¥ç¨‹å¸ˆ)

---

### 3. ç»Ÿè®¡åˆ†æå’Œå¯¹æ¯” â­â­â­

#### é—®é¢˜æè¿°

**Azure AI Evals çš„æ ¸å¿ƒä¼˜åŠ¿ï¼ŒRM-Gallery å®Œå…¨ç¼ºå¤±**

å½“å‰ RM-Gallery åªèƒ½è®¡ç®—ç®€å•çš„å‡†ç¡®ç‡ï¼Œç¼ºä¹ç§‘å­¦çš„ç»Ÿè®¡åˆ†æï¼š
- æ— ç½®ä¿¡åŒºé—´ä¼°è®¡
- æ— ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- æ— æ³•åˆ¤æ–­æ”¹è¿›æ˜¯å¦æ˜¾è‘—
- æ— æ³•è¿›è¡Œä¸¥è°¨çš„ A/B æµ‹è¯•

è¿™å¯¼è‡´æ— æ³•ç§‘å­¦è¯„ä¼°ï¼š
- æ¨¡å‹ A æ˜¯å¦çœŸçš„æ¯”æ¨¡å‹ B å¥½ï¼Ÿ
- æ–°ç‰ˆæœ¬çš„æ”¹è¿›æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Ÿ
- éœ€è¦å¤šå°‘æµ‹è¯•æ ·æœ¬æ‰èƒ½å¾—å‡ºå¯é ç»“è®ºï¼Ÿ

#### å¯¹æ¯”æ¡†æ¶å®ç°

**Azure AI Evals çš„ç»Ÿè®¡åˆ†æ**:

```python
from scipy.stats import binom, binomtest, t, ttest_rel, wilcoxon

class EvaluationScoreCI:
    """ç½®ä¿¡åŒºé—´è®¡ç®—"""

    def _compute_ci(self, data: pd.Series, confidence_level: float = 0.95):
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""

        if self.score.data_type == EvaluationScoreDataType.BOOLEAN:
            # Boolean æ•°æ®: Wilson Score Interval
            result = binomtest(data.sum(), data.count())
            ci = result.proportion_ci(
                confidence_level=confidence_level,
                method="wilsoncc"
            )
            return ci.low, result.proportion_estimate, ci.high

        elif self.score.data_type == EvaluationScoreDataType.CONTINUOUS:
            # Continuous æ•°æ®: t-distribution
            mean = data.mean()
            stderr = data.std() / (self.count**0.5)
            z_ao2 = t.ppf(1 - (1 - confidence_level) / 2, df=self.count - 1)
            ci_lower = mean - z_ao2 * stderr
            ci_upper = mean + z_ao2 * stderr
            return ci_lower, mean, ci_upper

def mcnemar(contingency_table: np.ndarray) -> float:
    """McNemar's test for paired boolean data

    ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šçš„è¡¨ç°å·®å¼‚
    """
    n12 = contingency_table[0, 1]  # Model A å¯¹ï¼ŒModel B é”™
    n21 = contingency_table[1, 0]  # Model A é”™ï¼ŒModel B å¯¹
    n = n12 + n21

    # Mid-p version
    pvalue_exact = 2 * binom.cdf(k=min(n12, n21), n=n, p=0.5)
    pvalue_midp = pvalue_exact - binom.pmf(k=n12, n=n, p=0.5)

    return float(pvalue_midp)
```

#### å½±å“åˆ†æ

**ä¸šåŠ¡å½±å“**:
- ğŸš« æ— æ³•ç§‘å­¦è¯„ä¼°æ¨¡å‹æ”¹è¿›æ•ˆæœ
- ğŸš« æ— æ³•åˆ¤æ–­å®éªŒç»“æœçš„å¯é æ€§
- ğŸš« æ— æ³•ç¡®å®šæ‰€éœ€çš„æµ‹è¯•æ ·æœ¬é‡
- ğŸš« éš¾ä»¥å‘ stakeholder æä¾›æœ‰è¯´æœåŠ›çš„æ•°æ®

**æŠ€æœ¯å€ºåŠ¡**:
- è¯„ä¼°ç»“æœå¯ä¿¡åº¦ä½
- æ— æ³•è¿›è¡Œç§‘å­¦çš„å¯¹æ¯”å®éªŒ
- å®¹æ˜“å¾—å‡ºé”™è¯¯ç»“è®º

#### å®ç°å»ºè®®

**Phase 1: ç½®ä¿¡åŒºé—´è®¡ç®—**

```python
from dataclasses import dataclass
from enum import Enum
import numpy as np
from scipy.stats import binom, binomtest, t

class ScoreType(Enum):
    BOOLEAN = "boolean"      # True/False
    CONTINUOUS = "continuous"  # 0.0 - 1.0
    ORDINAL = "ordinal"       # 1, 2, 3, 4, 5

@dataclass
class ConfidenceInterval:
    """ç½®ä¿¡åŒºé—´"""
    mean: float
    lower: float
    upper: float
    confidence_level: float
    sample_size: int
    score_type: ScoreType

class StatisticsAnalyzer:
    """ç»Ÿè®¡åˆ†æå™¨"""

    @staticmethod
    def compute_ci(
        scores: list[float],
        score_type: ScoreType,
        confidence_level: float = 0.95
    ) -> ConfidenceInterval:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""

        scores = np.array(scores)
        n = len(scores)
        mean = scores.mean()

        if score_type == ScoreType.BOOLEAN:
            # Wilson Score Interval for binary data
            result = binomtest(int(scores.sum()), n)
            ci = result.proportion_ci(
                confidence_level=confidence_level,
                method="wilsoncc"
            )
            return ConfidenceInterval(
                mean=result.proportion_estimate,
                lower=ci.low,
                upper=ci.high,
                confidence_level=confidence_level,
                sample_size=n,
                score_type=score_type
            )

        elif score_type == ScoreType.CONTINUOUS:
            # t-distribution for continuous data
            stderr = scores.std() / np.sqrt(n)
            t_critical = t.ppf(1 - (1 - confidence_level) / 2, df=n - 1)
            margin = t_critical * stderr

            return ConfidenceInterval(
                mean=mean,
                lower=mean - margin,
                upper=mean + margin,
                confidence_level=confidence_level,
                sample_size=n,
                score_type=score_type
            )

        elif score_type == ScoreType.ORDINAL:
            # For ordinal data, CI is not meaningful
            return ConfidenceInterval(
                mean=mean,
                lower=None,
                upper=None,
                confidence_level=confidence_level,
                sample_size=n,
                score_type=score_type
            )
```

**Phase 2: æ˜¾è‘—æ€§æ£€éªŒ**

```python
from scipy.stats import ttest_rel, wilcoxon, chi2_contingency

@dataclass
class SignificanceTestResult:
    """æ˜¾è‘—æ€§æ£€éªŒç»“æœ"""
    test_name: str
    p_value: float
    is_significant: bool
    alpha: float
    effect_size: Optional[float] = None

class SignificanceTest:
    """æ˜¾è‘—æ€§æ£€éªŒ"""

    @staticmethod
    def mcnemar_test(
        model_a_results: list[bool],
        model_b_results: list[bool],
        alpha: float = 0.05
    ) -> SignificanceTestResult:
        """McNemar's test for paired binary data

        ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šçš„å·®å¼‚
        """
        assert len(model_a_results) == len(model_b_results)

        # æ„å»º 2x2 contingency table
        #              Model B Correct  Model B Wrong
        # Model A Correct    n11             n12
        # Model A Wrong      n21             n22

        n11 = sum(a and b for a, b in zip(model_a_results, model_b_results))
        n12 = sum(a and not b for a, b in zip(model_a_results, model_b_results))
        n21 = sum(not a and b for a, b in zip(model_a_results, model_b_results))
        n22 = sum(not a and not b for a, b in zip(model_a_results, model_b_results))

        # McNemar's test focuses on discordant pairs
        n = n12 + n21
        if n == 0:
            return SignificanceTestResult(
                test_name="McNemar",
                p_value=1.0,
                is_significant=False,
                alpha=alpha
            )

        # Mid-p version
        pvalue_exact = 2 * binom.cdf(k=min(n12, n21), n=n, p=0.5)
        pvalue_midp = pvalue_exact - binom.pmf(k=n12, n=n, p=0.5)

        return SignificanceTestResult(
            test_name="McNemar",
            p_value=pvalue_midp,
            is_significant=pvalue_midp < alpha,
            alpha=alpha
        )

    @staticmethod
    def paired_t_test(
        model_a_scores: list[float],
        model_b_scores: list[float],
        alpha: float = 0.05
    ) -> SignificanceTestResult:
        """Paired t-test for continuous scores"""
        assert len(model_a_scores) == len(model_b_scores)

        statistic, p_value = ttest_rel(model_a_scores, model_b_scores)

        # Cohen's d for effect size
        diff = np.array(model_a_scores) - np.array(model_b_scores)
        effect_size = diff.mean() / diff.std()

        return SignificanceTestResult(
            test_name="Paired t-test",
            p_value=p_value,
            is_significant=p_value < alpha,
            alpha=alpha,
            effect_size=abs(effect_size)
        )

    @staticmethod
    def wilcoxon_test(
        model_a_scores: list[float],
        model_b_scores: list[float],
        alpha: float = 0.05
    ) -> SignificanceTestResult:
        """Wilcoxon signed-rank test (non-parametric)"""
        assert len(model_a_scores) == len(model_b_scores)

        statistic, p_value = wilcoxon(model_a_scores, model_b_scores)

        return SignificanceTestResult(
            test_name="Wilcoxon",
            p_value=p_value,
            is_significant=p_value < alpha,
            alpha=alpha
        )
```

**Phase 3: A/B æµ‹è¯•æ¡†æ¶**

```python
@dataclass
class ABTestResult:
    """A/B æµ‹è¯•ç»“æœ"""
    baseline_name: str
    treatment_name: str
    baseline_ci: ConfidenceInterval
    treatment_ci: ConfidenceInterval
    significance_test: SignificanceTestResult
    improvement: float
    recommendation: str

class ABTester:
    """A/B æµ‹è¯•å™¨"""

    def compare_models(
        self,
        baseline_scores: list[float],
        treatment_scores: list[float],
        score_type: ScoreType,
        baseline_name: str = "Baseline",
        treatment_name: str = "Treatment",
        alpha: float = 0.05
    ) -> ABTestResult:
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""

        # è®¡ç®—ç½®ä¿¡åŒºé—´
        baseline_ci = StatisticsAnalyzer.compute_ci(
            baseline_scores, score_type
        )
        treatment_ci = StatisticsAnalyzer.compute_ci(
            treatment_scores, score_type
        )

        # é€‰æ‹©åˆé€‚çš„æ˜¾è‘—æ€§æ£€éªŒ
        if score_type == ScoreType.BOOLEAN:
            baseline_bool = [s > 0.5 for s in baseline_scores]
            treatment_bool = [s > 0.5 for s in treatment_scores]
            sig_test = SignificanceTest.mcnemar_test(
                baseline_bool, treatment_bool, alpha
            )
        elif score_type == ScoreType.CONTINUOUS:
            sig_test = SignificanceTest.paired_t_test(
                baseline_scores, treatment_scores, alpha
            )
        else:
            sig_test = SignificanceTest.wilcoxon_test(
                baseline_scores, treatment_scores, alpha
            )

        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvement = (treatment_ci.mean - baseline_ci.mean) / baseline_ci.mean

        # ç”Ÿæˆå»ºè®®
        if sig_test.is_significant:
            if improvement > 0:
                recommendation = f"âœ… Treatment æ˜¾è‘—ä¼˜äº Baseline (p={sig_test.p_value:.4f})"
            else:
                recommendation = f"âš ï¸ Treatment æ˜¾è‘—å·®äº Baseline (p={sig_test.p_value:.4f})"
        else:
            recommendation = f"â– ä¸¤è€…æ— æ˜¾è‘—å·®å¼‚ (p={sig_test.p_value:.4f})"

        return ABTestResult(
            baseline_name=baseline_name,
            treatment_name=treatment_name,
            baseline_ci=baseline_ci,
            treatment_ci=treatment_ci,
            significance_test=sig_test,
            improvement=improvement,
            recommendation=recommendation
        )
```

**Phase 4: å¯è§†åŒ–æŠ¥å‘Š**

```python
import matplotlib.pyplot as plt
import seaborn as sns

class StatisticsVisualizer:
    """ç»Ÿè®¡å¯è§†åŒ–"""

    @staticmethod
    def plot_ab_test(result: ABTestResult, save_path: Path = None):
        """ç»˜åˆ¶ A/B æµ‹è¯•ç»“æœ"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # ç½®ä¿¡åŒºé—´å¯¹æ¯”
        models = [result.baseline_name, result.treatment_name]
        means = [result.baseline_ci.mean, result.treatment_ci.mean]
        lowers = [result.baseline_ci.lower, result.treatment_ci.lower]
        uppers = [result.baseline_ci.upper, result.treatment_ci.upper]

        ax1.errorbar(
            models, means,
            yerr=[
                [m - l for m, l in zip(means, lowers)],
                [u - m for m, u in zip(means, uppers)]
            ],
            fmt='o', markersize=10, capsize=5
        )
        ax1.set_title('Model Comparison with 95% CI')
        ax1.set_ylabel('Score')
        ax1.grid(True, alpha=0.3)

        # æ˜¾è‘—æ€§æ ‡æ³¨
        if result.significance_test.is_significant:
            ax1.text(
                0.5, max(uppers) * 1.05,
                f'p = {result.significance_test.p_value:.4f} *',
                ha='center', fontsize=12
            )

        # æ”¹è¿›å¹…åº¦
        ax2.bar(
            ['Improvement'],
            [result.improvement * 100],
            color='green' if result.improvement > 0 else 'red'
        )
        ax2.set_title('Relative Improvement')
        ax2.set_ylabel('Improvement (%)')
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
baseline_results = evaluator.evaluate(baseline_model, test_set)
treatment_results = evaluator.evaluate(treatment_model, test_set)

# A/B æµ‹è¯•
ab_tester = ABTester()
result = ab_tester.compare_models(
    baseline_scores=[r.score for r in baseline_results],
    treatment_scores=[r.score for r in treatment_results],
    score_type=ScoreType.CONTINUOUS,
    baseline_name="GPT-4",
    treatment_name="GPT-4-Turbo"
)

# æ‰“å°ç»“æœ
print(f"""
A/B Test Results
================
Baseline: {result.baseline_name}
  Mean: {result.baseline_ci.mean:.3f}
  95% CI: [{result.baseline_ci.lower:.3f}, {result.baseline_ci.upper:.3f}]

Treatment: {result.treatment_name}
  Mean: {result.treatment_ci.mean:.3f}
  95% CI: [{result.treatment_ci.lower:.3f}, {result.treatment_ci.upper:.3f}]

Statistical Test: {result.significance_test.test_name}
  p-value: {result.significance_test.p_value:.4f}
  Significant: {result.significance_test.is_significant}

Improvement: {result.improvement*100:+.2f}%

Recommendation: {result.recommendation}
""")

# å¯è§†åŒ–
StatisticsVisualizer.plot_ab_test(result, save_path="ab_test_result.png")
```

**é¢„è®¡å·¥ä½œé‡**: 2-3 å‘¨ (1 ä¸ªå·¥ç¨‹å¸ˆ)

---

### 4. Trajectory/Tool è¯„ä¼° â­â­â­

#### é—®é¢˜æè¿°

**Google ADK çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒRM-Gallery å®Œå…¨ç¼ºå¤±**

å½“å‰ RM-Gallery åªèƒ½è¯„ä¼°æœ€ç»ˆè¾“å‡ºï¼ˆfinal responseï¼‰ï¼Œæ— æ³•è¯„ä¼°ï¼š
- Agent çš„æ¨ç†è¿‡ç¨‹ï¼ˆreasoning trajectoryï¼‰
- å·¥å…·è°ƒç”¨çš„æ­£ç¡®æ€§å’Œæ•ˆç‡
- ä¸­é—´æ­¥éª¤çš„è´¨é‡
- ä»»åŠ¡è§„åˆ’èƒ½åŠ›

è¿™å¯¹äºè¯„ä¼° Agent ç³»ç»Ÿæ˜¯é‡å¤§ç¼ºé™·ã€‚

#### å¯¹æ¯”æ¡†æ¶å®ç°

**Google ADK çš„å®ç°**:

```python
@dataclass
class Invocation:
    """Agent çš„å•æ¬¡è°ƒç”¨"""
    request: Request
    response: Response
    tool_calls: list[ToolCall]
    timestamp: datetime

class TrajectoryEvaluator(Evaluator):
    """è½¨è¿¹è¯„ä¼°å™¨"""

    def evaluate_invocations(
        self,
        actual_invocations: list[Invocation],
        expected_invocations: list[Invocation],
    ) -> EvaluationResult:
        """è¯„ä¼°å®Œæ•´çš„æ‰§è¡Œè½¨è¿¹"""

        per_invocation_results = []
        for actual, expected in zip(actual_invocations, expected_invocations):
            # è¯„ä¼°æ¯ä¸ªæ­¥éª¤
            score = self._evaluate_single_invocation(actual, expected)
            per_invocation_results.append(
                PerInvocationResult(
                    actual_invocation=actual,
                    expected_invocation=expected,
                    score=score
                )
            )

        return EvaluationResult(
            overall_score=np.mean([r.score for r in per_invocation_results]),
            per_invocation_results=per_invocation_results
        )

class ToolUseQualityEvaluator:
    """å·¥å…·ä½¿ç”¨è´¨é‡è¯„ä¼°"""

    def evaluate_tool_calls(
        self,
        tool_calls: list[ToolCall],
        task: str
    ) -> dict:
        """è¯„ä¼°å·¥å…·è°ƒç”¨è´¨é‡

        è¯„ä¼°ç»´åº¦:
        - Correctness: å·¥å…·é€‰æ‹©æ˜¯å¦æ­£ç¡®
        - Efficiency: æ˜¯å¦ä½¿ç”¨äº†æœ€å°‘çš„å·¥å…·è°ƒç”¨
        - Parameter Accuracy: å‚æ•°æ˜¯å¦æ­£ç¡®
        """
        return {
            "correctness": self._eval_correctness(tool_calls, task),
            "efficiency": self._eval_efficiency(tool_calls, task),
            "parameter_accuracy": self._eval_parameters(tool_calls)
        }
```

#### å½±å“åˆ†æ

**ä¸šåŠ¡å½±å“**:
- ğŸš« æ— æ³•è¯„ä¼° Agent çš„æ¨ç†è´¨é‡
- ğŸš« æ— æ³•åˆ†æå¤±è´¥åŸå› ï¼ˆå“ªä¸€æ­¥å‡ºé”™äº†ï¼‰
- ğŸš« æ— æ³•ä¼˜åŒ–å·¥å…·è°ƒç”¨ç­–ç•¥
- ğŸš« æ— æ³•è¯„ä¼°ä»»åŠ¡è§„åˆ’èƒ½åŠ›

**æŠ€æœ¯å€ºåŠ¡**:
- Agent ç³»ç»Ÿè¯„ä¼°ä¸å®Œæ•´
- éš¾ä»¥è¿›è¡Œç»†ç²’åº¦çš„è°ƒè¯•
- æ— æ³•ä¼˜åŒ–ä¸­é—´æ­¥éª¤

#### å®ç°å»ºè®®

**Phase 1: è½¨è¿¹æ•°æ®ç»“æ„**

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional

@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨"""
    tool_name: str
    arguments: dict[str, Any]
    result: Optional[Any] = None
    error: Optional[str] = None
    timestamp: datetime = None

@dataclass
class ReasoningStep:
    """æ¨ç†æ­¥éª¤"""
    step_number: int
    thought: str  # æ¨ç†è¿‡ç¨‹
    action: str  # è¡ŒåŠ¨
    observation: str  # è§‚å¯Ÿç»“æœ
    tool_calls: list[ToolCall] = None

@dataclass
class AgentTrajectory:
    """Agent æ‰§è¡Œè½¨è¿¹"""
    task: str
    steps: list[ReasoningStep]
    final_answer: str
    total_time: float
    metadata: dict[str, Any] = None
```

**Phase 2: è½¨è¿¹è¯„ä¼°å™¨**

```python
from rm_gallery.core.reward.base import BaseReward

class TrajectoryReward(BaseReward):
    """è½¨è¿¹è¯„ä¼° Reward Model"""

    name: str = "trajectory_reward"

    def _evaluate(self, sample: DataSample, **kwargs) -> RewardResult:
        """è¯„ä¼° Agent æ‰§è¡Œè½¨è¿¹"""

        trajectory = self._extract_trajectory(sample)

        scores = {
            "correctness": self._eval_correctness(trajectory),
            "efficiency": self._eval_efficiency(trajectory),
            "reasoning_quality": self._eval_reasoning(trajectory),
            "tool_use_quality": self._eval_tool_use(trajectory)
        }

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = sum(scores.values()) / len(scores)

        return RewardResult(
            score=overall_score,
            details=scores,
            metadata={"trajectory_length": len(trajectory.steps)}
        )

    def _eval_correctness(self, trajectory: AgentTrajectory) -> float:
        """è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§"""
        # ä½¿ç”¨ LLM-as-Judge è¯„ä¼°æœ€ç»ˆç­”æ¡ˆ
        prompt = f"""
        Task: {trajectory.task}
        Agent's Answer: {trajectory.final_answer}

        Is this answer correct? Rate from 0.0 to 1.0.
        """
        return self._llm_judge(prompt)

    def _eval_efficiency(self, trajectory: AgentTrajectory) -> float:
        """è¯„ä¼°æ•ˆç‡ï¼ˆæ­¥éª¤æ•°ã€æ—¶é—´ï¼‰"""
        # æƒ©ç½šè¿‡å¤šçš„æ­¥éª¤
        optimal_steps = self._estimate_optimal_steps(trajectory.task)
        actual_steps = len(trajectory.steps)

        if actual_steps <= optimal_steps:
            return 1.0
        else:
            # è¶…å‡ºæœ€ä¼˜æ­¥æ•°ï¼Œåˆ†æ•°é€’å‡
            return max(0.0, 1.0 - (actual_steps - optimal_steps) * 0.1)

    def _eval_reasoning(self, trajectory: AgentTrajectory) -> float:
        """è¯„ä¼°æ¨ç†è´¨é‡"""
        step_scores = []
        for step in trajectory.steps:
            # è¯„ä¼°æ¯ä¸€æ­¥çš„æ¨ç†
            prompt = f"""
            Thought: {step.thought}
            Action: {step.action}
            Observation: {step.observation}

            Is this reasoning step logical and helpful? Rate from 0.0 to 1.0.
            """
            step_scores.append(self._llm_judge(prompt))

        return np.mean(step_scores) if step_scores else 0.0

    def _eval_tool_use(self, trajectory: AgentTrajectory) -> float:
        """è¯„ä¼°å·¥å…·ä½¿ç”¨è´¨é‡"""
        all_tool_calls = []
        for step in trajectory.steps:
            if step.tool_calls:
                all_tool_calls.extend(step.tool_calls)

        if not all_tool_calls:
            return 1.0  # ä¸éœ€è¦å·¥å…·

        scores = []
        for tool_call in all_tool_calls:
            # è¯„ä¼°å·¥å…·é€‰æ‹©å’Œå‚æ•°
            score = self._eval_single_tool_call(tool_call, trajectory.task)
            scores.append(score)

        return np.mean(scores)

class ToolUseReward(BaseReward):
    """å·¥å…·ä½¿ç”¨ä¸“é¡¹è¯„ä¼°"""

    name: str = "tool_use_reward"

    def _evaluate(self, sample: DataSample, **kwargs) -> RewardResult:
        """è¯„ä¼°å·¥å…·ä½¿ç”¨"""

        tool_calls = self._extract_tool_calls(sample)

        scores = {
            "tool_selection": self._eval_tool_selection(tool_calls),
            "parameter_correctness": self._eval_parameters(tool_calls),
            "error_handling": self._eval_error_handling(tool_calls),
            "necessity": self._eval_necessity(tool_calls)
        }

        return RewardResult(
            score=sum(scores.values()) / len(scores),
            details=scores
        )

    def _eval_tool_selection(self, tool_calls: list[ToolCall]) -> float:
        """è¯„ä¼°å·¥å…·é€‰æ‹©æ˜¯å¦åˆé€‚"""
        correct_selections = 0
        for tool_call in tool_calls:
            if self._is_tool_appropriate(tool_call):
                correct_selections += 1

        return correct_selections / len(tool_calls) if tool_calls else 1.0

    def _eval_parameters(self, tool_calls: list[ToolCall]) -> float:
        """è¯„ä¼°å‚æ•°æ­£ç¡®æ€§"""
        correct_params = 0
        for tool_call in tool_calls:
            if self._are_parameters_correct(tool_call):
                correct_params += 1

        return correct_params / len(tool_calls) if tool_calls else 1.0
```

**Phase 3: ReAct æ¨¡å¼æ”¯æŒ**

```python
class ReActTrajectoryParser:
    """è§£æ ReAct æ ¼å¼çš„è½¨è¿¹"""

    @staticmethod
    def parse_trajectory(text: str) -> AgentTrajectory:
        """è§£æ ReAct è¾“å‡º

        Example input:
        ```
        Task: Find the capital of France

        Thought 1: I need to search for information about France's capital
        Action 1: Search["capital of France"]
        Observation 1: Paris is the capital of France

        Thought 2: I have found the answer
        Action 2: Finish["Paris"]
        ```
        """
        lines = text.strip().split('\n')
        task = ""
        steps = []
        current_step = None

        for line in lines:
            line = line.strip()

            if line.startswith("Task:"):
                task = line[5:].strip()
            elif line.startswith("Thought"):
                if current_step:
                    steps.append(current_step)
                current_step = ReasoningStep(
                    step_number=len(steps) + 1,
                    thought=line.split(":", 1)[1].strip(),
                    action="",
                    observation=""
                )
            elif line.startswith("Action") and current_step:
                action_text = line.split(":", 1)[1].strip()
                current_step.action = action_text
                # è§£æå·¥å…·è°ƒç”¨
                if "[" in action_text and "]" in action_text:
                    tool_name = action_text[:action_text.index("[")]
                    tool_args = action_text[action_text.index("[")+1:action_text.index("]")]
                    current_step.tool_calls = [
                        ToolCall(
                            tool_name=tool_name,
                            arguments={"query": tool_args}
                        )
                    ]
            elif line.startswith("Observation") and current_step:
                current_step.observation = line.split(":", 1)[1].strip()

        if current_step:
            steps.append(current_step)

        return AgentTrajectory(
            task=task,
            steps=steps,
            final_answer=steps[-1].observation if steps else "",
            total_time=0.0
        )
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# è¯„ä¼° Agent è½¨è¿¹
trajectory_rm = TrajectoryReward(llm_model="gpt-4")

sample = DataSample(
    query="Find the population of Tokyo",
    response="""
    Thought 1: I need to search for Tokyo's population
    Action 1: Search["Tokyo population"]
    Observation 1: Tokyo has approximately 14 million people

    Thought 2: I have the answer
    Action 2: Finish["Tokyo has approximately 14 million people"]
    """
)

result = trajectory_rm.evaluate(sample)
print(f"""
Trajectory Evaluation:
- Overall Score: {result.score:.2f}
- Correctness: {result.details['correctness']:.2f}
- Efficiency: {result.details['efficiency']:.2f}
- Reasoning Quality: {result.details['reasoning_quality']:.2f}
- Tool Use Quality: {result.details['tool_use_quality']:.2f}
""")
```

**é¢„è®¡å·¥ä½œé‡**: 4-5 å‘¨ (1-2 ä¸ªå·¥ç¨‹å¸ˆ)

---

### 5. ä»£ç è¯„ä¼°èƒ½åŠ› â­â­

#### é—®é¢˜æè¿°

**LangChain OpenEvals çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼ŒRM-Gallery å®Œå…¨ç¼ºå¤±**

ä»£ç ç”Ÿæˆæ˜¯ LLM çš„é‡è¦åº”ç”¨åœºæ™¯ï¼Œä½† RM-Gallery ç¼ºä¹ä»£ç è¯„ä¼°èƒ½åŠ›ï¼š
- æ— é™æ€ç±»å‹æ£€æŸ¥ï¼ˆPyright, Mypyï¼‰
- æ— ä»£ç æ‰§è¡Œæµ‹è¯•
- æ— å®‰å…¨æ²™ç®±ç¯å¢ƒ
- æ— ä»£ç è´¨é‡è¯„ä¼°

#### å¯¹æ¯”æ¡†æ¶å®ç°

**OpenEvals çš„ä»£ç è¯„ä¼°**:

```python
from openevals.code import (
    pyright_evaluator,
    mypy_evaluator,
    sandbox_execution_evaluator,
    code_correctness_llm_evaluator
)

# é™æ€ç±»å‹æ£€æŸ¥
pyright_eval = pyright_evaluator()
result = pyright_eval(
    inputs={"task": "Write a function to add two numbers"},
    outputs={"code": "def add(a, b): return a + b"}
)

# æ²™ç®±æ‰§è¡Œ
exec_eval = sandbox_execution_evaluator(
    test_cases=[
        {"inputs": {"a": 1, "b": 2}, "expected_output": 3},
        {"inputs": {"a": -1, "b": 1}, "expected_output": 0}
    ]
)

# LLM è¯„ä¼°ä»£ç è´¨é‡
quality_eval = code_correctness_llm_evaluator()
```

#### å®ç°å»ºè®® (ç®€åŒ–ç‰ˆ)

```python
import subprocess
import tempfile
from pathlib import Path

class CodeReward(BaseReward):
    """ä»£ç è¯„ä¼° Reward Model"""

    name: str = "code_reward"

    def _evaluate(self, sample: DataSample, **kwargs) -> RewardResult:
        """è¯„ä¼°ä»£ç è´¨é‡"""

        code = self._extract_code(sample.response)

        scores = {
            "syntax_valid": self._check_syntax(code),
            "type_safety": self._run_pyright(code),
            "test_pass": self._run_tests(code, sample.test_cases),
            "code_quality": self._eval_quality_with_llm(code)
        }

        return RewardResult(
            score=sum(scores.values()) / len(scores),
            details=scores
        )

    def _check_syntax(self, code: str) -> float:
        """æ£€æŸ¥è¯­æ³•é”™è¯¯"""
        try:
            compile(code, '<string>', 'exec')
            return 1.0
        except SyntaxError:
            return 0.0

    def _run_pyright(self, code: str) -> float:
        """è¿è¡Œ Pyright ç±»å‹æ£€æŸ¥"""
        with tempfile.NamedTemporaryFile(
            mode='w', suffix='.py', delete=False
        ) as f:
            f.write(code)
            temp_path = f.name

        try:
            result = subprocess.run(
                ['pyright', temp_path],
                capture_output=True,
                text=True,
                timeout=10
            )
            # è§£æè¾“å‡ºï¼Œè®¡ç®—é”™è¯¯æ•°
            errors = result.stdout.count('error')
            if errors == 0:
                return 1.0
            else:
                return max(0.0, 1.0 - errors * 0.1)
        finally:
            Path(temp_path).unlink()

    def _run_tests(
        self, code: str, test_cases: list[dict]
    ) -> float:
        """è¿è¡Œæµ‹è¯•ç”¨ä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨æ²™ç®±ï¼‰"""
        if not test_cases:
            return 1.0

        # è­¦å‘Šï¼šè¿™æ˜¯ä¸å®‰å…¨çš„ï¼Œä»…ç”¨äºæ¼”ç¤º
        # ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Docker æ²™ç®±
        passed = 0
        for test in test_cases:
            try:
                exec(code, test['inputs'])
                # æ£€æŸ¥è¾“å‡º
                if 'expected_output' in test:
                    # ç®€åŒ–çš„æ£€æŸ¥é€»è¾‘
                    passed += 1
            except Exception:
                pass

        return passed / len(test_cases)
```

**é¢„è®¡å·¥ä½œé‡**: 2-3 å‘¨ (1 ä¸ªå·¥ç¨‹å¸ˆ)

---

### 6-10. å…¶ä»–ç¼ºå¤±èƒ½åŠ›

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œä»¥ä¸‹èƒ½åŠ›ç®€è¦è¯´æ˜ï¼š

#### 6. RAG ä¸“ç”¨è¯„ä¼° â­â­

**ç¼ºå¤±å†…å®¹**:
- Correctness: ç­”æ¡ˆæ­£ç¡®æ€§
- Helpfulness: ç­”æ¡ˆæœ‰ç”¨æ€§
- Groundedness: åŸºäºæ£€ç´¢å†…å®¹
- Retrieval Relevance: æ£€ç´¢è´¨é‡

**é¢„è®¡å·¥ä½œé‡**: 2 å‘¨

#### 7. å¯æ‰©å±• Solver ç³»ç»Ÿ â­â­

**ç¼ºå¤±å†…å®¹**:
- CompletionFn æŠ½è±¡
- Solver ç»„åˆ
- Postprocessor é“¾

**é¢„è®¡å·¥ä½œé‡**: 3 å‘¨

#### 8. CI/CD é›†æˆ â­â­

**ç¼ºå¤±å†…å®¹**:
- GitHub Action
- è‡ªåŠ¨åŒ–è¯„ä¼°è§¦å‘
- æŠ¥å‘Šç”Ÿæˆ

**é¢„è®¡å·¥ä½œé‡**: 1-2 å‘¨

#### 9. å¤šå­˜å‚¨åç«¯ â­

**ç¼ºå¤±å†…å®¹**:
- GCS/S3 æ”¯æŒ
- æ•°æ®åº“åç«¯
- å›¢é˜Ÿå…±äº«èƒ½åŠ›

**é¢„è®¡å·¥ä½œé‡**: 2 å‘¨

#### 10. ç³»ç»ŸåŒ– Safety è¯„ä¼° â­â­

**ç¼ºå¤±å†…å®¹**:
- Violence/Sexual/SelfHarm/Hate è¯„ä¼°å™¨
- é—´æ¥æ”»å‡»æ£€æµ‹
- ä»£ç æ¼æ´æ£€æµ‹

**é¢„è®¡å·¥ä½œé‡**: 3 å‘¨

---

## ä¼˜å…ˆçº§å»ºè®®

### P0 (å¿…é¡»å®Œæˆ) - 3 ä¸ªæœˆå†…

è¿™äº›æ˜¯**æ‰€æœ‰å¯¹æ¯”æ¡†æ¶çš„å…±æ€§èƒ½åŠ›**ï¼Œæ˜¯ RM-Gallery ä½œä¸ºé€šç”¨è¯„ä¼°æ¡†æ¶çš„åŸºç¡€è®¾æ–½ï¼š

| èƒ½åŠ› | å·¥ä½œé‡ | ä¸šåŠ¡ä»·å€¼ | æŠ€æœ¯ä»·å€¼ |
|------|--------|----------|----------|
| **Eval Case/Set ç®¡ç†ç³»ç»Ÿ** | 3-4 å‘¨ | é«˜ - ç³»ç»ŸåŒ–æµ‹è¯• | é«˜ - åŸºç¡€æ¶æ„ |
| **ç»Ÿè®¡åˆ†æå’Œå¯¹æ¯”** | 2-3 å‘¨ | é«˜ - ç§‘å­¦è¯„ä¼° | é«˜ - å¯ä¿¡åº¦ |
| **ç”¨æˆ·æ¨¡æ‹Ÿå™¨** | 2-3 å‘¨ | é«˜ - å¤šè½®å¯¹è¯ | ä¸­ - æ‰©å±•èƒ½åŠ› |

**é¢„æœŸæ•ˆæœ**:
- âœ… æ”¯æŒç³»ç»ŸåŒ–çš„æµ‹è¯•ç”¨ä¾‹ç®¡ç†
- âœ… æä¾›ç§‘å­¦ä¸¥è°¨çš„è¯„ä¼°ç»“æœ
- âœ… æ”¯æŒåŸºæœ¬çš„å¤šè½®å¯¹è¯è¯„ä¼°
- âœ… è¿½å¹³å…¶ä»–æ¡†æ¶çš„åŸºç¡€èƒ½åŠ›

### P1 (é‡è¦åŠŸèƒ½) - 6 ä¸ªæœˆå†…

è¿™äº›èƒ½åŠ›è®© RM-Gallery åœ¨ç‰¹å®šé¢†åŸŸå½¢æˆç«äº‰ä¼˜åŠ¿ï¼š

| èƒ½åŠ› | å·¥ä½œé‡ | ä¸šåŠ¡ä»·å€¼ | å·®å¼‚åŒ–ä»·å€¼ |
|------|--------|----------|------------|
| **Trajectory/Tool è¯„ä¼°** | 4-5 å‘¨ | é«˜ - Agent è¯„ä¼° | é«˜ - ä¸ ADK ç«äº‰ |
| **RAG ä¸“ç”¨è¯„ä¼°** | 2 å‘¨ | ä¸­ - è¦†ç›–ä¸»æµåœºæ™¯ | ä¸­ - ä¸ OpenEvals ç«äº‰ |
| **å¯æ‰©å±• Solver ç³»ç»Ÿ** | 3 å‘¨ | ä¸­ - æ¶æ„çµæ´»æ€§ | ä¸­ - ä¸ OpenAI Evals å¯¹é½ |

**é¢„æœŸæ•ˆæœ**:
- âœ… å…¨é¢çš„ Agent è¯„ä¼°èƒ½åŠ›
- âœ… è¦†ç›– RAG ç­‰ä¸»æµåº”ç”¨
- âœ… æ›´çµæ´»çš„æ¶æ„è®¾è®¡

### P2 (å¢å¼ºåŠŸèƒ½) - 12 ä¸ªæœˆå†…

è¿™äº›èƒ½åŠ›æå‡å·¥ç¨‹åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒï¼š

| èƒ½åŠ› | å·¥ä½œé‡ | ä¸šåŠ¡ä»·å€¼ | å·¥ç¨‹ä»·å€¼ |
|------|--------|----------|----------|
| **ä»£ç è¯„ä¼°èƒ½åŠ›** | 2-3 å‘¨ | ä¸­ - ä»£ç ç”Ÿæˆåœºæ™¯ | ä¸­ |
| **CI/CD é›†æˆ** | 1-2 å‘¨ | ä¸­ - å¼€å‘ä½“éªŒ | é«˜ |
| **å¤šå­˜å‚¨åç«¯** | 2 å‘¨ | ä½ - ä¼ä¸šéœ€æ±‚ | ä¸­ |
| **ç³»ç»ŸåŒ– Safety è¯„ä¼°** | 3 å‘¨ | ä¸­ - å®‰å…¨æ€§ | ä¸­ |

---

## å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€èƒ½åŠ›è¡¥é½ (Q1 2025)

**ç›®æ ‡**: å…·å¤‡é€šç”¨è¯„ä¼°æ¡†æ¶çš„æ ¸å¿ƒèƒ½åŠ›

**é‡Œç¨‹ç¢‘**:
1. Week 1-4: **Eval Case/Set ç®¡ç†ç³»ç»Ÿ**
   - æ•°æ®æ¨¡å‹è®¾è®¡
   - LocalManager å®ç°
   - YAML é…ç½®æ”¯æŒ
   - CLI å·¥å…·

2. Week 5-7: **ç»Ÿè®¡åˆ†ææ¨¡å—**
   - ç½®ä¿¡åŒºé—´è®¡ç®—
   - æ˜¾è‘—æ€§æ£€éªŒ
   - A/B æµ‹è¯•æ¡†æ¶
   - å¯è§†åŒ–æŠ¥å‘Š

3. Week 8-10: **ç”¨æˆ·æ¨¡æ‹Ÿå™¨**
   - åŸºç¡€æ¥å£è®¾è®¡
   - StaticUserSimulator
   - LLMUserSimulator
   - å¤šè½®è¯„ä¼°æ¡†æ¶

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥åˆ›å»ºå’Œç®¡ç†æµ‹è¯•é›†
- âœ… å¯ä»¥è¿›è¡Œç»Ÿè®¡ä¸¥è°¨çš„æ¨¡å‹å¯¹æ¯”
- âœ… å¯ä»¥è‡ªåŠ¨åŒ–å¤šè½®å¯¹è¯è¯„ä¼°

### Phase 2: Agent è¯„ä¼°èƒ½åŠ› (Q2 2025)

**ç›®æ ‡**: æ”¯æŒå…¨é¢çš„ Agent ç³»ç»Ÿè¯„ä¼°

**é‡Œç¨‹ç¢‘**:
1. Week 1-3: **è½¨è¿¹æ•°æ®ç»“æ„**
   - AgentTrajectory å®šä¹‰
   - ReActParser å®ç°
   - æ•°æ®é‡‡é›† hooks

2. Week 4-7: **Trajectory è¯„ä¼°å™¨**
   - TrajectoryReward å®ç°
   - æ¨ç†è´¨é‡è¯„ä¼°
   - æ•ˆç‡è¯„ä¼°
   - å·¥å…·ä½¿ç”¨è¯„ä¼°

3. Week 8-10: **é›†æˆå’Œä¼˜åŒ–**
   - ä¸ç°æœ‰ RM é›†æˆ
   - æ€§èƒ½ä¼˜åŒ–
   - æ–‡æ¡£å’Œç¤ºä¾‹

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥è¯„ä¼° Agent æ‰§è¡Œè½¨è¿¹
- âœ… å¯ä»¥åˆ†æå·¥å…·ä½¿ç”¨è´¨é‡
- âœ… å¯ä»¥è¯„ä¼°æ¨ç†è¿‡ç¨‹

### Phase 3: åº”ç”¨åœºæ™¯æ‰©å±• (Q3 2025)

**ç›®æ ‡**: è¦†ç›–ä¸»æµ LLM åº”ç”¨åœºæ™¯

**é‡Œç¨‹ç¢‘**:
1. Week 1-2: **RAG è¯„ä¼°å™¨**
   - Correctness/Helpfulness/Groundedness
   - Retrieval Relevance

2. Week 3-5: **ä»£ç è¯„ä¼°èƒ½åŠ›**
   - è¯­æ³•æ£€æŸ¥
   - ç±»å‹æ£€æŸ¥
   - ç®€åŒ–çš„æµ‹è¯•æ‰§è¡Œ

3. Week 6-8: **Solver æŠ½è±¡å±‚**
   - CompletionFn æ¥å£
   - Solver åŸºç±»
   - Postprocessor é“¾

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ”¯æŒ RAG åº”ç”¨è¯„ä¼°
- âœ… æ”¯æŒä»£ç ç”Ÿæˆè¯„ä¼°
- âœ… æ›´çµæ´»çš„è¯„ä¼°æ¶æ„

### Phase 4: å·¥ç¨‹åŒ–æå‡ (Q4 2025)

**ç›®æ ‡**: æå‡å·¥ç¨‹åŒ–å’Œå›¢é˜Ÿåä½œèƒ½åŠ›

**é‡Œç¨‹ç¢‘**:
1. Week 1-2: **CI/CD é›†æˆ**
   - GitHub Action
   - è‡ªåŠ¨åŒ–è¯„ä¼°
   - æŠ¥å‘Šç”Ÿæˆ

2. Week 3-4: **å¤šå­˜å‚¨åç«¯**
   - GCS/S3 æ”¯æŒ
   - æ•°æ®åº“åç«¯

3. Week 5-7: **Safety è¯„ä¼°å™¨**
   - å››å¤§ç±»å®‰å…¨è¯„ä¼°
   - é—´æ¥æ”»å‡»æ£€æµ‹

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯é›†æˆåˆ° CI/CD æµç¨‹
- âœ… æ”¯æŒå›¢é˜Ÿåä½œ
- âœ… å…¨é¢çš„å®‰å…¨è¯„ä¼°

---

## å·®å¼‚åŒ–å®šä½å»ºè®®

### RM-Gallery çš„æ ¸å¿ƒä¼˜åŠ¿

RM-Gallery åœ¨ä»¥ä¸‹æ–¹é¢å…·æœ‰**ç‹¬ç‰¹ä¼˜åŠ¿**ï¼Œåº”è¯¥ç»§ç»­ä¿æŒå’ŒåŠ å¼ºï¼š

1. **Reward Model ä¸“ä¸šæ€§** â­â­â­
   - Rubric ç”Ÿæˆå’Œç®¡ç†
   - Rubric-Critic-Score èŒƒå¼
   - ä¸°å¯Œçš„å†…ç½® RM Gallery

2. **Training é›†æˆ** â­â­â­
   - ä¸ VERL ç­‰æ¡†æ¶é›†æˆ
   - RM è®­ç»ƒæµç¨‹
   - é—­ç¯ä¼˜åŒ–èƒ½åŠ›

3. **é«˜æ€§èƒ½ Serving** â­â­
   - é«˜ååé‡
   - å®¹é”™æœºåˆ¶
   - æ‰¹å¤„ç†ä¼˜åŒ–

### å»ºè®®çš„å·®å¼‚åŒ–å®šä½

> **RM-Gallery: ä¸“ä¸šçš„ Reward Model å¹³å° + å®Œæ•´çš„ LLM è¯„ä¼°æ¡†æ¶**

**æ ¸å¿ƒä»·å€¼ä¸»å¼ **:
1. **æ·±åº¦**: Reward Model é¢†åŸŸçš„æœ€ä¸“ä¸šå·¥å…·
2. **å¹¿åº¦**: è¦†ç›– LLM è¯„ä¼°çš„å…¨ç”Ÿå‘½å‘¨æœŸ
3. **é—­ç¯**: ä»è¯„ä¼°åˆ°è®­ç»ƒåˆ°åº”ç”¨çš„å®Œæ•´é“¾è·¯

### ç«äº‰ç­–ç•¥

**vs OpenAI Evals**:
- âœ… æ›´ä¸“æ³¨äº Reward Model
- âœ… æ›´å¥½çš„ Training é›†æˆ
- âš ï¸ éœ€è¦è¡¥é½ Registry å’Œ Solver ç³»ç»Ÿ

**vs OpenEvals**:
- âœ… æ›´ç³»ç»Ÿçš„ RM ç®¡ç†
- âœ… æ›´å¼ºçš„ Training èƒ½åŠ›
- âš ï¸ éœ€è¦è¡¥é½ RAG å’Œä»£ç è¯„ä¼°

**vs Google ADK**:
- âœ… æ›´ä¸“æ³¨äºè¯„ä¼°ï¼ˆä¸æ˜¯ Agent å¼€å‘ï¼‰
- âœ… æ›´ä¸°å¯Œçš„ RM Gallery
- âš ï¸ éœ€è¦è¡¥é½ Trajectory å’Œç”¨æˆ·æ¨¡æ‹Ÿå™¨

**vs Azure AI Evals**:
- âœ… æ›´çµæ´»ï¼ˆä¸ç»‘å®š Azureï¼‰
- âœ… æ›´ä¸“ä¸šçš„ RM èƒ½åŠ›
- âš ï¸ éœ€è¦è¡¥é½ç»Ÿè®¡åˆ†æå’Œ CI/CD

### ç›®æ ‡ç”¨æˆ·ç”»åƒ

**Primary Users**:
1. **RLHF/RLAIF ç ”ç©¶è€…** - éœ€è¦è®­ç»ƒå’Œè¯„ä¼° RM
2. **LLM åº”ç”¨å¼€å‘è€…** - éœ€è¦è¯„ä¼° LLM è¾“å‡ºè´¨é‡
3. **Agent å¼€å‘è€…** - éœ€è¦è¯„ä¼° Agent ç³»ç»Ÿ

**Use Cases**:
- âœ… Reward Model è®­ç»ƒå’Œè¯„ä¼°
- âœ… LLM è¾“å‡ºè´¨é‡è¯„ä¼°
- âœ… Agent ç³»ç»Ÿè¯„ä¼°
- âœ… RLHF/RLAIF æµç¨‹
- âœ… Model-as-Judge åº”ç”¨

---

## æ€»ç»“å’Œå»ºè®®

### å…³é”®å‘ç°

1. **åŸºç¡€èƒ½åŠ›ç¼ºå¤±**: Eval Case/Set ç®¡ç†ã€ç»Ÿè®¡åˆ†æã€ç”¨æˆ·æ¨¡æ‹Ÿå™¨æ˜¯æ‰€æœ‰æ¡†æ¶çš„å…±æ€§ï¼ŒRM-Gallery å¿…é¡»è¡¥é½

2. **Agent è¯„ä¼°ä¸è¶³**: Trajectory å’Œ Tool è¯„ä¼°æ˜¯ Agent æ—¶ä»£çš„åˆšéœ€ï¼Œéœ€è¦å°½å¿«æ”¯æŒ

3. **åº”ç”¨åœºæ™¯è¦†ç›–**: RAGã€ä»£ç è¯„ä¼°ç­‰ä¸»æµåœºæ™¯éœ€è¦ä¸“é—¨çš„è¯„ä¼°å™¨

4. **å·¥ç¨‹åŒ–ç¨‹åº¦**: CI/CD é›†æˆã€å¤šå­˜å‚¨åç«¯ç­‰å·¥ç¨‹åŒ–èƒ½åŠ›æœ‰å¾…æå‡

### æˆ˜ç•¥å»ºè®®

**çŸ­æœŸ (3 ä¸ªæœˆ)**:
- ğŸ¯ **è¡¥é½åŸºç¡€èƒ½åŠ›** - P0 ä¸‰é¡¹å¿…é¡»å®Œæˆ
- ğŸ¯ **ä¿æŒæ ¸å¿ƒä¼˜åŠ¿** - ç»§ç»­æ·±åŒ– Reward Model èƒ½åŠ›

**ä¸­æœŸ (6 ä¸ªæœˆ)**:
- ğŸ¯ **Agent è¯„ä¼°èƒ½åŠ›** - Trajectory/Tool è¯„ä¼°
- ğŸ¯ **åº”ç”¨åœºæ™¯æ‰©å±•** - RAGã€ä»£ç è¯„ä¼°

**é•¿æœŸ (12 ä¸ªæœˆ)**:
- ğŸ¯ **å·¥ç¨‹åŒ–æå‡** - CI/CDã€å›¢é˜Ÿåä½œ
- ğŸ¯ **ç”Ÿæ€å»ºè®¾** - ä¸ä¸»æµæ¡†æ¶é›†æˆ

### æˆåŠŸæŒ‡æ ‡

**åŠŸèƒ½å®Œæ•´æ€§**:
- âœ… è¦†ç›– 80% çš„å¯¹æ¯”æ¡†æ¶æ ¸å¿ƒèƒ½åŠ›
- âœ… è‡³å°‘ 5 ä¸ªåº”ç”¨åœºæ™¯çš„ä¸“ç”¨è¯„ä¼°å™¨
- âœ… å®Œæ•´çš„ Agent è¯„ä¼°èƒ½åŠ›

**æ˜“ç”¨æ€§**:
- âœ… 5 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- âœ… å®Œå–„çš„æ–‡æ¡£å’Œç¤ºä¾‹
- âœ… CLI å’Œ API åŒæ¥å£

**æ€§èƒ½å’Œå¯é æ€§**:
- âœ… ç»Ÿè®¡å­¦ä¸¥è°¨æ€§
- âœ… é«˜ååé‡è¯„ä¼°
- âœ… å¯æ‰©å±•æ¶æ„

---

## é™„å½•

### A. å¯¹æ¯”æ¡†æ¶èµ„æº

- **OpenAI Evals**: https://github.com/openai/evals
- **LangChain OpenEvals**: https://github.com/langchain-ai/openevals
- **Google ADK**: https://github.com/google/adk-python
- **Azure AI Agent Evals**: https://github.com/microsoft/ai-agent-evals

### B. å‚è€ƒæ–‡çŒ®

1. McNemar, Q. (1947). "Note on the sampling error of the difference between correlated proportions or percentages"
2. Agresti, A. (2013). "Categorical Data Analysis" (3rd ed.)
3. Wilson, E. B. (1927). "Probable Inference, the Law of Succession, and Statistical Inference"

### C. æœ¯è¯­è¡¨

- **Eval Case**: å•ä¸ªæµ‹è¯•ç”¨ä¾‹
- **Eval Set**: æµ‹è¯•é›†åˆ
- **Trajectory**: Agent æ‰§è¡Œè½¨è¿¹
- **User Simulator**: ç”¨æˆ·æ¨¡æ‹Ÿå™¨
- **Confidence Interval**: ç½®ä¿¡åŒºé—´
- **Statistical Significance**: ç»Ÿè®¡æ˜¾è‘—æ€§
- **McNemar's Test**: é…å¯¹äºŒå…ƒæ•°æ®çš„æ˜¾è‘—æ€§æ£€éªŒ

---

**æ–‡æ¡£ç»“æŸ**

