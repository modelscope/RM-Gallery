#!/usr/bin/env python3
"""
RM-Benchè¯„ä¼°è¿è¡Œè„šæœ¬
æ”¯æŒDeepSeek-Chatæ¨¡å‹åœ¨RM-Benchä¸Šçš„è¯„ä¼°
"""

import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
from loguru import logger
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluations.rm_bench_evaluator import RMBenchEvaluator


def setup_logging(log_level="INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stderr,
        level=log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"rm_bench_{timestamp}.log"
    
    logger.add(
        str(log_file),
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="100 MB"
    )
    
    return str(log_file)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="è¿è¡ŒRM-Benchè¯„ä¼°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é»˜è®¤è¿è¡ŒDeepSeek-Chatè¯„ä¼°
  python scripts/evaluation/run_rm_bench.py
  
  # æŒ‡å®šæ¨¡å‹é…ç½®æ–‡ä»¶
  python scripts/evaluation/run_rm_bench.py --model configs/models/gpt-3.5-turbo.yaml
  
  # è¿è¡Œå°‘é‡æ ·æœ¬æµ‹è¯•
  python scripts/evaluation/run_rm_bench.py --max-samples 10
  
  # ä½¿ç”¨8ä¸ªçº¿ç¨‹å¹¶è¡Œè¯„ä¼°
  python scripts/evaluation/run_rm_bench.py --max-workers 8
  
  python scripts/evaluation/run_rm_bench.py --max-workers 32 --model configs/models/qwen2.5-14b-instruct.yaml
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python scripts/evaluation/run_rm_bench.py --output-dir data/outputs/rm_bench_test
        """
    )
    
    parser.add_argument(
        "--model",
        default="configs/models/deepseek-chat.yaml",
        help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/models/deepseek-chat.yaml)"
    )
    
    parser.add_argument(
        "--data",
        default="benchmarks/RM-Bench/total_dataset.json",
        help="æ•°æ®é›†è·¯å¾„ (é»˜è®¤: benchmarks/RM-Bench/total_dataset.json)"
    )
    
    parser.add_argument(
        "--template",
        default="rm_bench",
        help="Promptæ¨¡æ¿åç§° (é»˜è®¤: rm_bench)"
    )
    
    parser.add_argument(
        "--output-dir",
        default="data/outputs/rm_bench",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: data/outputs/rm_bench)"
    )
    
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•° (é»˜è®¤: å…¨éƒ¨æ ·æœ¬)"
    )
    
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: 4)"
    )
    
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å¹²è¿è¡Œæ¨¡å¼ï¼ŒåªåŠ è½½æ•°æ®å’Œæ¨¡å‹ï¼Œä¸è¿›è¡Œå®é™…è¯„ä¼°"
    )
    
    return parser.parse_args()


def validate_paths(args):
    """éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    errors = []
    
    # æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶
    if not Path(args.model).exists():
        errors.append(f"æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not Path(args.data).exists():
        errors.append(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
    
    # æ£€æŸ¥æ¨¡æ¿ç›®å½•
    template_dir = Path("templates/benchmarks")
    if not template_dir.exists():
        errors.append(f"æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {template_dir}")
    
    if errors:
        logger.error("è·¯å¾„éªŒè¯å¤±è´¥:")
        for error in errors:
            logger.error(f"  - {error}")
        return False
    
    return True


def print_config(args):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸš€ RM-Benchè¯„ä¼°é…ç½®")
    print("=" * 60)
    print(f"æ¨¡å‹é…ç½®:    {args.model}")
    print(f"æ•°æ®é›†:      {args.data}")
    print(f"æ¨¡æ¿:        {args.template}")
    print(f"è¾“å‡ºç›®å½•:    {args.output_dir}")
    print(f"æœ€å¤§æ ·æœ¬æ•°:  {args.max_samples or 'å…¨éƒ¨'}")
    print(f"æœ€å¤§çº¿ç¨‹æ•°:  {args.max_workers}")
    print(f"æ—¥å¿—çº§åˆ«:    {args.log_level}")
    print(f"å¹²è¿è¡Œ:      {args.dry_run}")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging(args.log_level)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    # æ‰“å°é…ç½®
    print_config(args)
    
    # éªŒè¯è·¯å¾„
    if not validate_paths(args):
        sys.exit(1)
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        logger.info("åˆå§‹åŒ–RM-Benchè¯„ä¼°å™¨...")
        evaluator = RMBenchEvaluator(
            model_config_path=args.model,
            template_name=args.template,
            data_path=args.data,
            max_workers=args.max_workers
        )
        
        logger.info("âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # å¦‚æœæ˜¯å¹²è¿è¡Œï¼Œå°±æ­¤ç»“æŸ
        if args.dry_run:
            logger.info("ğŸ å¹²è¿è¡Œæ¨¡å¼ï¼Œè¯„ä¼°å™¨åŠ è½½æˆåŠŸ")
            return
        
        # è¿è¡Œè¯„ä¼°
        logger.info("ğŸ”¥ å¼€å§‹RM-Benchè¯„ä¼°...")
        
        results = evaluator.evaluate(
            output_dir=args.output_dir,
            max_samples=args.max_samples
        )
        
        logger.info("âœ… è¯„ä¼°å®Œæˆ")
        
        # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        overall = results["overall_accuracy"]
        print(f"æ¨¡å‹: {results['model']}")
        print(f"Hard Accuracy:   {overall['hard_acc']:.4f}")
        print(f"Normal Accuracy: {overall['normal_acc']:.4f}")
        print(f"Easy Accuracy:   {overall['easy_acc']:.4f}")
        print(f"Overall Accuracy: {overall['overall_acc']:.4f}")
        
        details = results["evaluation_details"]
        print(f"\nè¯„ä¼°ç»Ÿè®¡:")
        print(f"æ€»æ ·æœ¬æ•°: {details['total_samples']}")
        print(f"æˆåŠŸè¯„ä¼°: {details['successful_samples']}")
        print(f"æˆåŠŸç‡: {details['success_rate']:.2%}")
        
        # æŒ‰é¢†åŸŸæ˜¾ç¤ºç»“æœ
        if results["domain_accuracy"]:
            print(f"\næŒ‰é¢†åŸŸå‡†ç¡®ç‡:")
            for domain, acc in results["domain_accuracy"].items():
                print(f"  {domain}: Hard={acc['hard_acc']:.4f}, Normal={acc['normal_acc']:.4f}, Easy={acc['easy_acc']:.4f}")
        
        print("=" * 60)
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        output_path = Path(args.output_dir) / "rm_bench_summary.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main() 